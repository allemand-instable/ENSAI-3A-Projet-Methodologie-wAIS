[INFO] â€”â€” main.py â€”â€” <module>  
program starts

[DEBUG] â€”â€” main.py â€”â€” <module>  
debug info only

[INFO] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

=========    BEGIN : SGD_L   =========

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

Î·_t = 0.01
Î¸_t = [5, 1]
ğ›¾ = 35
N = 100


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 1              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : {'loc': 5, 'scale': 1}

[4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 100

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [4.068395433782213, 5.988452552619513, 4.870272803872543, 2.9116718709146503, 4.062894541813671, 3.951907692366885, 4.414296444090939, 5.185153986903305, 4.707226273961055, 5.319509945892983, 4.3220588543640375, 5.631262219431296, 3.1863717469629935, 5.195623623321056, 4.649149768708871, 3.7035403343567204, 5.980251346533316, 4.868395354141642, 3.053180560159051, 6.4744507224852565, 4.3713069102964575, 4.361892743154881, 5.032936787330783, 6.0186986796766915, 4.758835667048313, 4.870937027448608, 4.964929274694722, 5.232282220706864, 4.82264258451891, 4.458072467891961, 4.613803930278985, 3.24759527534737, 5.286197399318152, 4.012118393769083, 4.88410549606166]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.068395433782213, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.068395433782213, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.068395433782213, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.068395433782213, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.068395433782213, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.93160462 -0.06605649]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9316046201135464   |
| -0.06605648694701927  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00010156015895301545

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2584941884502498

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0003928914594246744

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.0003660194988031806   |
| -2.5953029561081353e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.988452552619513, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.988452552619513, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.988452552619513, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.988452552619513, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.988452552619513, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.9884525 -0.0114808]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9884525042913594  |
| -0.011480800754526399  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.51134028677026e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24476474417841887

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.660244353665527e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.629525193407639e-08  |
| -3.0541735382787775e-10  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.870272803872543, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.870272803872543, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.870272803872543, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.870272803872543, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.870272803872543, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.12972725 -0.4915854 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.12972724672266622  |
| -0.49158540460503275  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8201594691336478e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3955994356063332

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.1288258154645844e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -9.248029454056863e-07  |
| -3.5044267228539604e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (2.9116718709146503, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.9116718709146503, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.9116718709146503, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.9116718709146503, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.9116718709146503, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.08832819  1.680557  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.088328185934074  |
| 1.6805569957867306  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00575402841740923

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.04507192550861565

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.1276632482965328

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.26660275972554964  |
| 0.21454536502959662  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.062894541813671, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.062894541813671, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.062894541813671, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.062894541813671, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.062894541813671, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.93710551 -0.0609167 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9371055109674842   |
| -0.06091670057273291  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00010385710830717312

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2571689921738989

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00040384770896852314

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00037844791366599574  |
| -2.4601069964219707e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (3.951907692366885, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.951907692366885, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.951907692366885, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.951907692366885, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.951907692366885, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.04809236  0.04924871]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.0480923617883775  |
| 0.04924870955491656  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00016203053745317205

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23034265465543066

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0007034326216981105

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0007372623578345629  |
| 3.4643148877463736e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.414296444090939, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.414296444090939, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.414296444090939, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.414296444090939, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.414296444090939, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.58570361 -0.32847566]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5857036078360522  |
| -0.3284756644461595  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.341941728095212e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.33606090824552237

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.968801400680067e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.08165212267125e-05   |
| -2.289081670481712e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.185153986903305, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.185153986903305, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.185153986903305, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.185153986903305, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.185153986903305, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.18515394 -0.48285898]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1851539377373257  |
| -0.4828589783567594  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.790659473502455e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3921622849522174

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4765977493751119e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.733978877508747e-07  |
| -7.129884807071568e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.707226273961055, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.707226273961055, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.707226273961055, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.707226273961055, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.707226273961055, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.29277378 -0.45714175]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.29277377633363244  |
| -0.45714175334055085  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.1569555570619945e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3822055368346499

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.610901717450949e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.716297791204485e-06  |
| -7.364104355748315e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.319509945892983, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.319509945892983, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.319509945892983, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.319509945892983, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.319509945892983, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.3195099  -0.44895668]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.31950989631823745  |
| -0.44895667850042287  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8592635324256003e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37908992384277124

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.542441390796473e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.409884666759764e-07  |
| -3.3862294345960943e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.3220588543640375, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3220588543640375, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3220588543640375, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3220588543640375, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3220588543640375, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.6779412 -0.2701979]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6779411987345441  |
| -0.2701979018304712  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.503948801321069e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3170357828958164

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00011052218678017581

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -7.492754379251557e-05   |
| -2.9862862973718947e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.631262219431296, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.631262219431296, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.631262219431296, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.631262219431296, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.631262219431296, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.63126217 -0.300754  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.631262171424396  |
| -0.3007540017030408  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.187153658754676e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3268726844641467

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.5869033740944583e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0017520697715684e-07  |
| -4.772675400749659e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (3.1863717469629935, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.1863717469629935, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.1863717469629935, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.1863717469629935, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.1863717469629935, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.81362831  1.14462358]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.8136283053138413  |
| 1.1446235781420455  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0024901284384557555

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0770298512787507

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.03232679795063654

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.058628795783435904  |
| 0.03700201514013254  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.195623623321056, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.195623623321056, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.195623623321056, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.195623623321056, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.195623623321056, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.19562357 -0.48086568]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1956235740063761  |
| -0.48086567838723226  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.484383555859501e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3913813658171279

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4012888795584783e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.741251388346198e-07  |
| -6.738317276853723e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.649149768708871, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.649149768708871, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.649149768708871, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.649149768708871, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.649149768708871, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.35085028 -0.43845204]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.35085028105008575  |
| -0.4384520391198521   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.079057654528163e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3751285635612837

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.153677016175365e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -7.5561818641623695e-06  |
| -9.442840793476475e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (3.7035403343567204, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7035403343567204, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7035403343567204, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7035403343567204, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7035403343567204, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.29645972  0.34040377]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.2964597195797012  |
| 0.3404037740573074  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00041924963104034835

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.17215804366821852

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.002435260195267568

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0031572167498601996  |
| 0.0008289717612806154  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.980251346533316, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.980251346533316, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.980251346533316, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.980251346533316, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.980251346533316, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.9802513  -0.01955367]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9802513001311297  |
| -0.019553672014893664  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.838881671065959e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24674869624397586

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.7715978950113394e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.7168624400255678e-08  |
| -5.419491619622141e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.868395354141642, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.868395354141642, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.868395354141642, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.868395354141642, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.868395354141642, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.1316047  -0.49134009]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.13160469714001977  |
| -0.4913400863948425   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8460593766547135e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39550239957064287

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.196060958781523e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -9.470354230815626e-07  |
| -3.5357132131902667e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (3.053180560159051, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.053180560159051, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.053180560159051, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.053180560159051, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.053180560159051, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.9468195  1.3950528]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.9468194967231511  |
| 1.395052797725782  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.003772964301283157

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.05996516319032027

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.06291927013203223

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.12249246181263099  |
| 0.0877757038285558  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.4744507224852565, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.4744507224852565, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.4744507224852565, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.4744507224852565, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.4744507224852565, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.47445067 0.58700238]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4744506726671602  |
| 0.5870023800369495  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.1507997761162065e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1345336399953495

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.3420162988417778e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 3.4531875072247123e-09  |
| 1.374769141505451e-09   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.3713069102964575, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3713069102964575, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3713069102964575, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3713069102964575, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3713069102964575, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.62869314 -0.3023725 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.628693141990766   |
| -0.3023724959305696  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8287216045762503e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3274021544038356

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.639899177594145e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -5.431845360445098e-05   |
| -2.6124678789176173e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.361892743154881, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.361892743154881, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.361892743154881, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.361892743154881, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.361892743154881, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.63810731 -0.29640956]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.6381073069938736   |
| -0.29640955911958144  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.9474275718030946e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3254556857066845

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.05630997167292e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -5.778897567325971e-05   |
| -2.6843768459538394e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.032936787330783, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.032936787330783, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.032936787330783, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.032936787330783, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.032936787330783, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.03293674 -0.49945756]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.03293673667137398  |
| -0.4994575619754471  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.2602948129492745e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3987259464103215

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.160804618549525e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0410658939082837e-07  |
| -1.5786877686614786e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.0186986796766915, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.01869863 0.01887347]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0186986343363458  |
| 0.01887347389484262  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.430115505065784e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23744671627793482

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.2868774898995678e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 2.32963897585522e-08  |
| 4.3161322606322706e-10  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.758835667048313, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.758835667048313, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.758835667048313, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.758835667048313, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.758835667048313, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.24116439 -0.47091986]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.2411643851285561  |
| -0.47091986421499143  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.822601784644709e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38750805203917543

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.244516535660829e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.0013306510496457e-06  |
| -5.860675579867092e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.870937027448608, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.870937027448608, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.870937027448608, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.870937027448608, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.870937027448608, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.12906302 -0.49167135]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.12906302249149348  |
| -0.4916713514102611   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.811050514546266e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3956334377572894

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.105189415943075e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -9.170172213961826e-07  |
| -3.4934180821626155e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.964929274694722, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.964929274694722, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.964929274694722, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.964929274694722, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.964929274694722, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.03507077 -0.499385  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.035070774062617716  |
| -0.49938499890878063   |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7705861915064637e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39869701514499295

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.440931645456538e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.5574691036533536e-07  |
| -2.2177346449202825e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.232282220706864, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.232282220706864, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.232282220706864, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.232282220706864, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.232282220706864, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.23228217 -0.47302246]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2322821712752443  |
| -0.47302246342084686  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.530210920333613e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38832368469700407

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1666069052338088e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.709819849724023e-07  |
| -5.518312721574667e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.82264258451891, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.82264258451891, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.82264258451891, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.82264258451891, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.82264258451891, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.17735747 -0.48427215]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.17735746649094608  |
| -0.48427215015856007  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.5524278663774355e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39271686988272037

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.045773530020038e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.604335475735216e-06  |
| -4.380616197230192e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.458072467891961, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.458072467891961, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.458072467891961, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.458072467891961, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.458072467891961, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.54192759 -0.35315727]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5419275850293559  |
| -0.3531572678738826  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9285759238612927e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3444586384699471

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.598860671422977e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.034177042580092e-05   |
| -1.9772783379262704e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.613803930278985, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.613803930278985, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.613803930278985, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.613803930278985, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.613803930278985, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.38619612 -0.42542628]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3861961206386155  |
| -0.42542628087005596  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.51603881384464e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3702739154033329

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.5699997806972134e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -9.925239453473565e-06  |
| -1.093345448538875e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (3.24759527534737, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.24759527534737, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.24759527534737, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.24759527534737, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.24759527534737, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.75240478  1.03546103]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                 â€”
| -1.7524047768446849  |
| 1.03546103247254  |
 â€”                 â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.002044962004362224

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.08591475521874736

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.023802221156954368

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.041711126054960454  |
| 0.024646272494319704  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.286197399318152, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.286197399318152, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.286197399318152, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.286197399318152, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.286197399318152, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.28619735 -0.45904551]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2861973502632509  |
| -0.4590455060515808  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.4117168698116967e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38293385430046367

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.909415637967441e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5498511479802526e-07  |
| -4.0898272101546314e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.012118393769083, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.012118393769083, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.012118393769083, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.012118393769083, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.012118393769083, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.98788166 -0.01204499]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.9878816586983419    |
| -0.012044989450288313  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00012748835434413532

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24490287706650446

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005205669932146828

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0005142585847205293  |
| -6.270223941439162e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.88410549606166, [5, 1]) âˆˆ [<class 'numpy.float64'>, <class 'list'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5 1]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0000001 1.       ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.88410549606166, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.88410549606166, array([5.0000001, 1.       ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.        1.0000001]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.88410549606166, [5, 1])

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.88410549606166, array([5.       , 1.0000001])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.11589456 -0.49328421]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.11589455617411204  |
| -0.4932842112559399   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.6361744029742475e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.396272052942296

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.652435828872645e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -7.709810978639563e-07  |
| -3.2815415607761975e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                        â€”
| -0.0003660194988031806   |
| -2.5953029561081353e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                      â€”
| 2.629525193407639e-08  |
| -3.0541735382787775e-10  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| -9.248029454056863e-07  |
| -3.5044267228539604e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                    â€”
| -0.26660275972554964  |
| 0.21454536502959662  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                        â€”
| -0.00037844791366599574  |
| -2.4601069964219707e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| -0.0007372623578345629  |
| 3.4643148877463736e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| -4.08165212267125e-05   |
| -2.289081670481712e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                      â€”
| 2.733978877508747e-07  |
| -7.129884807071568e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| -4.716297791204485e-06  |
| -7.364104355748315e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                      â€”
| 2.409884666759764e-07  |
| -3.3862294345960943e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                        â€”
| -7.492754379251557e-05   |
| -2.9862862973718947e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                       â€”
| 1.0017520697715684e-07  |
| -4.772675400749659e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                    â€”
| -0.058628795783435904  |
| 0.03700201514013254  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                      â€”
| 2.741251388346198e-07  |
| -6.738317276853723e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                        â€”
| -7.5561818641623695e-06  |
| -9.442840793476475e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                      â€”
| -0.0031572167498601996  |
| 0.0008289717612806154  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                       â€”
| 2.7168624400255678e-08  |
| -5.419491619622141e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| -9.470354230815626e-07  |
| -3.5357132131902667e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                   â€”
| -0.12249246181263099  |
| 0.0877757038285558  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                       â€”
| 3.4531875072247123e-09  |
| 1.374769141505451e-09   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                        â€”
| -5.431845360445098e-05   |
| -2.6124678789176173e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                        â€”
| -5.778897567325971e-05   |
| -2.6843768459538394e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| 1.0410658939082837e-07  |
| -1.5786877686614786e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                     â€”
| 2.32963897585522e-08  |
| 4.3161322606322706e-10  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                        â€”
| -3.0013306510496457e-06  |
| -5.860675579867092e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                       â€”
| -9.170172213961826e-07  |
| -3.4934180821626155e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                        â€”
| -1.5574691036533536e-07  |
| -2.2177346449202825e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                      â€”
| 2.709819849724023e-07  |
| -5.518312721574667e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                       â€”
| -1.604335475735216e-06  |
| -4.380616197230192e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                        â€”
| -3.034177042580092e-05   |
| -1.9772783379262704e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                       â€”
| -9.925239453473565e-06  |
| -1.093345448538875e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                     â€”
| -0.041711126054960454  |
| 0.024646272494319704  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                       â€”
| 2.5498511479802526e-07  |
| -4.0898272101546314e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                       â€”
| -0.0005142585847205293  |
| -6.270223941439162e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| -7.709810978639563e-07  |
| -3.2815415607761975e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.49487546  0.36459233]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.00494875 0.99635408]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : {'loc': 5, 'scale': 1}
type : <class 'dict'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.00494875 0.99635408]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 2              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.00494875 0.99635408]

[3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 200

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [4.119495359701023, 5.586355966218047, 5.310447194589536, 4.913065034204201, 7.405879356715559, 7.194080636994697, 6.584243588482547, 4.415717692938217, 5.195623623321056, 3.8344716745102545, 5.08660075475377, 4.879702242433275, 4.035612679030055, 5.215515562044457, 4.964929274694722, 6.069938050586868, 6.1141522712873195, 4.3569966403813005, 6.114315044446465, 5.143963586671972, 4.111326882270596, 5.010802007794817, 4.82264258451891, 4.794212663476959, 4.870272803872543, 3.9887648799226767, 4.228092373494506, 4.304118999872094, 6.256502992307158, 5.683552641208301, 5.447724251764727, 4.758835667048313, 4.820574508670093, 4.486912586006225, 5.661115785804077]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.119495359701023, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.119495359701023, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.119495359701023, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.119495359701023, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.119495359701023, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.88869356 -0.10694157]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8886935565222132   |
| -0.10694157337454158  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.238892441652632e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2696688430385445

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0003055188856383763

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.00027151266506267194  |
| -3.26726703258046e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.586355966218047, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.586355966218047, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.586355966218047, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.586355966218047, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.586355966218047, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.58353469 -0.33157323]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5835346850169287  |
| -0.33157323109378467  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.672907820990717e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3373116860319123

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.9782616782388643e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.1543843052921765e-07  |
| -6.559386166026733e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.310447194589536, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.30661629 -0.45482282]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.30661629124573153  |
| -0.45482281940678604  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.000360399116986e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3813843395867278

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.867025694783926e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.412158241669522e-07  |
| -3.578102806847255e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.913065034204201, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.913065034204201, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.913065034204201, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.913065034204201, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.913065034204201, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.09222    -0.49757735]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.0922199983044436  |
| -0.4975773459925392  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.287519424233396e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39798180285538937

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.747799039607318e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.300620176868695e-07  |
| -2.8599745914262752e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (7.405879356715559, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.405879356715559, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.405879356715559, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.405879356715559, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.405879356715559, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.4097162  2.40153629]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 2.409716204354595  |
| 2.4015362898310855  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.909357867014368e-13

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.022150549495035015

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.2163594036864808e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| 5.340797169737e-11  |
| 5.322667539261468e-11  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (7.194080636994697, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.194080636994697, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.194080636994697, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.194080636994697, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.194080636994697, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.19714245 1.91188772]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.1971424501998627  |
| 1.9118877236934395  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.3040869410049165e-12

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0360795309703492

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.386133297848179e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.4031244561337077e-10  |
| 1.2209569854025832e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (6.584243588482547, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.584243588482547, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.584243588482547, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.584243588482547, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.584243588482547, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.58507385 0.75439989]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.5850738499523231  |
| 0.754399889224544   |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5384579452450652e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.11431908554629694

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3457577428066641e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.133125406493707e-09  |
| 1.0152394920964199e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.415717692938217, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.415717692938217, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.415717692938217, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.415717692938217, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.415717692938217, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.59138727 -0.32696021]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.5913872658247499   |
| -0.32696020557665406  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.3272924654558755e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.33576488925957176

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.931315750696916e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -4.0990918703726726e-05  |
| -2.2662644227645637e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.195623623321056, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.195623623321056, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.195623623321056, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.195623623321056, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.195623623321056, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.19137255 -0.48351787]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.19137254891887778  |
| -0.4835178724071909  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.484383555859501e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39244565386620706

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3974886718274735e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.674409692128807e-07  |
| -6.75710749315171e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (3.8344716745102545, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.8344716745102545, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.8344716745102545, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.8344716745102545, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.8344716745102545, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.17476022  0.18820105]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.1747602224332354  |
| 0.18820104585870467  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.000255950870371042

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2009644676385761

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0012736125613576428

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0014961893758742672  |
| 0.00023969521606629206  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.08660075475377, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.08660075475377, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.08660075475377, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.08660075475377, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.08660075475377, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.08195074 -0.49847164]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.08195073619710058  |
| -0.498471640630882   |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.60614981039038e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3983365758541587

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4115660957801274e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 1.97629616937149e-07  |
| -1.202097308253331e-06  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.879702242433275, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.879702242433275, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.879702242433275, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.879702242433275, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.879702242433275, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.12570487 -0.49392875]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.12570487095686644  |
| -0.4939287545635551   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.693455757516801e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3965376503905692

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.792433845471888e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.538420200280958e-07  |
| -3.354978389749269e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.035612679030055, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.035612679030055, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.035612679030055, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.035612679030055, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.035612679030055, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.97288319 -0.02857886]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9728831851241182   |
| -0.02857885927909365  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00011598805100065082

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24941487627042466

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00046504062923212474

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00045243020857947366  |
| -1.3290330701886058e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.215515562044457, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.215515562044457, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.215515562044457, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.215515562044457, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.215515562044457, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.21133728 -0.47949788]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2113372765677468  |
| -0.4794978758493329  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.944895482175327e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39087691911832584

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2650773786615967e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6735800785380604e-07  |
| -6.066019158532778e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.964929274694722, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.964929274694722, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.964929274694722, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.964929274694722, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.964929274694722, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.04016597 -0.50102296]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.040165972770012104  |
| -0.5010229553459311    |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7705861915064637e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3993504407556581

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.43366529947013e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.7808247968986483e-07  |
| -2.2213680913552274e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (6.069938050586868, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.069938050586868, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.069938050586868, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.069938050586868, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.069938050586868, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.06888633 0.06942938]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0688863261165693  |
| 0.06942937558207518  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.983868510402242e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2262106772845836

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.761131949306862e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.8824498591011237e-08  |
| 1.222742915580183e-09   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (6.1141522712873195, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.1141522712873195, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.1141522712873195, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.1141522712873195, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.1141522712873195, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.11326234 0.1178469 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.1132623378706796  |
| 0.11784689890603772  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.043165648944427e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2155570832610307

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4117678727630952e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.5716680025629594e-08  |
| 1.663724657803044e-09   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.3569966403813005, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3569966403813005, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3569966403813005, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3569966403813005, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3569966403813005, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.6503232  -0.29036953]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.6503231952947885   |
| -0.29036953286265543  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.011014513413257e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3237442690670208

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.30059556603278e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -6.04839302664698e-05   |
| -2.700609589853423e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (6.114315044446465, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.114315044446465, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.114315044446465, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.114315044446465, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.114315044446465, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.11342571 0.11802879]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.1134257094091993  |
| 0.11802878896460811  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.0401384962870333e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.21551802300011805

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4106191463557404e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.5706196237373395e-08  |
| 1.6649366953465733e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.143963586671972, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143963586671972, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143963586671972, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143963586671972, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143963586671972, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.13952347 -0.4920962 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.13952347499568418  |
| -0.4920962015653174  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.163340534376284e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39581428305334665

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.8097731287303825e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5250583587427465e-07  |
| -8.905824823432014e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.111326882270596, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.111326882270596, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.111326882270596, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.111326882270596, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.111326882270596, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.89689192 -0.09962213]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.896891922952392    |
| -0.09962213276537568  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.52056425128626e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.26770936739269435

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00031827665704307293

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00028545976296622064  |
| -3.1707399384064957e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.010802007794817, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.010802007794817, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.010802007794817, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.010802007794817, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.010802007794817, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.00587462 -0.50181235]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| 0.0058746219000482824  |
| -0.5018123505617922    |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4084694862555672e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39966466009154145

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.5241281676817845e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0702920512440435e-08  |
| -1.768451039505418e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.82264258451891, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.82264258451891, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.82264258451891, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.82264258451891, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.82264258451891, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.18297333 -0.48509   ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.18297332760042195  |
| -0.4850899970598732   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.5524278663774355e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3930608600469263

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.037857053366551e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.6536867794314222e-06  |
| -4.384174051445135e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.794212663476959, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.794212663476959, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.794212663476959, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.794212663476959, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.794212663476959, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.21150728 -0.47946195]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.2115072827990616  |
| -0.47946195458337115  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.072815353801508e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3908629297475398

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.0420060445313038e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.2039186713901407e-06  |
| -4.996022547986662e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.870272803872543, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.870272803872543, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.870272803872543, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.870272803872543, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.870272803872543, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.13516882 -0.49269431]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.13516881613995224  |
| -0.49269430868470465  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8201594691336478e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39605023041015996

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.1207115981551555e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -9.624981567966597e-07  |
| -3.5083340781962127e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (3.9887648799226767, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9887648799226767, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9887648799226767, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9887648799226767, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9887648799226767, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.01990241  0.01827076]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -1.019902411414364    |
| 0.018270756019234113  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00013997297819783543

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23804003468281892

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005880228440747169

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0005997259166385363  |
| 1.0743621917825297e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.228092373494506, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.228092373494506, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.228092373494506, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.228092373494506, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.228092373494506, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.77969916 -0.19786429]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.7796991563502331   |
| -0.19786428939738698  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.236243606989025e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.29523921512276935

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00017735596556208284

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.000138284296722437   |
| -3.509241209632896e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.304118999872094, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.304118999872094, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.304118999872094, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.304118999872094, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.304118999872094, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.70339433 -0.25444788]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.7033943294487699  |
| -0.2544478761024038  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.785836056303515e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.31236213993189943

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0001212002215482611

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.525154856498147e-05  |
| -3.083913895609584e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (6.256502992307158, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.256502992307158, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.256502992307158, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.256502992307158, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.256502992307158, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.25613396 0.28710664]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.256133959071093  |
| 0.2871066362430952  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.261639994238096e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1821049409645584

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.9280931508807265e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.702613078429129e-09  |
| 1.9891015201281917e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.683552641208301, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.683552641208301, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.683552641208301, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.683552641208301, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.683552641208301, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.68108703 -0.26988983]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6810870289974957  |
| -0.269889826043368  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.858802484066658e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.31720519574077405

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.216500402855993e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 8.28542645155445e-08  |
| -3.2832108210849106e-08  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.447724251764727, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.447724251764727, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.447724251764727, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.447724251764727, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.447724251764727, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.44439568 -0.40308583]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.44439568025467224  |
| -0.4030858336001586  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.433770024837789e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3622227190314999

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.95825537578471e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.7590315903435595e-07  |
| -1.595516667750489e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.758835667048313, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.758835667048313, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.758835667048313, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.758835667048313, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.758835667048313, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.24701373 -0.47132173]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.2470137294707797  |
| -0.47132172831254593  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.822601784644709e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38770563883898357

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2438822914947502e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.0725600384477777e-06  |
| -5.862687514446757e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.820574508670093, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.820574508670093, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.820574508670093, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.820574508670093, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.820574508670093, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.18504897 -0.48470806]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.1850489717014625  |
| -0.48470805813494167  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.5880279281789237e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39291130962311277

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.131902900989593e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.6898492415057266e-06  |
| -4.426306922215506e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.486912586006225, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.486912586006225, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.486912586006225, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.486912586006225, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.486912586006225, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.51993185 -0.36666508]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.5199318509063744   |
| -0.36666508052007885  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6951897043409796e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.349314024000051

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.852910527121385e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.5231827526492507e-05  |
| -1.779392829183701e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.661115785804077, array([5.00494875, 0.99635408])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00494875 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494885 0.99635408]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.661115785804077, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.661115785804077, array([5.00494885, 0.99635408])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00494875 0.99635418]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.661115785804077, array([5.00494875, 0.99635408]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.661115785804077, array([5.00494875, 0.99635418])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.65856807 -0.28497364]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6585680711879149  |
| -0.28497364468194064  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.382530251600801e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.32200842011375774

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3609986503000635e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.963102560174683e-08  |
| -3.878487457832111e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| -0.00027151266506267194  |
| -3.26726703258046e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                       â€”
| 1.1543843052921765e-07  |
| -6.559386166026733e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                      â€”
| 2.412158241669522e-07  |
| -3.578102806847255e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                       â€”
| -5.300620176868695e-07  |
| -2.8599745914262752e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                   â€”
| 5.340797169737e-11  |
| 5.322667539261468e-11  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| 1.4031244561337077e-10  |
| 1.2209569854025832e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                      â€”
| 2.133125406493707e-09  |
| 1.0152394920964199e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                        â€”
| -4.0990918703726726e-05  |
| -2.2662644227645637e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                      â€”
| 2.674409692128807e-07  |
| -6.75710749315171e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| -0.0014961893758742672  |
| 0.00023969521606629206  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                     â€”
| 1.97629616937149e-07  |
| -1.202097308253331e-06  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                       â€”
| -8.538420200280958e-07  |
| -3.354978389749269e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                        â€”
| -0.00045243020857947366  |
| -1.3290330701886058e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| 2.6735800785380604e-07  |
| -6.066019158532778e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                        â€”
| -1.7808247968986483e-07  |
| -2.2213680913552274e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| 1.8824498591011237e-08  |
| 1.222742915580183e-09   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                       â€”
| 1.5716680025629594e-08  |
| 1.663724657803044e-09   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| -6.04839302664698e-05   |
| -2.700609589853423e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                       â€”
| 1.5706196237373395e-08  |
| 1.6649366953465733e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                       â€”
| 2.5250583587427465e-07  |
| -8.905824823432014e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                        â€”
| -0.00028545976296622064  |
| -3.1707399384064957e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                       â€”
| 2.0702920512440435e-08  |
| -1.768451039505418e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                        â€”
| -1.6536867794314222e-06  |
| -4.384174051445135e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                        â€”
| -2.2039186713901407e-06  |
| -4.996022547986662e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                       â€”
| -9.624981567966597e-07  |
| -3.5083340781962127e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                       â€”
| -0.0005997259166385363  |
| 1.0743621917825297e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                       â€”
| -0.000138284296722437   |
| -3.509241209632896e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| -8.525154856498147e-05  |
| -3.083913895609584e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                      â€”
| 8.702613078429129e-09  |
| 1.9891015201281917e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                     â€”
| 8.28542645155445e-08  |
| -3.2832108210849106e-08  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                       â€”
| 1.7590315903435595e-07  |
| -1.595516667750489e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                        â€”
| -3.0725600384477777e-06  |
| -5.862687514446757e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                        â€”
| -1.6898492415057266e-06  |
| -4.426306922215506e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                       â€”
| -2.5231827526492507e-05  |
| -1.779392829183701e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                      â€”
| 8.963102560174683e-08  |
| -3.878487457832111e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-3.46493299e-03  1.97008670e-06]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.0049834  0.99635406]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.00494875 0.99635408]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.0049834  0.99635406]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 3              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.0049834  0.99635406]

[3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 300

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [4.3569966403813005, 4.649149768708871, 5.143963586671972, 3.7944390494735445, 5.215515562044457, 4.536066331415189, 5.9869670759644285, 5.548405292854267, 4.895052026654668, 5.310447194589536, 5.186306410333221, 5.602419780339141, 5.234388006882316, 5.638997507425143, 3.9614236173154596, 5.362768994549103, 5.143890922660086, 7.405879356715559, 4.53195970305815, 5.352430620406992, 4.695996151221829, 6.737748945656159, 5.188899709528759, 5.03554097046952, 5.4722858473660745, 4.062894541813671, 5.37414252802516, 4.515629132401616, 7.241251340705833, 6.934783121380934, 4.2400444120983325, 6.0186986796766915, 6.539771633182656, 6.129881283109167, 3.8344716745102545]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.3569966403813005, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3569966403813005, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3569966403813005, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3569966403813005, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3569966403813005, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.65035798 -0.29034692]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6503579830230422  |
| -0.2903469176196438  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.011014513413257e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3237369757918135

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.300805093544704e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -6.0488528411281705e-05  |
| -2.7004600902917876e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.649149768708871, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.649149768708871, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.649149768708871, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.649149768708871, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.649149768708871, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.35713578 -0.43805666]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3571357820586485  |
| -0.43805665650431536  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.079057654528163e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3750662195065965

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1540350035138455e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -7.692829755616208e-06  |
| -9.435893716325363e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.143963586671972, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143963586671972, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143963586671972, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143963586671972, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143963586671972, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.1394887  -0.49210106]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.13948870059010687  |
| -0.4921010621217192  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.163340534376284e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3958162001800972

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.8097643631354522e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5244167938804655e-07  |
| -8.905869652889929e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.7944390494735445, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7944390494735445, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7944390494735445, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7944390494735445, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7944390494735445, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.21497414  0.23625132]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.2149741368538969  |
| 0.2362513162879054  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0002981772697793619

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.19156995486835388

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0015564928748052801

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0018910985870857857  |
| 0.0003677234904654934  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.215515562044457, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.215515562044457, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.215515562044457, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.215515562044457, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.215515562044457, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.2113025  -0.47950523]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.21130250438261555  |
| -0.4795052344075401  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.944895482175327e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39087978485897334

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.265068103728979e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6731205853249973e-07  |
| -6.066067776200663e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.536066331415189, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.536066331415189, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.536066331415189, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.536066331415189, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.536066331415189, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.47063303 -0.39108193]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.47063302588412625  |
| -0.3910819312835656  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3580339791194019e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3579162893741933

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.794278213751843e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.7857126367842475e-05  |
| -1.4838736516612284e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.9869670759644285, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9869670759644285, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9869670759644285, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9869670759644285, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9869670759644285, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.98557698 -0.01614862]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9855769822486593  |
| -0.016148624659706456  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.569514408906515e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2463449460033727

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.6667948807103077e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6283316508066385e-08  |
| -4.3065069573017414e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.548405292854267, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.548405292854267, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.548405292854267, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.548405292854267, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.548405292854267, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.54541037 -0.35309337]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5454103724211734  |
| -0.35309336787747725  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.242812132053538e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3446223111646336

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.39183937458877e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.3045340040660877e-07  |
| -8.445426201955077e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.895052026654668, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.895052026654668, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.895052026654668, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.895052026654668, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.895052026654668, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.1103337  -0.49574286]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.11033369773372215  |
| -0.4957428589857926   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.49878559769379e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3972550364671096

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.290129434018328e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -6.94013239678967e-07  |
| -3.1182867490109318e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.310447194589536, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.30658152 -0.45483349]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3065815201708233  |
| -0.45483349198072176  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.000360399116986e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3813883946425106

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.866942049795023e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.4118590527219306e-07  |
| -3.5781487237182474e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.186306410333221, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.186306410333221, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.186306410333221, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.186306410333221, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.186306410333221, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.18198647 -0.48527007]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1819864703378471  |
| -0.4852700707935753  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.756156782893564e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39313138824173893

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4641814301925103e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6646121041495584e-07  |
| -7.105234262841578e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.602419780339141, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.602419780339141, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.602419780339141, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.602419780339141, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.602419780339141, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.59962252 -0.32205602]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5996225183046988  |
| -0.3220560240713155  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.099388456861389e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3341282435631703

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.8254632987074135e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0945889002437419e-07  |
| -5.8790145206981774e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.234388006882316, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.234388006882316, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.234388006882316, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.234388006882316, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.234388006882316, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.23024401 -0.47532346]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.23024401052396115  |
| -0.4753234550403107  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.480560787870209e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3892545598288154

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.151061862920925e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6502509968009577e-07  |
| -5.471267016487106e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.638997507425143, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.638997507425143, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.638997507425143, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.638997507425143, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.638997507425143, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.63633409 -0.29936907]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6363340920856331  |
| -0.2993690650932024  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.965906040634254e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3266602495199384

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.520205181968781e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.673583842519788e-08  |
| -4.551024040758356e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.9614236173154596, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9614236173154596, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9614236173154596, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9614236173154596, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9614236173154596, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.04737852  0.04667116]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -1.0473785216902343   |
| 0.046671158049349515  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00015604329282340324

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23139865976231225

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0006743482999585546

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0007062979255149136  |
| 3.1472616087675857e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.362768994549103, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.362768994549103, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.362768994549103, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.362768994549103, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.362768994549103, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.35909478 -0.43735507]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3590947816878298  |
| -0.437355074378587  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.2693909086436316e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37480413044044386

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.05487166317191e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.174272818034544e-07  |
| -2.6481288465993494e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.143890922660086, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143890922660086, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143890922660086, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143890922660086, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143890922660086, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.13941577 -0.49211123]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.13941577114984227  |
| -0.4921112328748478  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.166018536735166e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3958202110703161

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.8104225949852133e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.524014621869621e-07  |
| -8.909292952426546e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (7.405879356715559, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.405879356715559, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.405879356715559, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.405879356715559, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.405879356715559, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.40968147 2.40145259]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.4096814721374926  |
| 2.401452587896813   |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.909357867014368e-13

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.022152397972162702

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.216174462549652e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.340274541430162e-11  |
| 5.3220378983206906e-11  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.53195970305815, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.53195970305815, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.53195970305815, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.53195970305815, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.53195970305815, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.47475468 -0.38913365]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.47475468001323406  |
| -0.38913364752701796  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3835567750924603e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35722218413894924

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.873098694660845e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.838771731443384e-05   |
| -1.5071530222855065e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.352430620406992, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.352430620406992, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.352430620406992, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.352430620406992, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.352430620406992, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.34871858 -0.44102728]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.34871857512719373  |
| -0.441027284692197   |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.398636245045867e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.376177983155541

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.376333417828138e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.2235459040009377e-07  |
| -2.8121370135568596e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.695996151221829, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.695996151221829, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.695996151221829, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.695996151221829, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.695996151221829, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.31011798 -0.45374306]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3101179768538742  |
| -0.45374305868151055  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.490776042055121e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.380974256937322

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7037308752131737e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.283575721245899e-06  |
| -7.730560584893524e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (6.737748945656159, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.737748945656159, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.737748945656159, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.737748945656159, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.737748945656159, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.73910618 1.01041546]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.739106179066141  |
| 1.0104154579693159  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.5337575110761806e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.08858046780098987

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.247153179986189e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0864462696886672e-09  |
| 6.312220141360212e-10   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.188899709528759, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.188899709528759, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.188899709528759, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.188899709528759, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.188899709528759, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.18458926 -0.48479301]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.18458925721276387  |
| -0.48479301129056296  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.679237722716466e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3929445692767591

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4453025100129225e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.667873167710286e-07  |
| -7.006725560549737e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.03554097046952, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.03554097046952, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.03554097046952, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.03554097046952, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.03554097046952, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.03066933 -0.50135931]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| 0.030669333739297144  |
| -0.5013593107339176   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.2438800814485445e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3994842969561343

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.1137145838429032e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.549555174079472e-08  |
| -1.5610897975776251e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.4722858473660745, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.4722858473660745, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.4722858473660745, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.4722858473660745, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.4722858473660745, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.46901239 -0.3918433 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.4690123867057139  |
| -0.39184329558850095  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.2538275591458426e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3581879038924993

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.5004743195408016e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.641765815209891e-07  |
| -1.371637393491783e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.062894541813671, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.062894541813671, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.062894541813671, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.062894541813671, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.062894541813671, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.94553629 -0.05481028]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9455362870447459   |
| -0.05481027853804221  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00010385710830717312

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.25601946365992795

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00040566098695186346

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.0003835671834013721   |
| -2.2234391686848745e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.37414252802516, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.37414252802516, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.37414252802516, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.37414252802516, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.37414252802516, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.37050993 -0.4331908 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.37050993162424106  |
| -0.4331908010080099  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.1349712103782527e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.373252255142875

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.719915100207552e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.1192853526743638e-07  |
| -2.4778146039567203e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.515629132401616, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.515629132401616, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.515629132401616, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.515629132401616, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.515629132401616, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.49114501 -0.38121794]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4911450091071856  |
| -0.3812179438789087  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4896388396333622e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35441590875883233

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.203081190260591e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.0643223494687786e-05  |
| -1.602289969307259e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (7.241251340705833, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.241251340705833, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.241251340705833, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.241251340705833, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.241251340705833, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.24445104 2.01695045]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.2444510383579086  |
| 2.0169504466593935  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.639222580372072e-12

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0324936946133744

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.044740525435258e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.1322673110559387e-10  |
| 1.0174991656057388e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.934783121380934, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.934783121380934, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.934783121380934, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.934783121380934, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.934783121380934, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.93686136 1.37388624]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                 â€”
| 1.93686136018556  |
| 1.37388624033008  |
 â€”                 â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4389145137504974e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.06166803675812666

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.3333230460930413e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.519323248808082e-10  |
| 3.2057204272722983e-10  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.2400444120983325, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.2400444120983325, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.2400444120983325, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.2400444120983325, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.2400444120983325, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.76773817 -0.20711873]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.7677381730353261   |
| -0.20711873327527996  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.977852750633089e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.297974119776577

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00016705654686942332

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.00012825568808712138  |
| -3.46005403729374e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (6.0186986796766915, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.0174247  0.01574689]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0174247000449554  |
| 0.015746890458245844  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.430115505065784e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23863938042768437

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.2754482078079684e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.3150972102968536e-08  |
| 3.5831233671763903e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.539771633182656, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.539771633182656, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.539771633182656, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.539771633182656, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.539771633182656, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.54040441 0.6845932 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.5404044084732504  |
| 0.68459320434755    |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.0597938184175473e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1225532629285414

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.6807335595940646e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.589009384667636e-09  |
| 1.1506187732169645e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (6.129881283109167, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.129881283109167, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.129881283109167, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.129881283109167, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.129881283109167, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.12901415 0.13550685]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.1290141510578167  |
| 0.135506852405598   |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.7637958179549005e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.21179741087740742

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3049242700868713e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 1.47327796698687e-08  |
| 1.7682618046714438e-09  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (3.8344716745102545, array([5.0049834 , 0.99635406])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.0049834  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049835  0.99635406]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.8344716745102545, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.8344716745102545, array([5.0049835 , 0.99635406])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.0049834  0.99635416]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.8344716745102545, array([5.0049834 , 0.99635406]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.8344716745102545, array([5.0049834 , 0.99635416])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.17479502  0.18824192]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.1747950190432732  |
| 0.18824191982957927  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.000255950870371042

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2009562867499004

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0012736644098603643

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0014962946046366459  |
| 0.00023975703373072308  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                        â€”
| -6.0488528411281705e-05  |
| -2.7004600902917876e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                       â€”
| -7.692829755616208e-06  |
| -9.435893716325363e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| 2.5244167938804655e-07  |
| -8.905869652889929e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                      â€”
| -0.0018910985870857857  |
| 0.0003677234904654934  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                       â€”
| 2.6731205853249973e-07  |
| -6.066067776200663e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                        â€”
| -1.7857126367842475e-05  |
| -1.4838736516612284e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 2.6283316508066385e-08  |
| -4.3065069573017414e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| 1.3045340040660877e-07  |
| -8.445426201955077e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                      â€”
| -6.94013239678967e-07  |
| -3.1182867490109318e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| 2.4118590527219306e-07  |
| -3.5781487237182474e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                       â€”
| 2.6646121041495584e-07  |
| -7.105234262841578e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                       â€”
| 1.0945889002437419e-07  |
| -5.8790145206981774e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                       â€”
| 2.6502509968009577e-07  |
| -5.471267016487106e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                      â€”
| 9.673583842519788e-08  |
| -4.551024040758356e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                       â€”
| -0.0007062979255149136  |
| 3.1472616087675857e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                      â€”
| 2.174272818034544e-07  |
| -2.6481288465993494e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                      â€”
| 2.524014621869621e-07  |
| -8.909292952426546e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                      â€”
| 5.340274541430162e-11  |
| 5.3220378983206906e-11  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                        â€”
| -1.838771731443384e-05   |
| -1.5071530222855065e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                       â€”
| 2.2235459040009377e-07  |
| -2.8121370135568596e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                       â€”
| -5.283575721245899e-06  |
| -7.730560584893524e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                       â€”
| 1.0864462696886672e-09  |
| 6.312220141360212e-10   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                      â€”
| 2.667873167710286e-07  |
| -7.006725560549737e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                      â€”
| 9.549555174079472e-08  |
| -1.5610897975776251e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                      â€”
| 1.641765815209891e-07  |
| -1.371637393491783e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                        â€”
| -0.0003835671834013721   |
| -2.2234391686848745e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                       â€”
| 2.1192853526743638e-07  |
| -2.4778146039567203e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| -2.0643223494687786e-05  |
| -1.602289969307259e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                       â€”
| 1.1322673110559387e-10  |
| 1.0174991656057388e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                      â€”
| 4.519323248808082e-10  |
| 3.2057204272722983e-10  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| -0.00012825568808712138  |
| -3.46005403729374e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                       â€”
| 2.3150972102968536e-08  |
| 3.5831233671763903e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                      â€”
| 2.589009384667636e-09  |
| 1.1506187732169645e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                     â€”
| 1.47327796698687e-08  |
| 1.7682618046714438e-09  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| -0.0014962946046366459  |
| 0.00023975703373072308  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.00473343  0.00048151]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.00503074 0.99634924]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.0049834  0.99635406]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.00503074 0.99634924]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 4              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.00503074 0.99634924]

[4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 400

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [6.402353722870568, 4.99361706657747, 4.736229525144517, 3.2136246945061395, 5.5079948041310836, 5.069264393369226, 5.9193141612464135, 5.497622685783328, 5.010802007794817, 4.611942257619824, 4.383481509023446, 5.8119416483858455, 5.499094833641162, 5.286197399318152, 6.934783121380934, 4.913065034204201, 5.905524370638401, 4.903301032798097, 6.011633478419664, 5.288946216263123, 2.7920118172842194, 4.8821063563572915, 4.04546333767802, 5.262123257167052, 5.772940924220628, 5.545460014797378, 6.0186986796766915, 4.958388622977857, 5.55177578013804, 4.013069392924608, 6.223400843230536, 5.543861664243698, 4.765757417029921, 5.945935256321971, 4.895052026654668]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.402353722870568, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.402353722870568, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.402353722870568, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.402353722870568, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.402353722870568, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.40244292 0.48159099]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4024429173531416  |
| 0.481590991530112   |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.012053946073478e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.15002659381038835

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.3407770041143376e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| 4.6852490478764e-09  |
| 1.6088881098924209e-09  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.99361706657747, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.99361706657747, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.99361706657747, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.99361706657747, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.99361706657747, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.01145554 -0.50176643]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.011455542070493152  |
| -0.5017664284068246    |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5349016709168196e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3996463737047309

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.840649563983895e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.399672265823869e-08  |
| -1.9271090144824272e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.736229525144517, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.736229525144517, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.736229525144517, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.736229525144517, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.736229525144517, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.26978619 -0.46543977]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.26978618783424224  |
| -0.4654397656178588   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.368974743713434e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3854401948941865

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3929462507633276e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.7579765885143862e-06  |
| -6.483325764735584e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.2136246945061395, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.2136246945061395, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.2136246945061395, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.2136246945061395, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.2136246945061395, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.79797005  1.11451585]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.7979700528414355  |
| 1.114515848499309  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0022821658064853113

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.07985386726577953

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.028579277180020905

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.05138468450153222  |
| 0.03185205735578794  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.5079948041310836, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.5079948041310836, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.5079948041310836, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.5079948041310836, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.5079948041310836, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.50480695 -0.374417  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5048069451341064  |
| -0.37441700406759537  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0306129646937287e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35202266962344

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.9276891905745143e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.477917836596065e-07  |
| -1.096176615575993e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.069264393369226, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.069264393369226, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.069264393369226, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.069264393369226, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.069264393369226, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.06446896 -0.49975392]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.06446896461120843  |
| -0.4997539171380794  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0490146340768267e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3988458194153563

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.6301256851946278e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.6956147972184278e-07  |
| -1.3144156137414902e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.9193141612464135, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9193141612464135, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9193141612464135, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9193141612464135, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9193141612464135, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.91763343 -0.08080648]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9176334314631163  |
| -0.08080648372654764  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.82752609854995e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2627378915155145

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.7404296890194236e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 3.4323433306814105e-08  |
| -3.022509707960437e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.497622685783328, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.497622685783328, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.497622685783328, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.497622685783328, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.497622685783328, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.49439682 -0.37961792]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.49439682348051406  |
| -0.37961792154206364  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0911470912064195e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35385156108787225

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.08362944012971e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.5245365999911248e-07  |
| -1.1706009988679579e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.010802007794817, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.010802007794817, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.010802007794817, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.010802007794817, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.010802007794817, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.00579237 -0.50181527]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| 0.005792366586376829  |
| -0.5018152671176779   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4084694862555672e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3996658208985355

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.5241179320488858e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0412982956051375e-08  |
| -1.7684561814253101e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.611942257619824, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.611942257619824, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.611942257619824, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.611942257619824, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.611942257619824, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.39452886 -0.42400556]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3945288618023568  |
| -0.42400556066191086  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.59811121760208e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3698520643998187

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.595121709859188e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.0238504144293314e-05  |
| -1.1003460355747419e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.383481509023446, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.383481509023446, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.383481509023446, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.383481509023446, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.383481509023446, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.62382672 -0.30725221]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.623826721390941    |
| -0.30725220589999935  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.6819170656107977e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3292363797138909

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.145870963413598e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -5.08161197597997e-05    |
| -2.5028368224855807e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.8119416483858455, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.8119416483858455, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.8119416483858455, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.8119416483858455, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.8119416483858455, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.80986749 -0.17388936]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.809867493067884  |
| -0.17388935846796016  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.8448709857535823e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.28827070536514593

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.399786559708613e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.182979097280752e-08  |
| -1.1128547791996045e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.499094833641162, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.499094833641162, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.499094833641162, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.499094833641162, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.499094833641162, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.49587436 -0.37888634]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.49587436379283645  |
| -0.3788863400799869  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0823505627144098e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3535937281096196

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.0610004552424195e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.5178716533129176e-07  |
| -1.159771259469974e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.286197399318152, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.286197399318152, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.286197399318152, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.286197399318152, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.286197399318152, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.28219684 -0.4620145 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2821968447808132  |
| -0.4620145022116162  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.4117168698116967e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3841270225578618

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.881741375791345e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5063993924075166e-07  |
| -4.103493320508554e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (6.934783121380934, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.934783121380934, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.934783121380934, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.934783121380934, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.934783121380934, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.93682321 1.37380993]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.9368232129224339  |
| 1.3738099324811515  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4389145137504974e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.06167328270202135

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.333124573087362e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 4.5188498317953457e-10  |
| 3.2052697122232637e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.913065034204201, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.913065034204201, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.913065034204201, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.913065034204201, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.913065034204201, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.09230273 -0.49757215]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.0923027287935696  |
| -0.49757215236923    |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.287519424233396e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39797974998207747

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.747828688108909e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.305402725504156e-07  |
| -2.8599594917919574e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.905524370638401, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.905524370638401, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.905524370638401, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.905524370638401, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.905524370638401, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.90379312 -0.09341104]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.903793115902829  |
| -0.09341104112436938  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0662343740308533e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2660583035480374

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.007521508676922e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.621970351374721e-08  |
| -3.743467564538148e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.903301032798097, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.903301032798097, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.903301032798097, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.903301032798097, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.903301032798097, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.10210251 -0.49661959]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.10210250755982031  |
| -0.4966195887856628   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.399814675042673e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3976022121557607

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.035717613418472e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -6.162619032530002e-07  |
| -2.997455599202264e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.011633478419664, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.011633478419664, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.011633478419664, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.011633478419664, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.011633478419664, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.01029102 0.00851193]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0102910197318238  |
| 0.008511926719023677  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.665860407941348e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24036643850926023

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.3571761694688785e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.3814339159402677e-08  |
| 2.006411081834803e-10   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.288946216263123, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.288946216263123, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.288946216263123, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.288946216263123, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.288946216263123, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.28495573 -0.46123215]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.28495573234366134  |
| -0.4612321469199543  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.3624876716304964e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3838277124287617

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.760408805173423e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.4963287067080523e-07  |
| -4.0405821611066096e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (2.7920118172842194, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.7920118172842194, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.7920118172842194, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.7920118172842194, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.7920118172842194, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.22112778  1.96487188]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.221127779478138  |
| 1.9648718829756717  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00809423234481343

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.03422466656502154

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.23650288394879315

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.5253031254653587  |
| 0.46469786691364195  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.8821063563572915, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.8821063563572915, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.8821063563572915, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.8821063563572915, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.8821063563572915, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.12337484 -0.49422137]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.12337484367463958  |
| -0.4942213738257095   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.6620347856315617e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3966532922517254

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.711238347524348e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.279979819890628e-07  |
| -3.3168374361852677e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.04546333767802, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.04546333767802, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.04546333767802, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.04546333767802, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.04546333767802, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.96308344 -0.03806729]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9630834396645582   |
| -0.03806728710031848  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00011146216617589817

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2517845483072642

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00044268866745498527

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.000426346124553067   |
| -1.685195660006634e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.262123257167052, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.262123257167052, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.262123257167052, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.262123257167052, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.262123257167052, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.25803449 -0.46854113]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2580344915514843  |
| -0.4685411325588973  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.873608613899683e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3866330646819508

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.001882396448985e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5852021476209645e-07  |
| -4.694231127230296e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.772940924220628, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.772940924220628, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.772940924220628, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.772940924220628, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.772940924220628, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.77072386 -0.2048244 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7707238647292058  |
| -0.20482439744284875  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.312475864731312e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.29729416212028004

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.7784099366059e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.995006167788956e-08  |
| -1.593208128328771e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.545460014797378, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.545460014797378, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.545460014797378, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.545460014797378, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.545460014797378, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.54240943 -0.35472803]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5424094329242735  |
| -0.35472803361358274  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.378583132147346e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.34518430193933874

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4272781482455027e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.3165785639393237e-07  |
| -8.610236045603455e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (6.0186986796766915, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.01738211 0.01570113]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0173821118897308  |
| 0.015701133726508942  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.430115505065784e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2386508550567841

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.2753388014359945e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.314888995069601e-08  |
| 3.5725398794461125e-10  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.958388622977857, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.958388622977857, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.958388622977857, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.958388622977857, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.958388622977857, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.04681307 -0.50073631]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.04681306897680315  |
| -0.5007363135245413   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.828988636664514e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39923640528076726

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.581217074575797e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.1446083090982514e-07  |
| -2.2939817493787683e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.55177578013804, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.55177578013804, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.55177578013804, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.55177578013804, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.55177578013804, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.54874834 -0.35126966]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5487483401012128  |
| -0.35126966002607674  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.090051154959938e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3439969305034723

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.3517800415019335e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.2905353940573472e-07  |
| -8.261089756344969e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.013069392924608, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.013069392924608, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.013069392924608, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.013069392924608, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.013069392924608, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.99559608 -0.00622637]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.9955960789120866    |
| -0.006226370530271197  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00012700278921566076

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2439221406011374

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005206693779525996

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0005183763910992034  |
| -3.241880470898702e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.223400843230536, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.223400843230536, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.223400843230536, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.223400843230536, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.223400843230536, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.22283433 0.24582984]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.2228343293507749  |
| 0.24582983870757857  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5511060792876206e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1897510672111461

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.174426115673027e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.995968876986486e-09  |
| 2.0095178535429184e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.543861664243698, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.543861664243698, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.543861664243698, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.543861664243698, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.543861664243698, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.54080523 -0.35559689]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5408052272670716  |
| -0.3555968852708702  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.453166751147758e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.345483249929844

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4467660162581864e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.323223851491856e-07  |
| -8.700623743680264e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.765757417029921, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.765757417029921, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.765757417029921, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.765757417029921, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.765757417029921, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.2401501  -0.47299602]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.24015010202660392  |
| -0.4729960223492924   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.666223843526028e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3883529980565507

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2015418618827163e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.8855040072036995e-06  |
| -5.6832452135668764e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.945935256321971, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.945935256321971, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.945935256321971, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.945935256321971, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.945935256321971, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.94435207 -0.05593163]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9443520698759755  |
| -0.05593162821782016  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.391793529644296e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2563062137639399

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.2741280074361385e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.091929560861221e-08  |
| -1.8312731044947042e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.895052026654668, array([5.00503074, 0.99634924])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.00503074 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503084 0.99634924]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.895052026654668, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.895052026654668, array([5.00503084, 0.99634924])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.00503074 0.99634934]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.895052026654668, array([5.00503074, 0.99634924]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.895052026654668, array([5.00503074, 0.99634934])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.11038174 -0.49573998]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.11038173930444373  |
| -0.49573998350815884  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.49878559769379e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3972539095996468

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.29014727686902e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -6.943173969019128e-07  |
| -3.1182775072989384e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                    â€”
| 4.6852490478764e-09  |
| 1.6088881098924209e-09  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                       â€”
| -4.399672265823869e-08  |
| -1.9271090144824272e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                        â€”
| -3.7579765885143862e-06  |
| -6.483325764735584e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                    â€”
| -0.05138468450153222  |
| 0.03185205735578794  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                      â€”
| 1.477917836596065e-07  |
| -1.096176615575993e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| 1.6956147972184278e-07  |
| -1.3144156137414902e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 3.4323433306814105e-08  |
| -3.022509707960437e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| 1.5245365999911248e-07  |
| -1.1706009988679579e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| 2.0412982956051375e-08  |
| -1.7684561814253101e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                        â€”
| -1.0238504144293314e-05  |
| -1.1003460355747419e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                        â€”
| -5.08161197597997e-05    |
| -2.5028368224855807e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                      â€”
| 5.182979097280752e-08  |
| -1.1128547791996045e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                       â€”
| 1.5178716533129176e-07  |
| -1.159771259469974e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| 2.5063993924075166e-07  |
| -4.103493320508554e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                       â€”
| 4.5188498317953457e-10  |
| 3.2052697122232637e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| -5.305402725504156e-07  |
| -2.8599594917919574e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                      â€”
| 3.621970351374721e-08  |
| -3.743467564538148e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| -6.162619032530002e-07  |
| -2.997455599202264e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                       â€”
| 2.3814339159402677e-08  |
| 2.006411081834803e-10   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                       â€”
| 2.4963287067080523e-07  |
| -4.0405821611066096e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                    â€”
| -0.5253031254653587  |
| 0.46469786691364195  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                       â€”
| -8.279979819890628e-07  |
| -3.3168374361852677e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| -0.000426346124553067   |
| -1.685195660006634e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                       â€”
| 2.5852021476209645e-07  |
| -4.694231127230296e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                      â€”
| 5.995006167788956e-08  |
| -1.593208128328771e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                       â€”
| 1.3165785639393237e-07  |
| -8.610236045603455e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                      â€”
| 2.314888995069601e-08  |
| 3.5725398794461125e-10  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                        â€”
| -2.1446083090982514e-07  |
| -2.2939817493787683e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                       â€”
| 1.2905353940573472e-07  |
| -8.261089756344969e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                       â€”
| -0.0005183763910992034  |
| -3.241880470898702e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| 9.995968876986486e-09  |
| 2.0095178535429184e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                      â€”
| 1.323223851491856e-07  |
| -8.700623743680264e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                        â€”
| -2.8855040072036995e-06  |
| -5.6832452135668764e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                      â€”
| 3.091929560861221e-08  |
| -1.8312731044947042e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| -6.943173969019128e-07  |
| -3.1182775072989384e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.57770109  0.49646012]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.01080775 0.99138464]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.00503074 0.99634924]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.01080775 0.99138464]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 5              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.01080775 0.99138464]

[4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 500

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [3.509387105833827, 3.967421844437092, 5.077373356563462, 4.6519615102397385, 3.9904471584232315, 5.499094833641162, 5.945935256321971, 4.695996151221829, 5.197374169448914, 5.86539785051527, 4.013069392924608, 4.093561787638524, 6.237838483577062, 4.5206883397532, 6.737748945656159, 5.731481560043303, 5.38886877752516, 5.046460597885442, 3.9116393763485187, 5.22191806826484, 4.8821063563572915, 2.784718036601556, 6.934783121380934, 3.3920732473675006, 5.099009240601548, 4.828453374610873, 4.789511181850781, 5.2710888266117655, 5.656456978387318, 5.99799053118142, 5.934944323484632, 4.580613659052775, 6.246103631662809, 5.456232480847415, 5.215515562044457]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.509387105833827, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.509387105833827, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.509387105833827, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.509387105833827, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.509387105833827, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.51446839  0.64246196]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.514468386254464  |
| 0.6424619591882674  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0008444395015244974

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.12853690737140674

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.006569626722731812

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.00994949198106985  |
| 0.004220735255421877  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.967421844437092, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.967421844437092, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.967421844437092, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.967421844437092, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.967421844437092, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.05245322  0.04948369]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.052453220129479   |
| 0.04948369047852452  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00015237643097555995

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2313856245300324

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0006585388841033314

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0006930813691550249  |
| 3.2586934309042184e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (5.077373356563462, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.077373356563462, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.077373356563462, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.077373356563462, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.077373356563462, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.06714403 -0.50209093]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.06714402811525133  |
| -0.5020909255026851  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0067344992297686e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3997775812804518

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.51823650542206e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.6908454272091105e-07  |
| -1.2643836976420096e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.6519615102397385, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6519615102397385, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6519615102397385, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6519615102397385, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6519615102397385, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.36196475 -0.43883588]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3619647459540687  |
| -0.4388358754159327  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.974102302486762e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3754773891709509

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.123723700139035e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -7.687131095974606e-06  |
| -9.319661490920772e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.9904471584232315, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9904471584232315, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9904471584232315, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9904471584232315, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9904471584232315, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.02922781  0.02530975]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -1.0292278118662068   |
| 0.025309745410595497  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0001390366771447839

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23699792063066585

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005866577933460296

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0006038045169597914  |
| 1.4848159392729754e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (5.499094833641162, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.499094833641162, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.499094833641162, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.499094833641162, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.499094833641162, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.49253036 -0.383052  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.4925303631608813  |
| -0.38305199900889875  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0823505627144098e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35527600619402544

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.046506220077553e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.500496814946681e-07  |
| -1.1669702975937508e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.945935256321971, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.945935256321971, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.945935256321971, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.945935256321971, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.945935256321971, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.94325393 -0.0594811 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9432539327391964  |
| -0.05948110004538876  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.391793529644296e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2577813429817282

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.255392121313882e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 3.0706614210375145e-08  |
| -1.9363430445484133e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.695996151221829, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.695996151221829, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.695996151221829, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.695996151221829, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.695996151221829, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.31754743 -0.45392692]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3175474339034423  |
| -0.45392692382684174  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.490776042055121e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3811371519890959

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.703002713899908e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.407841417295139e-06  |
| -7.730387831893483e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (5.197374169448914, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.197374169448914, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.197374169448914, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.197374169448914, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.197374169448914, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.18818768 -0.48663778]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1881876754428191  |
| -0.48663778118296364  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.434719933802147e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3936996597471756

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3804228170509959e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5977856106905487e-07  |
| -6.717658967840328e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.86539785051527, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.86539785051527, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.86539785051527, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.86539785051527, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.86539785051527, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.86201664 -0.13280875]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8620166358497272  |
| -0.13280874622267902  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3502587467348287e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.27721894961622373

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.870730332843766e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 4.1986505756492055e-08  |
| -6.468755886937527e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.013069392924608, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.013069392924608, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.013069392924608, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.013069392924608, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.013069392924608, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.00640899  0.00208433]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -1.0064089850381208   |
| 0.002084326045093121  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00012700278921566076

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2425181838827949

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005236835736698373

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0005270398538581969  |
| 1.091527311987484e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (4.093561787638524, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.093561787638524, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.093561787638524, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.093561787638524, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.093561787638524, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.92521709 -0.07633185]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9252170918294667   |
| -0.07633184750943656  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.164734180973534e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2621238999736717

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0003496336725454665

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.0003234870497181726   |
| -2.6688184176904823e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.237838483577062, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.237838483577062, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.237838483577062, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.237838483577062, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.237838483577062, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.23769387 0.26159794]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.2376938651748048  |
| 0.26159794330737896  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4176673791022746e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.18750375160767022

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.560741408889666e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.357883257955851e-09  |
| 1.9778744024444714e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.5206883397532, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5206883397532, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5206883397532, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5206883397532, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5206883397532, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.49437871 -0.38213997]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4943787135225364  |
| -0.3821399729986297  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4559744465517916e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35495492180402827

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.101857326423099e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.0278709480900422e-05  |
| -1.5674836479635543e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (6.737748945656159, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.737748945656159, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.737748945656159, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.737748945656159, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.737748945656159, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.74194867 1.01284741]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.7419486653125205  |
| 1.0128474103865415  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.5337575110761806e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.08903429152844203

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.215310321538775e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 1.08267515191076e-09  |
| 6.295160963919291e-10  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.731481560043303, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.731481560043303, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.731481560043303, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.731481560043303, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.731481560043303, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.72693658 -0.24012668]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7269365820405937  |
| -0.24012668298212247  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.935276844925096e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3083393594125268

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.519630742301675e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 6.92016783409734e-08  |
| -2.2859173533635417e-08  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.38886877752516, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.38886877752516, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.38886877752516, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.38886877752516, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.38886877752516, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.38134642 -0.43163253]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3813464177770953  |
| -0.4316325330488979  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9723066913025383e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37280555014281

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.290443477965953e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0174916687745134e-07  |
| -2.2835275193464657e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (5.046460597885442, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.046460597885442, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.046460597885442, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.046460597885442, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.046460597885442, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.03596263 -0.50369843]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.0359626295409754  |
| -0.5036984318529392  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.177260043884164e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4004151976221558

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.9400983051474063e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0573366616206582e-07  |
| -1.4809229057962328e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.9116393763485187, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9116393763485187, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9116393763485187, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9116393763485187, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9116393763485187, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.10872045  0.11028531]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.108720451004075   |
| 0.11028530755297083  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0001898265422129378

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.21785024806032027

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0008713625249597007

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0009660974516613689  |
| 9.609848405531382e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.22191806826484, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.22191806826484, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.22191806826484, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.22191806826484, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.22191806826484, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.21294487 -0.48167232]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.21294486729495077  |
| -0.48167232313822694  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.78240214907098e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3917663649107564

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2207281118072506e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.599477857720108e-07  |
| -5.879909455343396e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (4.8821063563572915, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.8821063563572915, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.8821063563572915, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.8821063563572915, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.8821063563572915, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.12981989 -0.4959185 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.12981988817273304  |
| -0.4959184951580653   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.6620347856315617e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3973387104495465

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.699661310673084e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.697492821467659e-07  |
| -3.322485955257707e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (2.784718036601556, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.784718036601556, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.784718036601556, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.784718036601556, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.784718036601556, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.245435   2.0166437]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.2454350023792813  |
| 2.0166436964785817  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.008260535980354262

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.03291318301772786

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.2509795535699155

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.5635582744674142  |
| 0.5061363346517787  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.934783121380934, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.934783121380934, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.934783121380934, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.934783121380934, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.934783121380934, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.94069511 1.37880356]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.940695111279922  |
| 1.378803564655584  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4389145137504974e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.061943225265813556

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.3229570426398734e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 4.5081513763644675e-10  |
| 3.2029014509336513e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (3.3920732473675006, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3920732473675006, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3920732473675006, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3920732473675006, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3920732473675006, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.63280173  0.82867543]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.6328017338196332  |
| 0.828675430497583  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0012658404488615296

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.10686933135615106

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.011844749403764954

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.019340127363126482  |
| 0.009815452811300912  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.099009240601548, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.099009240601548, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.099009240601548, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.099009240601548, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.099009240601548, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.08896793 -0.50038744]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.08896793080026555  |
| -0.5003874381515061  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.017882044292259e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3991030021955333

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.2595375115404696e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0102637696733672e-07  |
| -1.1306441868069647e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.828453374610873, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.828453374610873, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.828453374610873, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.828453374610873, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.828453374610873, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.18393913 -0.4874283 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.18393912837311177  |
| -0.48742830105474866  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.4542005092747025e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39400832607960595

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.766821106660602e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.6125614329621503e-06  |
| -4.273196717670489e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (4.789511181850781, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.789511181850781, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.789511181850781, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.789511181850781, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.789511181850781, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.22321974 -0.47943158]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.22321973602856815  |
| -0.47943157666097136  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.165612585780475e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39089704519104457

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.065654662020942e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.3787515235392767e-06  |
| -5.1090849478881476e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.2710888266117655, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.2710888266117655, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.2710888266117655, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.2710888266117655, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.2710888266117655, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.26254293 -0.46988068]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2625429307290261  |
| -0.46988068436171204  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.6949551220506466e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38721326129189504

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.54242917642549e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5052973222529153e-07  |
| -4.483803151891978e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.656456978387318, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.656456978387318, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.656456978387318, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.656456978387318, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.656456978387318, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.65126002 -0.29227527]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6512600214669817  |
| -0.2922752706524534  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.499604196499292e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3246995548397509

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3857746736733234e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.024996437248882e-08  |
| -4.050276678111859e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (5.99799053118142, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.99799053118142, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.99799053118142, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.99799053118142, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.99799053118142, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.99576158 -0.00857453]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9957615776379214  |
| -0.00857452997493624  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.149569612552109e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.245094468665294

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.509060953533834e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.4984264934805577e-08  |
| -2.1514018355017964e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.934944323484632, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.934944323484632, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.934944323484632, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.934944323484632, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.934944323484632, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.93216748 -0.06987698]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9321674832030169  |
| -0.06987697753757516  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.9579843640522e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.26045185688702865

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.439401227973482e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.206097986405407e-08  |
| -2.4033496234981145e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.580613659052775, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.580613659052775, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.580613659052775, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.580613659052775, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.580613659052775, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.43393263 -0.41019636]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4339326276081579  |
| -0.4101963591018887  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.1084650106842131e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3649664655454289

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.037169480838885e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.317926933311722e-05   |
| -1.2458358630154841e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61a20>

args = (6.246103631662809, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.246103631662809, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.246103631662809, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.246103631662809, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.246103631662809, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.24603084 0.27195132]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.246030838508716  |
| 0.2719513170212906  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3463833735593352e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.18558902334676242

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.2546498132257274e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.039517389860753e-09  |
| 1.9729115712349965e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.456232480847415, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.456232480847415, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.456232480847415, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.456232480847415, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.456232480847415, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.44929553 -0.40341184]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.44929552744932266  |
| -0.4034118417095556  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3687811539857475e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.36251991418137813

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.775740588145761e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.6964233590627655e-07  |
| -1.523178464481402e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.215515562044457, array([5.01080775, 0.99138464])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01080775 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080785 0.99138464]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.215515562044457, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.215515562044457, array([5.01080785, 0.99138464])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01080775 0.99138474]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.215515562044457, array([5.01080775, 0.99138464]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.215515562044457, array([5.01080775, 0.99138474])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.20648672 -0.4830267 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.20648671883449765  |
| -0.4830266997490895  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.944895482175327e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3922927456564152

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2605115788978296e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6027889997950485e-07  |
| -6.088607479505327e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                     â€”
| -0.00994949198106985  |
| 0.004220735255421877  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                       â€”
| -0.0006930813691550249  |
| 3.2586934309042184e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| 1.6908454272091105e-07  |
| -1.2643836976420096e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                       â€”
| -7.687131095974606e-06  |
| -9.319661490920772e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                       â€”
| -0.0006038045169597914  |
| 1.4848159392729754e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                      â€”
| 1.500496814946681e-07  |
| -1.1669702975937508e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 3.0706614210375145e-08  |
| -1.9363430445484133e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| -5.407841417295139e-06  |
| -7.730387831893483e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| 2.5977856106905487e-07  |
| -6.717658967840328e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| 4.1986505756492055e-08  |
| -6.468755886937527e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                      â€”
| -0.0005270398538581969  |
| 1.091527311987484e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                        â€”
| -0.0003234870497181726   |
| -2.6688184176904823e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                      â€”
| 9.357883257955851e-09  |
| 1.9778744024444714e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                        â€”
| -2.0278709480900422e-05  |
| -1.5674836479635543e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                     â€”
| 1.08267515191076e-09  |
| 6.295160963919291e-10  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                     â€”
| 6.92016783409734e-08  |
| -2.2859173533635417e-08  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                       â€”
| 2.0174916687745134e-07  |
| -2.2835275193464657e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| 1.0573366616206582e-07  |
| -1.4809229057962328e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                      â€”
| -0.0009660974516613689  |
| 9.609848405531382e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                      â€”
| 2.599477857720108e-07  |
| -5.879909455343396e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                       â€”
| -8.697492821467659e-07  |
| -3.322485955257707e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                   â€”
| -0.5635582744674142  |
| 0.5061363346517787  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| 4.5081513763644675e-10  |
| 3.2029014509336513e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                     â€”
| -0.019340127363126482  |
| 0.009815452811300912  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                       â€”
| 2.0102637696733672e-07  |
| -1.1306441868069647e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                        â€”
| -1.6125614329621503e-06  |
| -4.273196717670489e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                        â€”
| -2.3787515235392767e-06  |
| -5.1090849478881476e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| 2.5052973222529153e-07  |
| -4.483803151891978e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                      â€”
| 9.024996437248882e-08  |
| -4.050276678111859e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                       â€”
| 2.4984264934805577e-08  |
| -2.1514018355017964e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| 3.206097986405407e-08  |
| -2.4033496234981145e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                        â€”
| -1.317926933311722e-05   |
| -1.2458358630154841e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                      â€”
| 9.039517389860753e-09  |
| 1.9729115712349965e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                       â€”
| 1.6964233590627655e-07  |
| -1.523178464481402e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| 2.6027889997950485e-07  |
| -6.088607479505327e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.59601048  0.52022581]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.01676785 0.98618238]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.01080775 0.99138464]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.01676785 0.98618238]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 6              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.01676785 0.98618238]

[4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 600

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [5.72081756683573, 5.346106766030754, 3.967421844437092, 4.789511181850781, 4.3859933966975175, 3.53316674864284, 5.99799053118142, 4.757913804231076, 5.1552073596116506, 7.636426262795658, 6.598570317774973, 5.55683746821213, 4.825133460880819, 4.53195970305815, 6.032366311400969, 5.822092679004669, 4.913065034204201, 4.765757417029921, 5.067893487853502, 5.431872307175255, 5.543861664243698, 4.579025627430244, 7.410651171142116, 5.9869670759644285, 5.627978841350949, 4.345702639157525, 5.637663953330408, 4.716700787345068, 3.3104116950608886, 6.352534184519216, 5.205770919120722, 4.012118393769083, 4.782687120837187, 3.935260854841958, 5.511395240211079]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.72081756683573, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.72081756683573, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.72081756683573, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.72081756683573, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.72081756683573, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.71391426 -0.25216879]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7139142566003898  |
| -0.2521687925138849  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.120101065981546e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3124542782225333

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.985784428144e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 7.128993866590171e-08  |
| -2.518103201549027e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.346106766030754, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.346106766030754, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.346106766030754, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.346106766030754, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.346106766030754, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.3339533  -0.45124317]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.33395330278906954  |
| -0.45124316838673906  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.4811653297818673e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3802320840054655

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.525397077607483e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.179177906077161e-07  |
| -2.944540852281168e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (3.967421844437092, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.967421844437092, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.967421844437092, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.967421844437092, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.967421844437092, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.06404868  0.0590941 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.0640486824975426  |
| 0.05909409850346492  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00015237643097555995

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22986591034206946

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0006628926870835462

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0007053500903285031  |
| 3.9173045747741627e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.789511181850781, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.789511181850781, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.789511181850781, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.789511181850781, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.789511181850781, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.23044086 -0.4804541 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.23044086416845744  |
| -0.4804541009484353   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.165612585780475e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3913448435279969

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.0644352812285018e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.4528938605769095e-06  |
| -5.114122960604347e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.3859933966975175, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3859933966975175, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3859933966975175, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3859933966975175, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3859933966975175, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.63961243 -0.30245361]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6396124274665738  |
| -0.3024536088247487  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.652540589720831e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.32833950708691656

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.078651921161173e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.167206165951398e-05  |
| -2.443417427994186e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (3.53316674864284, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.53316674864284, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.53316674864284, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.53316674864284, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.53316674864284, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.50438822  0.62458618]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.504388218442898  |
| 0.6245861827736121  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0007766100730366458

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.13160667741596066

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.005900992930488359

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.008877384241741518  |
| 0.003685678649027795  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.99799053118142, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.99799053118142, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.99799053118142, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.99799053118142, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.99799053118142, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.99497076 -0.01202218]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9949707568956967  |
| -0.012022183248916463  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.149569612552109e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24656601645093196

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4940864524109747e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.4815430853186505e-08  |
| -2.9984364369524705e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.757913804231076, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.757913804231076, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.757913804231076, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.757913804231076, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.757913804231076, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.26248096 -0.47255747]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.26248096252068365  |
| -0.4725574720421122   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.843802925008954e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38830907481716065

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2474091488306598e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -3.274211540421783e-06  |
| -5.894725139736195e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.1552073596116506, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.1552073596116506, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.1552073596116506, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.1552073596116506, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.1552073596116506, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.14037916 -0.49715242]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1403791605181226  |
| -0.49715242256276326  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.760356717047004e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39784270599119986

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.699253653577487e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.385398013965604e-07  |
| -8.447880704246741e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (7.636426262795658, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.636426262795658, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.636426262795658, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.636426262795658, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.636426262795658, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.65636297 3.02112629]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.6563629695885993  |
| 3.0211262913582004  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.668967849060455e-14

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.012383833519260133

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.000229642604545e-12

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.859515080123115e-11  |
| 2.114857781881761e-11  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (6.598570317774973, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.598570317774973, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.598570317774973, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.598570317774973, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.598570317774973, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.6039654  0.77934686]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.6039654004984527  |
| 0.779346858159613   |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.399825068055669e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.11297826830070058

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2390215296360572e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.987347664008904e-09  |
| 9.65627536313979e-10   |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.55683746821213, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.55683746821213, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.55683746821213, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.55683746821213, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.55683746821213, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.5476366  -0.35705265]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5476365982914899  |
| -0.3570526496687876  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.865773562160656e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.34650344949817163

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.270041921242738e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.2431580357284514e-07  |
| -8.105244828389449e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.825133460880819, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.825133460880819, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.825133460880819, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.825133460880819, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.825133460880819, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.19431948 -0.48812557]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.19431947606562971  |
| -0.4881255677435803   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.5099983489875487e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39431677666151904

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.90146845565368e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.7297286865173531e-06  |
| -4.3450343436675234e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.53195970305815, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.53195970305815, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.53195970305815, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.53195970305815, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.53195970305815, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.49160096 -0.38616987]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.4916009554989387   |
| -0.38616986941519826  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3835567750924603e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35659749038044025

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.8798836571070545e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.9073545130585446e-05  |
| -1.498294165211193e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (6.032366311400969, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.032366311400969, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.032366311400969, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.032366311400969, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.032366311400969, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.02982818 0.02326745]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0298281805098952  |
| 0.023267452409214684  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.000838739565405e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23813262100006893

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1000225498563495e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.1626624015483152e-08  |
| 4.886217473706028e-10   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.822092679004669, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.822092679004669, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.822092679004669, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.822092679004669, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.822092679004669, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.81660836 -0.17358097]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8166083564908888  |
| -0.17358097403885608  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7390876372275166e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.289153074937969

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.014418617545732e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 4.9114245025222233e-08  |
| -1.0439886419110183e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.913065034204201, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.913065034204201, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.913065034204201, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.913065034204201, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.913065034204201, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.10515587 -0.50147671]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.10515587289816608  |
| -0.5014767090472105   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.287519424233396e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39954294359066905

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.725340569590825e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -6.020531852346066e-07  |
| -2.8711249470128888e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.765757417029921, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.765757417029921, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.765757417029921, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.765757417029921, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.765757417029921, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.25452745 -0.47461349]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.25452745133769383  |
| -0.4746134885103004   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.666223843526028e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3890972115659458

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1992437120653016e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -3.052404455647364e-06  |
| -5.69177241757355e-06   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.067893487853502, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.067893487853502, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.067893487853502, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.067893487853502, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.067893487853502, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.05184192 -0.50566179]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.05184191631535384  |
| -0.5056617868870461  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0563291403373597e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4011953652258604

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.6329544952312186e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.3649740660391154e-07  |
| -1.3313844748508985e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.431872307175255, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.431872307175255, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.431872307175255, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.431872307175255, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.431872307175255, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.42092052 -0.41841853]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.4209205228811186  |
| -0.41841852871371543  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.562893228217347e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3681206465639925

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.245600573630581e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.787060413396961e-07  |
| -1.776437945524614e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.543861664243698, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.543861664243698, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.543861664243698, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.543861664243698, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.543861664243698, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.53447899 -0.36417168]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5344789899552893  |
| -0.3641716750379942  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.453166751147758e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3489446921197789

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4224947225293e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.294772532469479e-07  |
| -8.822039608741961e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.579025627430244, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.579025627430244, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.579025627430244, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.579025627430244, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.579025627430244, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.44387558 -0.40849285]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.44387558073566424  |
| -0.40849285065647223  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.1165561514653036e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3645348653072607

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.062961208180117e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.3595736850517613e-05  |
| -1.2511977553796884e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (7.410651171142116, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.410651171142116, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.410651171142116, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.410651171142116, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.410651171142116, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.4274245  2.43918907]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 2.4274244969646475  |
| 2.439189068503822  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.738839552295996e-13

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.021983412120316532

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1556433215917724e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 5.23266140555011e-11  |
| 5.25802162561992e-11  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.9869670759644285, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9869670759644285, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9869670759644285, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9869670759644285, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9869670759644285, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.98379285 -0.0230814 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9837928471867485  |
| -0.023081401234748  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.569514408906515e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2492698835931507

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.635502658487633e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5927886641617934e-08  |
| -6.083109431579809e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.627978841350949, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.627978841350949, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.627978841350949, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.627978841350949, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.627978841350949, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.61977475 -0.3149452 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6197747492997507  |
| -0.3149452010298148  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.283925521174392e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3324093353205635

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.5895839736506696e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.851840087602453e-08  |
| -5.0063184413518205e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.345702639157525, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.345702639157525, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.345702639157525, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.345702639157525, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.345702639157525, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.68046771 -0.27548749]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.680467711067223    |
| -0.27548749059747024  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.1626845806558396e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3197228872791602

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.891955523016403e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -6.731156332725747e-05  |
| -2.725110004137575e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.637663953330408, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.637663953330408, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.637663953330408, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.637663953330408, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.637663953330408, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.62959556 -0.30881029]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.629595562351426  |
| -0.30881028623142015  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.003385510516371e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.33040428180714576

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.51432223673082e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.534105602158098e-08  |
| -4.676382833714489e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.716700787345068, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.716700787345068, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.716700787345068, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.716700787345068, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.716700787345068, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.30427142 -0.46071505]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3042714247225575  |
| -0.46071505499512    |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.888132391179627e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3838004750787788

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.534164956406328e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.668025570451738e-06  |
| -7.068128922623274e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (3.3104116950608886, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3104116950608886, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3104116950608886, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3104116950608886, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3104116950608886, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.73026435  0.98990152]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.73026434957535   |
| 0.9899015163128411  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0016642988945206605

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.09179429662636182

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.018130744018825033

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.031370980007049465  |
| 0.017947650996114874  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (6.352534184519216, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.352534184519216, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.352534184519216, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.352534184519216, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.352534184519216, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.354482   0.41030513]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.3544819998934088  |
| 0.41030512987205725  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.886528514424369e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.16257476279962169

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.235914846704851e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 5.7374704129429684e-09  |
| 1.7380175913042092e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.205770919120722, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.205770919120722, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.205770919120722, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.205770919120722, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.205770919120722, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.19165118 -0.48864049]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.19165117715047586  |
| -0.4886404869619554  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.202460499057934e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39451706460032987

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3186908668521976e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.527286569298051e-07  |
| -6.443657473309409e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.012118393769083, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.012118393769083, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.012118393769083, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.012118393769083, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.012118393769083, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.01872588  0.01189552]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.01872587920937    |
| 0.01189551568359093  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00012748835434413532

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.240818263554373

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005293965352231287

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.000539309950795576  |
| 6.297444787585426e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.782687120837187, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.782687120837187, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.782687120837187, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.782687120837187, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.782687120837187, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.23736054 -0.47883558]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.23736054122203143  |
| -0.47883558451644603  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.303910279704543e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3907206957968897

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1015311771306492e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.6146003637667224e-06  |
| -5.274523250644432e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.935260854841958, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.935260854841958, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.935260854841958, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.935260854841958, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.935260854841958, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.09666028  0.09432619]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.09666028480504    |
| 0.09432618686133765  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00017302446954179672

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22201631560001012

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0007793322264365549

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0008546627014016582  |
| 7.351143721791679e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.511395240211079, array([5.01676785, 0.98618238])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01676785 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676795 0.98618238]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.511395240211079, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.511395240211079, array([5.01676795, 0.98618238])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01676785 0.98618248]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.511395240211079, array([5.01676785, 0.98618238]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.511395240211079, array([5.01676785, 0.98618248])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.50155767 -0.38122552]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5015576687306122  |
| -0.38122552448172087  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0114838092688963e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3548629429246261

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.8503506196862553e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.4296152118746942e-07  |
| -1.0866264099466907e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| 7.128993866590171e-08  |
| -2.518103201549027e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                      â€”
| 2.179177906077161e-07  |
| -2.944540852281168e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| -0.0007053500903285031  |
| 3.9173045747741627e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                        â€”
| -2.4528938605769095e-06  |
| -5.114122960604347e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                       â€”
| -5.167206165951398e-05  |
| -2.443417427994186e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                     â€”
| -0.008877384241741518  |
| 0.003685678649027795  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 2.4815430853186505e-08  |
| -2.9984364369524705e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| -3.274211540421783e-06  |
| -5.894725139736195e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                      â€”
| 2.385398013965604e-07  |
| -8.447880704246741e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                      â€”
| 1.859515080123115e-11  |
| 2.114857781881761e-11  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                      â€”
| 1.987347664008904e-09  |
| 9.65627536313979e-10   |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                       â€”
| 1.2431580357284514e-07  |
| -8.105244828389449e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                        â€”
| -1.7297286865173531e-06  |
| -4.3450343436675234e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| -1.9073545130585446e-05  |
| -1.498294165211193e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                       â€”
| 2.1626624015483152e-08  |
| 4.886217473706028e-10   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| 4.9114245025222233e-08  |
| -1.0439886419110183e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                       â€”
| -6.020531852346066e-07  |
| -2.8711249470128888e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| -3.052404455647364e-06  |
| -5.69177241757355e-06   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                       â€”
| 1.3649740660391154e-07  |
| -1.3313844748508985e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                      â€”
| 1.787060413396961e-07  |
| -1.776437945524614e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                      â€”
| 1.294772532469479e-07  |
| -8.822039608741961e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                        â€”
| -1.3595736850517613e-05  |
| -1.2511977553796884e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                     â€”
| 5.23266140555011e-11  |
| 5.25802162561992e-11  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                       â€”
| 2.5927886641617934e-08  |
| -6.083109431579809e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                      â€”
| 9.851840087602453e-08  |
| -5.0063184413518205e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                       â€”
| -6.731156332725747e-05  |
| -2.725110004137575e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                      â€”
| 9.534105602158098e-08  |
| -4.676382833714489e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| -4.668025570451738e-06  |
| -7.068128922623274e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                     â€”
| -0.031370980007049465  |
| 0.017947650996114874  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                       â€”
| 5.7374704129429684e-09  |
| 1.7380175913042092e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| 2.527286569298051e-07  |
| -6.443657473309409e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                      â€”
| -0.000539309950795576  |
| 6.297444787585426e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                        â€”
| -2.6146003637667224e-06  |
| -5.274523250644432e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                      â€”
| -0.0008546627014016582  |
| 7.351143721791679e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| 1.4296152118746942e-07  |
| -1.0866264099466907e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.04251592  0.02163317]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.01719301 0.98596605]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.01676785 0.98618238]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.01719301 0.98596605]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 7              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.01719301 0.98596605]

[4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 700

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [5.006792385634919, 4.861567822983643, 5.4722858473660745, 4.458852548946068, 4.88410549606166, 5.143890922660086, 4.543324019776738, 4.449538349639722, 5.980251346533316, 5.755206684399332, 4.029214745185353, 6.188170057382675, 5.310447194589536, 4.317734324641174, 4.184849798656908, 5.37414252802516, 5.86539785051527, 6.135959274378711, 4.758835667048313, 5.014176919759004, 5.1916247724180185, 4.579025627430244, 4.158940389859275, 4.926155833330847, 4.888817497310663, 4.820574508670093, 3.24759527534737, 5.519559049000368, 5.631262219431296, 5.147057473169987, 6.400651394067637, 5.634771311039012, 6.837919540774682, 3.509387105833827, 4.600664009685107]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.006792385634919, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.006792385634919, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.006792385634919, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.006792385634919, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.006792385634919, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.01054872 -0.50706119]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.010548718565317472  |
| -0.5070611897028954    |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4370422789392471e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4017494226316961

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.576961653176179e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.7732361798288235e-08  |
| -1.8137384313811488e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.861567822983643, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.861567822983643, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.861567822983643, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.861567822983643, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.861567822983643, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.15784036 -0.49466005]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.15784036300203752  |
| -0.4946600451471994   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.9421810969867104e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3968670982347637

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.413517296025094e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.1701522611264844e-06  |
| -3.6671708003513164e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.4722858473660745, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.4722858473660745, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.4722858473660745, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.4722858473660745, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.4722858473660745, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.46157044 -0.40059318]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.4615704418675648  |
| -0.4005931786288386  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.2538275591458426e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3617143139940921

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.466347641322045e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.5999636124716073e-07  |
| -1.3885952198697753e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.458852548946068, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.458852548946068, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.458852548946068, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.458852548946068, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.458852548946068, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.56628777 -0.34677595]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.5662877700096658   |
| -0.34677595239784864  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9218800605560435e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3430213706744088

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.602799781184088e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -3.172796993897381e-05  |
| -1.94291623021457e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.88410549606166, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.88410549606166, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.88410549606166, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.88410549606166, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.88410549606166, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.1349819  -0.49800678]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.13498189566085728  |
| -0.4980067780380182   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.6361744029742475e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3981788294231038

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.620579016703712e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.936583060471616e-07  |
| -3.2970932248547263e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.143890922660086, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143890922660086, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143890922660086, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143890922660086, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143890922660086, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.12850124 -0.49886054]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.12850123964902593  |
| -0.49886053621328585  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.166018536735166e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3985141482847218

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7981842219602559e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.310689016392121e-07  |
| -8.970431451773635e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.543324019776738, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.543324019776738, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.543324019776738, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.543324019776738, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.543324019776738, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.48061396 -0.39162197]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -0.48061395530041295  |
| -0.391621974848988  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3140189708319393e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3585289501370166

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.665028919783938e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.761464045427758e-05  |
| -1.435305863444439e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.449538349639722, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.449538349639722, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.449538349639722, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.449538349639722, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.449538349639722, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.57573454 -0.34138174]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.5757345444834527   |
| -0.34138174065390103  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.0032906816773216e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3412018505187817

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.8712773058862675e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.380297165240464e-05   |
| -2.0043468665452005e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.980251346533316, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.980251346533316, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.980251346533316, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.980251346533316, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.980251346533316, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.97676617 -0.03008075]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9767661746096223  |
| -0.030080749002792118  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.838881671065959e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2510223551439636

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.7244114043722513e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6611129055115128e-08  |
| -8.195233563526607e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.755206684399332, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.755206684399332, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.755206684399332, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.755206684399332, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.755206684399332, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.74851829 -0.226977  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.748518291793232  |
| -0.2269770016383177  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.561364746824763e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3048058571430733

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.403266166970276e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.289998436784451e-08  |
| -1.907348158547632e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.029214745185353, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.029214745185353, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.029214745185353, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.029214745185353, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.029214745185353, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.00204091 -0.00507393]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -1.0020409102473593    |
| -0.005073934605803743  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00011901937480545464

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2449088567296637

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00048597415542562877

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.0004869659850593887   |
| -2.4658010847403444e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (6.188170057382675, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.188170057382675, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.188170057382675, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.188170057382675, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.188170057382675, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.18764434 0.19813269]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.1876443362091038  |
| 0.1981326946953743  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9301601412161715e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.20044327079032886

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.62945841786423e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.143637175073753e-08  |
| 1.9079105447884954e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.310447194589536, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.29742823 -0.46288504]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.29742822538203484  |
| -0.4628850414079011  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.000360399116986e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38462636449857396

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.800714345280171e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.3201526244288632e-07  |
| -3.6108339827262205e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.317734324641174, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.317734324641174, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.317734324641174, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.317734324641174, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.317734324641174, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.70941463 -0.25548233]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.7094146292629944   |
| -0.25548233306693646  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.57002315824285e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.31349403386937863

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00011387850397594318

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -8.078707667911816e-05   |
| -2.9093945881946364e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.184849798656908, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.184849798656908, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.184849798656908, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.184849798656908, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.184849798656908, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.84419059 -0.15078803]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.844190592985683   |
| -0.1507880265094741  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.28083550046486e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2827477872982988

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00022213561989217553

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00018752480068001795  |
| -3.3495391740999825e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.37414252802516, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.37414252802516, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.37414252802516, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.37414252802516, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.37414252802516, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.36203018 -0.44158389]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.362030178058248  |
| -0.4415838894544777  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.1349712103782527e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37663259628739826

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.668577896399369e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0521962651705125e-07  |
| -2.503152675167715e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.86539785051527, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.86539785051527, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.86539785051527, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.86539785051527, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.86539785051527, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.86027789 -0.1370778 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8602778867050631  |
| -0.1370778002218742  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3502587467348287e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.27895137159037325

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.840480758480081e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.164158557541765e-08  |
| -6.635224543887587e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (6.135959274378711, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.135959274378711, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.135959274378711, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.135959274378711, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.135959274378711, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.1346904  0.13664432]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.134690399418048  |
| 0.13664431586235537  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.662669511565693e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.21297115373457912

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2502489021983298e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.4186454262073989e-08  |
| 1.7083940589855161e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.758835667048313, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.758835667048313, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.758835667048313, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.758835667048313, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.758835667048313, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.26203478 -0.47278573]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.26203477943020914  |
| -0.47278573056530604  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.822601784644709e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3883993742746563

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.241660544291817e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.2535824685069967e-06  |
| -5.870393875471222e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.014176919759004, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.014176919759004, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.014176919759004, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.014176919759004, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.014176919759004, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.00305908 -0.50711215]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                       â€”
| -0.0030590752153614176  |
| -0.5071121478295026     |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3848432182852162e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40176960832649167

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.4468590694392332e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.0544201150165278e-08  |
| -1.7479441059689303e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.1916247724180185, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.1916247724180185, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.1916247724180185, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.1916247724180185, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.1916247724180185, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.17691452 -0.49146745]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1769145174801423  |
| -0.4914674467393354  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.599477159242692e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39561980594454466

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4153682588954077e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5039919257919017e-07  |
| -6.956074243952247e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.579025627430244, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.579025627430244, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.579025627430244, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.579025627430244, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.579025627430244, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.44440418 -0.40836932]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4444041823425948  |
| -0.4083693205814143  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.1165561514653036e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3644982410487027

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.063268970113121e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.361329541958564e-05   |
| -1.2509450680832238e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.158940389859275, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.158940389859275, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.158940389859275, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.158940389859275, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.158940389859275, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.87046879 -0.12825895]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8704687881611051   |
| -0.12825895456813896  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.997789745579924e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.27653637990896823

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00025305132539463687

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.00022027328055883102  |
| -3.245609844719808e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.926155833330847, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.926155833330847, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.926155833330847, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.926155833330847, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.926155833330847, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.09233303 -0.50285414]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.09233302677991162  |
| -0.5028541372187334   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.144843045903551e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4000864132838122

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.360949471638389e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.949926911305404e-07  |
| -2.6957756212339468e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.888817497310663, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.888817497310663, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.888817497310663, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.888817497310663, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.888817497310663, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.13020283 -0.49864045]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.13020282629838675  |
| -0.49864044560088416  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.5761699070021216e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39842767938309076

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.46584070411714e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.418707340712026e-07  |
| -3.224129689885305e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.820574508670093, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.820574508670093, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.820574508670093, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.820574508670093, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.820574508670093, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.19941716 -0.48723323]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.1994171638930453  |
| -0.487233234869322   |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.5880279281789237e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3939716200943796

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.107325871136042e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.8161570958717078e-06  |
| -4.43739184520268e-06    |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.24759527534737, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.24759527534737, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.24759527534737, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.24759527534737, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.24759527534737, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.79478572  1.10351081]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.794785724484882  |
| 1.103510807176633  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.002044962004362224

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0820935207482931

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.024910151077967298

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.044708383549497396  |
| 0.02748862092293957  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.519559049000368, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.519559049000368, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.519559049000368, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.519559049000368, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.519559049000368, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.50951652 -0.37731327]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5095165156454584  |
| -0.3773132739581797  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.669495757281066e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35350637364245646

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.7353101607896297e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.3936857023351508e-07  |
| -1.0320688320586101e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.631262219431296, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.631262219431296, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.631262219431296, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.631262219431296, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.631262219431296, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.62280963 -0.31317089]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6228096349758516  |
| -0.31317089366211803  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.187153658754676e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3318421151983642

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.5631390414848242e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 9.73538055843666e-08  |
| -4.8952965053994897e-08  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.147057473169987, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.147057473169987, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.147057473169987, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.147057473169987, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.147057473169987, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.13171286 -0.49844268]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.13171286283863992  |
| -0.4984426815735077  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.05020575634982e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39834999797528753

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7698520879086823e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.3311228529939674e-07  |
| -8.821698206856751e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (6.400651394067637, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.400651394067637, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.400651394067637, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.400651394067637, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.400651394067637, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.40315007 0.4772982 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4031500716882306  |
| 0.47729819874220425  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.066971300856849e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.15221340682239548

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.328860056833926e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.670890227386611e-09  |
| 1.5888589089917046e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.634771311039012, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.634771311039012, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.634771311039012, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.634771311039012, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.634771311039012, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.62636867 -0.31094796]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6263686747054464  |
| -0.31094795627240046  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.0856271076093264e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.331115599773349

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.5359068286394465e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.620439247259353e-08  |
| -4.775870893902599e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (6.837919540774682, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.837919540774682, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.837919540774682, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.837919540774682, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.837919540774682, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.84664216 1.19792673]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.846642163627621  |
| 1.197926726703713  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8036598884280475e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0747962832081697

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.7483946637094595e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.921943631922664e-10  |
| 4.490302149891138e-10  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.509387105833827, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.509387105833827, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.509387105833827, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.509387105833827, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.509387105833827, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.52926763  0.6622127 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.5292676280864725  |
| 0.6622127024513702  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0008444395015244974

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.12684477380548972

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.006657266800912146

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.010180742610169735  |
| 0.00440852663917182  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.600664009685107, array([5.01719301, 0.98596605])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01719301 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719311 0.98596605]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.600664009685107, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.600664009685107, array([5.01719311, 0.98596605])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01719301 0.98596615]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.600664009685107, array([5.01719301, 0.98596605]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.600664009685107, array([5.01719301, 0.98596615])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.42245781 -0.41788156]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.4224578054046191   |
| -0.41788155824562523  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0109922423449961e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3679328579113929

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.7477628610937146e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.1608138680699678e-05  |
| -1.1482394260832989e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                        â€”
| -3.7732361798288235e-08  |
| -1.8137384313811488e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                        â€”
| -1.1701522611264844e-06  |
| -3.6671708003513164e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| 1.5999636124716073e-07  |
| -1.3885952198697753e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                      â€”
| -3.172796993897381e-05  |
| -1.94291623021457e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                       â€”
| -8.936583060471616e-07  |
| -3.2970932248547263e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                      â€”
| 2.310689016392121e-07  |
| -8.970431451773635e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| -1.761464045427758e-05  |
| -1.435305863444439e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                        â€”
| -3.380297165240464e-05   |
| -2.0043468665452005e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| 2.6611129055115128e-08  |
| -8.195233563526607e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                      â€”
| 6.289998436784451e-08  |
| -1.907348158547632e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                        â€”
| -0.0004869659850593887   |
| -2.4658010847403444e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                      â€”
| 1.143637175073753e-08  |
| 1.9079105447884954e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                       â€”
| 2.3201526244288632e-07  |
| -3.6108339827262205e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                        â€”
| -8.078707667911816e-05   |
| -2.9093945881946364e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                        â€”
| -0.00018752480068001795  |
| -3.3495391740999825e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| 2.0521962651705125e-07  |
| -2.503152675167715e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                      â€”
| 4.164158557541765e-08  |
| -6.635224543887587e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| 1.4186454262073989e-08  |
| 1.7083940589855161e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                        â€”
| -3.2535824685069967e-06  |
| -5.870393875471222e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                        â€”
| -1.0544201150165278e-08  |
| -1.7479441059689303e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                       â€”
| 2.5039919257919017e-07  |
| -6.956074243952247e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                        â€”
| -1.361329541958564e-05   |
| -1.2509450680832238e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| -0.00022027328055883102  |
| -3.245609844719808e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                       â€”
| -4.949926911305404e-07  |
| -2.6957756212339468e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                       â€”
| -8.418707340712026e-07  |
| -3.224129689885305e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                        â€”
| -1.8161570958717078e-06  |
| -4.43739184520268e-06    |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                    â€”
| -0.044708383549497396  |
| 0.02748862092293957  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| 1.3936857023351508e-07  |
| -1.0320688320586101e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                     â€”
| 9.73538055843666e-08  |
| -4.8952965053994897e-08  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                       â€”
| 2.3311228529939674e-07  |
| -8.821698206856751e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| 4.670890227386611e-09  |
| 1.5888589089917046e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                      â€”
| 9.620439247259353e-08  |
| -4.775870893902599e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                      â€”
| 6.921943631922664e-10  |
| 4.490302149891138e-10  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                    â€”
| -0.010180742610169735  |
| 0.00440852663917182  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                        â€”
| -1.1608138680699678e-05  |
| -1.1482394260832989e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.05597976  0.03169162]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.01775281 0.98564913]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.01719301 0.98596605]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.01775281 0.98564913]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 8              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.01775281 0.98564913]

[3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 800

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [3.7045246848469064, 4.398735026901141, 6.488978577529594, 5.002918527311009, 5.247808176707644, 4.611942257619824, 5.4741995747482814, 5.446508284371847, 7.055361979976823, 5.6522484943069085, 4.210917534229328, 6.284719622689429, 3.935260854841958, 3.953721076237072, 6.697726951435652, 4.021463590542871, 4.835497711250678, 6.490140411593667, 4.765341336617399, 5.312327690403269, 5.5079948041310836, 4.797842753811596, 5.677168255085978, 6.160360878331058, 5.4325658622178485, 5.189500835817977, 5.315952471562018, 5.362768994549103, 4.514892773948391, 4.794212663476959, 5.934944323484632, 4.486912586006225, 4.6519615102397385, 4.383481509023446, 3.3104116950608886]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.7045246848469064, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7045246848469064, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7045246848469064, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7045246848469064, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7045246848469064, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.33234853  0.38029627]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.3323485337224383  |
| 0.3802962678101096  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0004177238024507097

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.16753719978265982

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.002493319710444058

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0033219708603113955  |
| 0.0009482001803392585  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.398735026901141, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.398735026901141, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.398735026901141, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.398735026901141, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.398735026901141, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.62803062 -0.3100687 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6280306164008209  |
| -0.3100687018253723  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.5081665130302705e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3308493443353263

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.580992847572834e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.7610956109913816e-05  |
| -2.350628610794341e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.488978577529594, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.488978577529594, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.488978577529594, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.488978577529594, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.488978577529594, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.49264649 0.60671685]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4926464908526782  |
| 0.6067168500933917  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.867644344680454e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.13402604420314823

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1396172376272326e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 3.1936921615121896e-09  |
| 1.2981418308187187e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.002918527311009, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.002918527311009, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.002918527311009, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.002918527311009, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.002918527311009, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.01505032 -0.50716662]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.015050317658804602  |
| -0.5071666231426519    |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.465175644741359e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40179119299739935

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.6466096576458374e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.488263372523455e-08  |
| -1.8494387059876213e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.247808176707644, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.247808176707644, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.247808176707644, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.247808176707644, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.247808176707644, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.23340488 -0.48004095]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2334048798680044  |
| -0.4800409514338355  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.176241953647817e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3911910795409987

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.0675708552833008e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.491762472279815e-07  |
| -5.124777290932292e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.611942257619824, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.611942257619824, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.611942257619824, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.611942257619824, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.611942257619824, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.41171913 -0.42252359]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4117191299002343  |
| -0.42252358722905115  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.59811121760208e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.36963062264927443

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.5966764195046926e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.0691013560709277e-05  |
| -1.0971570356422111e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.4741995747482814, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.4741995747482814, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.4741995747482814, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.4741995747482814, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.4741995747482814, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.46309249 -0.40005254]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.46309249324139046  |
| -0.400052537763429   |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.2407631398367787e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.36153382907054155

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.431942020547914e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.589306586955429e-07  |
| -1.3729571147771432e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.446508284371847, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.446508284371847, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.446508284371847, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.446508284371847, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.446508284371847, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.43499802 -0.41266823]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.43499802426794076  |
| -0.4126682262572956  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4432981431909635e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3660574399895998

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.94281876426817e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.7151183725032174e-07  |
| -1.6270760259045276e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (7.055361979976823, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.055361979976823, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.055361979976823, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.055361979976823, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.055361979976823, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.06727633 1.6295357 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 2.067276327899492  |
| 1.6295357019302514  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.190502411782314e-12

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.048905678552932264

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2658044208674305e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6167675150097647e-10  |
| 2.0626734954646236e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.6522484943069085, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.6522484943069085, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.6522484943069085, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.6522484943069085, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.6522484943069085, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.64373377 -0.30008328]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6437337707332347  |
| -0.30008328488762004  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.607962060910469e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3276090569109721

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.406542940039257e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.054391904896809e-08  |
| -4.22080025782471e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.210917534229328, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.210917534229328, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.210917534229328, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.210917534229328, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.210917534229328, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.8185827  -0.17224114]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8185826994022705   |
| -0.17224113912917005  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.6297987286991595e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2888228387833052

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00019492221433787036

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.00015955995238616186  |
| -3.357362423913503e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.284719622689429, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.284719622689429, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.284719622689429, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.284719622689429, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.284719622689429, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.28541356 0.31886411]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.2854135622575313  |
| 0.3188641106710577  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0570446369371937e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.17799509108357184

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.9386167927569e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 7.633578566460043e-09  |
| 1.8936117622386378e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.935260854841958, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.935260854841958, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.935260854841958, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.935260854841958, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.935260854841958, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.09825289  0.0957997 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.0982528864111885  |
| 0.09579970150497275  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00017302446954179672

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22176521537780713

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0007802146483930135

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0008568729896179176  |
| 7.474433042585795e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.953721076237072, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.953721076237072, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.953721076237072, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.953721076237072, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.953721076237072, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.07952389  0.07540591]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.0795238880589864  |
| 0.07540591484556103  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00016087326006628024

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22626804890920904

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0007109853151685206

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0007675256317835653  |
| 5.3612498132041835e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.697726951435652, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.697726951435652, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.697726951435652, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.697726951435652, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.697726951435652, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.7044342  0.94526802]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.7044342026650838  |
| 0.9452680238553057  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.240746665949786e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0959993512280462

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.542495416192357e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.285568716080287e-09  |
| 7.129679737001851e-10  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.021463590542871, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.021463590542871, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.021463590542871, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.021463590542871, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.021463590542871, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.01079505  0.00357334]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -1.0107950543236655   |
| 0.003573337181705938  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0001227914416313585

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24286897017049733

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005055871960306712

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.000511045037277172   |
| 1.8066335261708462e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.835497711250678, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.835497711250678, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.835497711250678, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.835497711250678, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.835497711250678, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.18490875 -0.49018427]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.18490875053345235  |
| -0.490184266510596    |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.338604705977618e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39512172680144736

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.449559919177263e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.5623975672126064e-06  |
| -4.141841331319237e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.490140411593667, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.490140411593667, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.490140411593667, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.490140411593667, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.490140411593667, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.49382524 0.608477  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4938252412832753  |
| 0.6084770021175245  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8461042109514336e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.13379372502734593

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.127232955334581e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.177714282768415e-09  |
| 1.2943723314675878e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.765341336617399, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.765341336617399, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.765341336617399, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.765341336617399, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.765341336617399, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.25608659 -0.47448973]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.2560865897116571  |
| -0.47448972528840727  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.6754854528074576e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38905649834001865

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2017497388570244e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.0775199231076987e-06  |
| -5.7021790345568464e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.312327690403269, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.312327690403269, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.312327690403269, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.312327690403269, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.312327690403269, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.29886378 -0.46262009]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.29886378372090405  |
| -0.46262008779329733  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.9705418363036754e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38453133236402925

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.725096985052739e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.3087517145638097e-07  |
| -3.5737850454368346e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.5079948041310836, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.5079948041310836, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.5079948041310836, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.5079948041310836, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.5079948041310836, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.49737978 -0.38358655]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.497379775143969  |
| -0.3835865469703492  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0306129646937287e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3557136043662023

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.8973110728504123e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.4410639299364696e-07  |
| -1.1113695499336476e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.797842753811596, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.797842753811596, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.797842753811596, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.797842753811596, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.797842753811596, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.22311196 -0.48239042]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.2231119589080066  |
| -0.4823904209416696  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.002521268219396e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3920980308730859

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.020796064521663e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.2775180960101214e-06  |
| -4.924222432602048e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.677168255085978, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.677168255085978, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.677168255085978, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.677168255085978, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.677168255085978, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.66901636 -0.28348843]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6690163578682018  |
| -0.28348842606717994  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.001312820789453e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3222940369185438

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2415100381769522e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.305905239979567e-08  |
| -3.5195372666938865e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.160360878331058, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.160360878331058, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.160360878331058, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.160360878331058, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.160360878331058, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.15924418 0.16464364]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.1592441762076078  |
| 0.1646436387225947  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.2917236685077422e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2072163177911662

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1059571432098097e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.2820743774011751e-08  |
| 1.8208880832930882e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.4325658622178485, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.4325658622178485, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.4325658622178485, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.4325658622178485, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.4325658622178485, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.4208526  -0.41872141]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.42085260165691807  |
| -0.41872140976728645  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5570160391045826e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3682479826039006

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.228172624585317e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.7794374493112902e-07  |
| -1.7704264021058115e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.189500835817977, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.189500835817977, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.189500835817977, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.189500835817977, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.189500835817977, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.17424859 -0.49209859]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.17424859333203813  |
| -0.4920985852141513  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.661549711041342e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.395867964629104

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4301611185804724e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.492035631508215e-07  |
| -7.037802630817385e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.315952471562018, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.315952471562018, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.315952471562018, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.315952471562018, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.315952471562018, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.30254134 -0.46151424]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3025413430890467  |
| -0.4615142357167201  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.9138690770012506e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38411242879403434

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.585979673059998e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.2950724789337792e-07  |
| -3.5010376109748595e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.362768994549103, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.362768994549103, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.362768994549103, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.362768994549103, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.362768994549103, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.35003951 -0.44601604]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.35003950515921645  |
| -0.44601604076710544  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.2693909086436316e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37828939866376615

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.999086722122836e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0999173476191032e-07  |
| -2.67568890801974e-07   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.514892773948391, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.514892773948391, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.514892773948391, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.514892773948391, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.514892773948391, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.51018164 -0.37713727]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.5101816391572811   |
| -0.37713726808163983  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.494599909017428e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35345960180395564

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.228488634597623e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.1572972627569487e-05  |
| -1.594720651766411e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.794212663476959, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.794212663476959, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.794212663476959, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.794212663476959, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.794212663476959, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.2267949  -0.48156193]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.2267948995271496  |
| -0.48156192811177334  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.072815353801508e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39177797389398056

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.0395723152378282e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.357696987855695e-06  |
| -5.006184485375488e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.934944323484632, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.934944323484632, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.934944323484632, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.934944323484632, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.934944323484632, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.9305456  -0.07432232]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.930545600574817  |
| -0.07432231940995848  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.9579843640522e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2622504276358403

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.415813062654454e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.178569817839094e-08  |
| -2.5387114948731286e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.486912586006225, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.486912586006225, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.486912586006225, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.486912586006225, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.486912586006225, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.53856921 -0.36225152]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.5385692092296779   |
| -0.36225152433289054  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6951897043409796e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3483114590452326

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.8668789392910504e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.6211511417505543e-05  |
| -1.7630343145018245e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.6519615102397385, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6519615102397385, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6519615102397385, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6519615102397385, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6519615102397385, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.3711172  -0.43841591]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.37111720252447356  |
| -0.4384159135728538   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.974102302486762e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3754661986592872

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.123786996262419e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -7.881738888107635e-06  |
| -9.311020162005355e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.383481509023446, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.383481509023446, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.383481509023446, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.383481509023446, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.383481509023446, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.64350622 -0.3002298 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.6435062238629996   |
| -0.30022980324062587  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.6819170656107977e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.32765637253651764

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.185151550232387e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.2671959658364206e-05  |
| -2.457426439420973e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.3104116950608886, array([5.01775281, 0.98564913])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01775281 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775291 0.98564913]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3104116950608886, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3104116950608886, array([5.01775291, 0.98564913])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01775281 0.98564923]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3104116950608886, array([5.01775281, 0.98564913]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3104116950608886, array([5.01775281, 0.98564923])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.73219973  0.99297783]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.7321997347607976  |
| 0.9929778332917749  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0016642988945206605

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.09158946884347477

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.018171291039638257

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.031476305519122644  |
| 0.01804368920465424  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| -0.0033219708603113955  |
| 0.0009482001803392585  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                       â€”
| -4.7610956109913816e-05  |
| -2.350628610794341e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| 3.1936921615121896e-09  |
| 1.2981418308187187e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                       â€”
| -5.488263372523455e-08  |
| -1.8494387059876213e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                      â€”
| 2.491762472279815e-07  |
| -5.124777290932292e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                        â€”
| -1.0691013560709277e-05  |
| -1.0971570356422111e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                      â€”
| 1.589306586955429e-07  |
| -1.3729571147771432e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| 1.7151183725032174e-07  |
| -1.6270760259045276e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| 2.6167675150097647e-10  |
| 2.0626734954646236e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                      â€”
| 9.054391904896809e-08  |
| -4.22080025782471e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                       â€”
| -0.00015955995238616186  |
| -3.357362423913503e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                      â€”
| 7.633578566460043e-09  |
| 1.8936117622386378e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                      â€”
| -0.0008568729896179176  |
| 7.474433042585795e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| -0.0007675256317835653  |
| 5.3612498132041835e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                      â€”
| 1.285568716080287e-09  |
| 7.129679737001851e-10  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| -0.000511045037277172   |
| 1.8066335261708462e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                        â€”
| -1.5623975672126064e-06  |
| -4.141841331319237e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                      â€”
| 3.177714282768415e-09  |
| 1.2943723314675878e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                        â€”
| -3.0775199231076987e-06  |
| -5.7021790345568464e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                       â€”
| 2.3087517145638097e-07  |
| -3.5737850454368346e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                       â€”
| 1.4410639299364696e-07  |
| -1.1113695499336476e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                        â€”
| -2.2775180960101214e-06  |
| -4.924222432602048e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                      â€”
| 8.305905239979567e-08  |
| -3.5195372666938865e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                       â€”
| 1.2820743774011751e-08  |
| 1.8208880832930882e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                       â€”
| 1.7794374493112902e-07  |
| -1.7704264021058115e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                      â€”
| 2.492035631508215e-07  |
| -7.037802630817385e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                       â€”
| 2.2950724789337792e-07  |
| -3.5010376109748595e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| 2.0999173476191032e-07  |
| -2.67568890801974e-07   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                       â€”
| -2.1572972627569487e-05  |
| -1.594720651766411e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                       â€”
| -2.357696987855695e-06  |
| -5.006184485375488e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| 3.178569817839094e-08  |
| -2.5387114948731286e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                        â€”
| -2.6211511417505543e-05  |
| -1.7630343145018245e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                       â€”
| -7.881738888107635e-06  |
| -9.311020162005355e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                       â€”
| -5.2671959658364206e-05  |
| -2.457426439420973e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                    â€”
| -0.031476305519122644  |
| 0.01804368920465424  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.0372672   0.01896206]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.01812548 0.98545951]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.01775281 0.98564913]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.01812548 0.98545951]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 9              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.01812548 0.98545951]

[5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 900

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [5.559781198234466, 4.345702639157525, 6.37469851742308, 4.964929274694722, 3.7931851831315924, 4.3200161395055625, 2.7221024564318363, 3.674507657822029, 3.695477784606563, 5.731481560043303, 5.229495417953454, 5.2106314983131234, 4.709619903202896, 4.1777980316709495, 5.497622685783328, 3.236238053622853, 5.809400694679507, 4.734874894609524, 5.459385056453483, 4.184849798656908, 5.752802221204449, 3.4456366356424297, 4.427602586491956, 4.857322360163403, 5.615807021617604, 4.106479277730971, 5.7652235553793725, 4.695996151221829, 5.310447194589536, 5.721829412707804, 5.903454622985064, 5.204336062374974, 4.059651123910417, 5.680442034038884, 4.242708927436795]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.559781198234466, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.559781198234466, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.559781198234466, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.559781198234466, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.559781198234466, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.54964781 -0.35632112]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5496478117095194  |
| -0.356321121497416  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.738119695007458e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3462913282904942

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.234569295514141e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 1.22822612339263e-07  |
| -7.962242374412896e-08  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.345702639157525, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.345702639157525, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.345702639157525, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.345702639157525, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.345702639157525, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.68234451 -0.27458053]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.6823445142245532   |
| -0.27458053164508556  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.1626845806558396e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3194907624826778

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.899142485621362e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -6.754625570590944e-05   |
| -2.7181118065323674e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (6.37469851742308, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.37469851742308, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.37469851742308, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.37469851742308, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.37469851742308, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.37658926 0.44012148]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.376589264800998  |
| 0.4401214792615349  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.980605414480303e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.15797209329317083

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.785861977141279e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.211576955750966e-09  |
| 1.6662391736594188e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.964929274694722, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.964929274694722, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.964929274694722, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.964929274694722, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.964929274694722, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.05398117 -0.50592051]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.05398117064459029  |
| -0.5059205088198127   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7705861915064637e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40129811008865773

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.412146847926328e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.3817285190690228e-07  |
| -2.2321955782906202e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.7931851831315924, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7931851831315924, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7931851831315924, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7931851831315924, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7931851831315924, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.24301439  0.26516474]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.243014389196162  |
| 0.2651647434959159  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0002995990581645428

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.18769741830606987

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0015961810283186735

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0019840759859620378  |
| 0.0004232509329471683  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.3200161395055625, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3200161395055625, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3200161395055625, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3200161395055625, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3200161395055625, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.70841002 -0.25645518]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.7084100217547018  |
| -0.2564551770944945  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.535013820543432e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.31383474399741085

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00011263933927508664

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -7.979483678629937e-05   |
| -2.8886941701599194e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (2.7221024564318363, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.7221024564318363, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.7221024564318363, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.7221024564318363, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.7221024564318363, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.32990098  2.20684137]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -2.329900978814692  |
| 2.206841367424772  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.009814843220298694

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.027698749979679964

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.35434246049005624

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.8255828455313883  |
| 0.7819776000445341  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.674507657822029, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.674507657822029, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.674507657822029, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.674507657822029, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.674507657822029, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.363443    0.42211075]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.363443000723663  |
| 0.4221107507973443  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0004666449830968409

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.16080094590671606

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.002902003968108193

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.003956716998389412  |
| 0.0012249670737950215  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (3.695477784606563, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.695477784606563, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.695477784606563, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.695477784606563, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.695477784606563, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.34216346  0.39332372]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.342163460371637   |
| 0.39332372248068737  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00043194315610641133

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.16542794020270984

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.002611065310836384

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.003504476452848505  |
| 0.0010269939276983597  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.731481560043303, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.731481560043303, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.731481560043303, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.731481560043303, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.731481560043303, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.72388162 -0.24537518]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7238816213117616  |
| -0.24537518239498013  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.935276844925096e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.31042665864155883

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.455621040312713e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.844750289171172e-08  |
| -2.3201747374245438e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.229495417953454, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.229495417953454, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.229495417953454, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.229495417953454, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.229495417953454, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.21448866 -0.48437479]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.21448865572537557  |
| -0.4843747913874097  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.596733555173521e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3928673925319119

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1700471056019587e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.509618308159307e-07  |
| -5.667413226893912e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.2106314983131234, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.2106314983131234, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.2106314983131234, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.2106314983131234, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.2106314983131234, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.1953464  -0.48829738]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.19534639794649422  |
| -0.48829737586686406  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.072413866786833e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39438897955930974

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2861449304325768e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5124377939714825e-07  |
| -6.280211945146976e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.709619903202896, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.709619903202896, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.709619903202896, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.709619903202896, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.709619903202896, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.31305764 -0.45837497]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -0.313057639811376  |
| -0.458374967982067  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.0879548751672245e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3829292923184999

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.589837862313127e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.977108888585109e-06  |
| -7.287418792344574e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.1777980316709495, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.1777980316709495, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.1777980316709495, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.1777980316709495, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.1777980316709495, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.85272656 -0.14380628]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.852726562694528    |
| -0.14380628021726238  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.468787807257307e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2808598011215471

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00023032088541776835

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00019640073693905384  |
| -3.3121589788275574e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.497622685783328, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.497622685783328, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.497622685783328, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.497622685783328, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.497622685783328, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.48657215 -0.38900125]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.4865721492386399  |
| -0.3890012534313314  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0911470912064195e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3576251457335318

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.051091636658677e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.4845762151730517e-07  |
| -1.1868784709940777e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (3.236238053622853, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.236238053622853, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.236238053622853, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.236238053622853, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.236238053622853, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.80817929  1.12737841]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.8081792907409522  |
| 1.1273784128462694  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.002121659097799586

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.08025061616893553

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.026437916605316556

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.047804493296069735  |
| 0.02980553646146381  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.809400694679507, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.809400694679507, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.809400694679507, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.809400694679507, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.809400694679507, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.80295045 -0.18501277]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8029504505735474  |
| -0.18501277176952158  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.8723119330491222e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.29249949748330745

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.40107743486285e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.139748010479292e-08  |
| -1.184281078535315e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.734874894609524, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.734874894609524, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.734874894609524, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.734874894609524, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.734874894609524, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.28743001 -0.46606951]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.28743001090347775  |
| -0.4660695052116637   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.403527013227127e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38584394907569913

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.400443631725064e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -4.0252952833644115e-06  |
| -6.527040705149259e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.459385056453483, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.459385056453483, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.459385056453483, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.459385056453483, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.459385056453483, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.44777032 -0.40712835]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.44777032304210707  |
| -0.407128346591179   |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3454311515241644e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3640709919102219

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.6955186802027365e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.6547435932425204e-07  |
| -1.5045504100677562e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.184849798656908, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.184849798656908, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.184849798656908, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.184849798656908, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.184849798656908, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.84557074 -0.14988263]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8455707445342853   |
| -0.14988262853066203  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.28083550046486e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2825466344054184

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00022229376448535792

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00018796510394121315  |
| -3.3317973727041374e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.752802221204449, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.752802221204449, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.752802221204449, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.752802221204449, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.752802221204449, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.74551687 -0.22947978]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7455168660186473  |
| -0.22947978184362228  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.5970482524996115e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.30560193673158

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.498140686787213e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.335507211799158e-08  |
| -1.95015147088034e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (3.4456366356424297, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.4456366356424297, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.4456366356424297, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.4456366356424297, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.4456366356424297, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.59569103  0.76573721]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.595691028200008  |
| 0.765737211416706  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0010540193760895024

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.11461024945311071

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.009196554244659613

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.014674859098558046  |
| 0.007042143801948123  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.427602586491956, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.427602586491956, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.427602586491956, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.427602586491956, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.427602586491956, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.59923613 -0.32783557]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -0.5992361340467767  |
| -0.32783557202265  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.2081489818673903e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.33670563112861784

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.558099353627684e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -3.929850103362518e-05  |
| -2.149978252977903e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.857322360163403, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.857322360163403, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.857322360163403, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.857322360163403, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.857322360163403, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.16317583 -0.49406433]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.16317582951508314  |
| -0.4940643250073151   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.0035104947874737e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39663671093915387

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.572447057852464e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.2356403301241266e-06  |
| -3.741275944291507e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.615807021617604, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.615807021617604, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.615807021617604, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.615807021617604, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.615807021617604, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.6065003  -0.32345618]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6065002988719925  |
| -0.3234561751774834  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.658155517039914e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.335255635651327

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.687713766853577e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0235989040070706e-07  |
| -5.459014398208409e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.106479277730971, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.106479277730971, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.106479277730971, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.106479277730971, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.106479277730971, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.92509763 -0.07947477]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9250976251706788   |
| -0.07947477120850976  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.691981226642622e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2636070420829272

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0003297325123775807

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.00030503476414206135  |
| -2.620541598121534e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.7652235553793725, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.7652235553793725, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.7652235553793725, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.7652235553793725, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.7652235553793725, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.75812148 -0.2200034 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7581214767071742  |
| -0.22000339550132253  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.4177585428498404e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.30276132745479795

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.985691446047745e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.054124191605566e-08  |
| -1.7568792335563702e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.695996151221829, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.695996151221829, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.695996151221829, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.695996151221829, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.695996151221829, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.32688241 -0.45395146]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.32688241113909555  |
| -0.4539514575352399   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.490776042055121e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3812636632103776

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7024376221432817e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.564969147401044e-06  |
| -7.728240399347708e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.310447194589536, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.29663488 -0.46338136]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2966348777722061  |
| -0.46338135550882953  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.000360399116986e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38482317699408464

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.796724777736311e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.3127805014673415e-07  |
| -3.612856896036729e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (5.721829412707804, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.721829412707804, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.721829412707804, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.721829412707804, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.721829412707804, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.71408706 -0.25241732]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7140870561528345  |
| -0.25241732037883935  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.10209067876997e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3125884323444557

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.923881877214292e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 7.086515595308418e-08  |
| -2.5049596712025576e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.903454622985064, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.903454622985064, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.903454622985064, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.903454622985064, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.903454622985064, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.89839215 -0.10382326]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8983921517824456  |
| -0.10382325665858616  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0793445599030681e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2700086421381562

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.9974444942054725e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.591272760780144e-08  |
| -4.150277057003469e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.204336062374974, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.204336062374974, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.204336062374974, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.204336062374974, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.204336062374974, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.18895807 -0.48952491]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.18895807363250583  |
| -0.4895249050562711  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.24146056192215e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3948663536448673

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3274011608079898e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5082316628382985e-07  |
| -6.497959272161152e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61510>

args = (4.059651123910417, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.059651123910417, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.059651123910417, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.059651123910417, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.059651123910417, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.97261673 -0.03438594]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9726167315982082   |
| -0.03438593854099281  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00010523420613487524

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2521505399425503

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0004173467411921929

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.0004059184233615139   |
| -1.4350859392918377e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.680442034038884, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.680442034038884, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.680442034038884, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.680442034038884, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.680442034038884, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.672089   -0.28152567]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6720890044498162  |
| -0.28152566722283723  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.927611026347099e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.32168490527139587

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2209497436733913e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.205868977087078e-08  |
| -3.4372869123320355e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.242708927436795, array([5.01812548, 0.98545951])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.01812548 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812558 0.98545951]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.242708927436795, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.242708927436795, array([5.01812558, 0.98545951])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.01812548 0.98545961]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.242708927436795, array([5.01812548, 0.98545951]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.242708927436795, array([5.01812548, 0.98545961])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.78685791 -0.19780489]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.7868579054992608  |
| -0.1978048858042314  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.92191366061201e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.29621011816762605

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00016616291472618388

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.00013074660305309734  |
| -3.286783637231104e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                     â€”
| 1.22822612339263e-07  |
| -7.962242374412896e-08  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                        â€”
| -6.754625570590944e-05   |
| -2.7181118065323674e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                      â€”
| 5.211576955750966e-09  |
| 1.6662391736594188e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                        â€”
| -2.3817285190690228e-07  |
| -2.2321955782906202e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                      â€”
| -0.0019840759859620378  |
| 0.0004232509329471683  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                        â€”
| -7.979483678629937e-05   |
| -2.8886941701599194e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                   â€”
| -0.8255828455313883  |
| 0.7819776000445341  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                      â€”
| -0.003956716998389412  |
| 0.0012249670737950215  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                      â€”
| -0.003504476452848505  |
| 0.0010269939276983597  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                      â€”
| 6.844750289171172e-08  |
| -2.3201747374245438e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                      â€”
| 2.509618308159307e-07  |
| -5.667413226893912e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                       â€”
| 2.5124377939714825e-07  |
| -6.280211945146976e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                       â€”
| -4.977108888585109e-06  |
| -7.287418792344574e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                        â€”
| -0.00019640073693905384  |
| -3.3121589788275574e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                       â€”
| 1.4845762151730517e-07  |
| -1.1868784709940777e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                    â€”
| -0.047804493296069735  |
| 0.02980553646146381  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                      â€”
| 5.139748010479292e-08  |
| -1.184281078535315e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                        â€”
| -4.0252952833644115e-06  |
| -6.527040705149259e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                       â€”
| 1.6547435932425204e-07  |
| -1.5045504100677562e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                        â€”
| -0.00018796510394121315  |
| -3.3317973727041374e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                      â€”
| 6.335507211799158e-08  |
| -1.95015147088034e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                     â€”
| -0.014674859098558046  |
| 0.007042143801948123  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| -3.929850103362518e-05  |
| -2.149978252977903e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                        â€”
| -1.2356403301241266e-06  |
| -3.741275944291507e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                       â€”
| 1.0235989040070706e-07  |
| -5.459014398208409e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                       â€”
| -0.00030503476414206135  |
| -2.620541598121534e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                      â€”
| 6.054124191605566e-08  |
| -1.7568792335563702e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| -5.564969147401044e-06  |
| -7.728240399347708e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                       â€”
| 2.3127805014673415e-07  |
| -3.612856896036729e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                      â€”
| 7.086515595308418e-08  |
| -2.5049596712025576e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| 3.591272760780144e-08  |
| -4.150277057003469e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                       â€”
| 2.5082316628382985e-07  |
| -6.497959272161152e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                        â€”
| -0.0004059184233615139   |
| -1.4350859392918377e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                      â€”
| 8.205868977087078e-08  |
| -3.4372869123320355e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| -0.00013074660305309734  |
| -3.286783637231104e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.89893425  0.8212528 ]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.02711483 0.97724699]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.01812548 0.98545951]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.02711483 0.97724699]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 10              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.02711483 0.97724699]

[5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1000

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [5.0053447581300405, 4.167114631122481, 5.68324474562505, 5.59654302219537, 3.0500257779373445, 4.398735026901141, 5.638650936706819, 4.622945444386177, 5.776381406378199, 5.361952384984476, 3.7035403343567204, 4.530333967265844, 5.046460597885442, 4.488209060437271, 3.812411132294809, 3.748202528473394, 4.338418726764614, 4.6324562902621675, 5.854834685558841, 3.94709156894294, 6.328013047797941, 7.241251340705833, 4.479009875440771, 3.9642028572820385, 5.869158131059629, 5.7340549665814295, 4.242708927436795, 4.376619005816564, 4.818347791548248, 4.450585687671854, 3.4723592737079136, 5.950265838791173, 5.352430620406992, 3.2630334611788037, 4.963679938297505]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.0053447581300405, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.0053447581300405, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.0053447581300405, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.0053447581300405, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.0053447581300405, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.02227698 -0.51139323]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.022276984701363745  |
| -0.5113932266542776    |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4474942395761436e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4034619526004094

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.587684613745348e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -7.99227952537232e-08  |
| -1.834717610841139e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.167114631122481, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.167114631122481, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.167114631122481, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.167114631122481, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.167114631122481, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.88002343 -0.12442082]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8800234341244106   |
| -0.12442082475416782  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.763664015246625e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.27641808960301195

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0002446896302973698

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00021533260874892382  |
| -3.0444485610391165e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.68324474562505, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.68324474562505, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.68324474562505, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.68324474562505, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.68324474562505, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.67140639 -0.28624808]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6714063927049096  |
| -0.286248076353246  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.865560888412778e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3237785348950082

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.19389041329323e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.015856556741813e-08  |
| -3.417488341817689e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.59654302219537, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.59654302219537, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.59654302219537, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.59654302219537, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.59654302219537, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.58268601 -0.34187985]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5826860127733369  |
| -0.3418798510956833  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.303438707690443e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.34186835580679503

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.8438204649899788e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0743683950148907e-07  |
| -6.303650660179475e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (3.0500257779373445, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.0500257779373445, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.0500257779373445, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.0500257779373445, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.0500257779373445, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.02312121  1.53486803]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.0231212083388073  |
| 1.5348680282301075  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0038094625749144524

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.05461886524798427

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0697462782798301

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.14110517479062457  |
| 0.1070513326197512  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.398735026901141, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.398735026901141, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.398735026901141, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.398735026901141, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.398735026901141, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.64301027 -0.30491031]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6430102716947772  |
| -0.3049103081131932  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.5081665130302705e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3297376569028448

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.606551634377892e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.8910908330816796e-05  |
| -2.319316002517076e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.638650936706819, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.638650936706819, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.638650936706819, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.638650936706819, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.638650936706819, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.62577431 -0.3158446 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6257743145887673  |
| -0.3158445993634018  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.97562015809551e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3332799622603137

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4929250844697413e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.342341714664298e-08  |
| -4.7153232518391827e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.622945444386177, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.622945444386177, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.622945444386177, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.622945444386177, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.622945444386177, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.41357961 -0.42611734]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4135796127791025  |
| -0.42611733697128784  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.122644543013082e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3712020699199203

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.457595278221675e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.016411303534671e-05  |
| -1.0472239553090315e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.776381406378199, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.776381406378199, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.776381406378199, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.776381406378199, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.776381406378199, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.76671153 -0.21771806]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7667115298204408  |
| -0.21771806357762102  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.2669858613237012e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.30280498749413975

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.486619953271327e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 5.7400778375568964e-08  |
| -1.629972398967813e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.361952384984476, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.361952384984476, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.361952384984476, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.361952384984476, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.361952384984476, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.34263345 -0.4529425 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3426334538314535  |
| -0.45294250350380594  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.27935025923384e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38106173331596366

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.981577418963448e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0494885304196774e-07  |
| -2.709310651047138e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.7035403343567204, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7035403343567204, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7035403343567204, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7035403343567204, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7035403343567204, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.35439102  0.40554599]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.354391023777879  |
| 0.4055459945639939  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00041924963104034835

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1646802981573151

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.002545839640391283

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0034480623569238564  |
| 0.0010324550689629233  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.530333967265844, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.530333967265844, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.530333967265844, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.530333967265844, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.530333967265844, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.50834734 -0.38243289]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5083473442191178  |
| -0.3824328875801086  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3937863049535659e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3556887580631766

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.918555966016797e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.9919875184986182e-05  |
| -1.4985846732280658e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.046460597885442, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.046460597885442, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.046460597885442, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.046460597885442, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.046460597885442, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.01979614 -0.51144541]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| 0.019796144723471798  |
| -0.511445411577327    |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.177260043884164e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.403482529211283

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.917747259554065e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 5.77601470166455e-08  |
| -1.492268448041247e-06  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.488209060437271, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.488209060437271, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.488209060437271, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.488209060437271, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.488209060437271, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.55145304 -0.35959118]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.5514530365680059   |
| -0.35959117683503905  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6853557121696313e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3478370348620483

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.8452451672894496e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.6719251604182228e-05  |
| -1.742307411759899e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (3.812411132294809, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.812411132294809, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.812411132294809, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.812411132294809, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.812411132294809, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.24298541  0.26086486]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.2429854145956654  |
| 0.2608648608237729  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00027847622952710885

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.18969077073729218

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0014680536561938377

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0018247692824927798  |
| 0.00038296361270483646  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.748202528473394, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.748202528473394, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.748202528473394, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.748202528473394, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.748202528473394, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.30868897  0.34469189]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.3086889683044944  |
| 0.34469189280628143  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0003549798145841364

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.17477081774234643

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0020311160591321414

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0026580991799323326  |
| 0.0007001092389314928  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.338418726764614, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.338418726764614, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.338418726764614, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.338418726764614, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.338418726764614, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.7047309 -0.2633186]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.7047309025232096  |
| -0.2633185980371877  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.2643099977555876e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3166040633916459

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00010310385668416288

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -7.266047397465375e-05   |
| -2.7149162994300894e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.6324562902621675, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6324562902621675, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6324562902621675, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6324562902621675, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6324562902621675, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.40384733 -0.43009505]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.40384732891141084  |
| -0.430095051795476    |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.729834256528748e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37264781859046586

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.342650036044542e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -9.460729596308086e-06  |
| -1.0075621885912511e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.854834685558841, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.854834685558841, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.854834685558841, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.854834685558841, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.854834685558841, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.84699142 -0.15294412]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8469914192232864  |
| -0.15294411737087898  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.43648278178186e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.28423144252398247

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.05391933076037e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.280626306600727e-08  |
| -7.729672313067682e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.94709156894294, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.94709156894294, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.94709156894294, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.94709156894294, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.94709156894294, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.10516924  0.09905805]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.1051692405672497  |
| 0.09905804843057808  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0001651420640911701

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22218724302075127

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0007432562817107668

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.000821423980405126  |
| 7.362551675003652e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (6.328013047797941, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.328013047797941, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.328013047797941, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.328013047797941, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.328013047797941, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.33118669 0.37438761]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.331186685327168  |
| 0.3743876098027954  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.044895815840499e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.16977184503010298

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.738651343757279e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.308029575217383e-09  |
| 1.774092350278092e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (7.241251340705833, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.241251340705833, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.241251340705833, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.241251340705833, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.241251340705833, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.26568769 2.05502885]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.2656876907944934  |
| 2.0550288493126345  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.639222580372072e-12

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0328533492527535

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.9895143650679254e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 1.13046812799767e-10  |
| 1.0253595964274399e-10  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.479009875440771, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.479009875440771, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.479009875440771, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.479009875440771, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.479009875440771, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.5608664  -0.35435584]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5608664022105359  |
| -0.3543558424468074  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7563226145665258e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.34606197025773

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.07516793381978e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.846491179655779e-05  |
| -1.798415408747731e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (3.9642028572820385, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9642028572820385, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9642028572820385, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9642028572820385, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9642028572820385, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.08765955  0.07986018]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.0876595535336264  |
| 0.07986017624972419  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00015433412236450736

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22639505978905564

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0006817026948746528

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0007414604487500348  |
| 5.444089736260172e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.869158131059629, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.869158131059629, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.869158131059629, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.869158131059629, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.869158131059629, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.86164835 -0.14042241]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8616483548884446  |
| -0.14042240703560083  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3207947345517934e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2807745515585868

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.704111277963147e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.053289743869125e-08  |
| -6.605626286149014e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.7340549665814295, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.7340549665814295, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.7340549665814295, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.7340549665814295, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.7340549665814295, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.72339961 -0.24998785]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7233996135447285  |
| -0.2499878459794047  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8922913158904048e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3125062736384725

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.255146407833062e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 6.69516933472632e-08  |
| -2.3136741147182123e-08  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.242708927436795, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.242708927436795, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.242708927436795, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.242708927436795, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.242708927436795, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.80266909 -0.1895026 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8026690911933088   |
| -0.18950259583760953  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.92191366061201e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2945696599600366

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00016708827586927152

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00013411659454104502  |
| -3.1663662011257565e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.376619005816564, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.376619005816564, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.376619005816564, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.376619005816564, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.376619005816564, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.66564122 -0.2901023 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.6656412154804059   |
| -0.29010230218062816  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.763754194100631e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.32500035623899415

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.50384973753155e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -5.6605128755532316e-05  |
| -2.4669863862560333e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.818347791548248, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.818347791548248, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.818347791548248, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.818347791548248, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.818347791548248, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.21362776 -0.48882296]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.21362775992628258  |
| -0.48882296099783673  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.6267404767610127e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3946603271064782

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.189523820017838e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.963137388459626e-06  |
| -4.492050243861271e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (4.450585687671854, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.450585687671854, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.450585687671854, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.450585687671854, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.450585687671854, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.58995238 -0.3376195 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5899523847219257  |
| -0.3376194968041091  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.993975630619758e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3404479739018111

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.8569173074146196e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.455302332628375e-05   |
| -1.9774094741526013e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.4723592737079136, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.4723592737079136, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.4723592737079136, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.4723592737079136, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.4723592737079136, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.59095461  0.75392673]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.5909546124959206  |
| 0.7539267254941251  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0009609597858135344

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.11716133957053222

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.008202021155920872

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.013049043389801434  |
| 0.006183722952516962  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.950265838791173, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.950265838791173, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.950265838791173, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.950265838791173, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.950265838791173, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.94464448 -0.06546476]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9446444759753092  |
| -0.06546476472024665  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.178391781844495e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2609424540023085

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.134174472725754e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.960680602403211e-08  |
| -2.051779944491946e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61ab0>

args = (5.352430620406992, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.352430620406992, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.352430620406992, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.352430620406992, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.352430620406992, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.33289    -0.45623347]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.33288999889968807  |
| -0.4562334698920978  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.398636245045867e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3822892342445322

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.274401762283407e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.088685595742724e-07  |
| -2.862592087503652e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.2630334611788037, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.2630334611788037, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.2630334611788037, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.2630334611788037, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.2630334611788037, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.80515412  1.11764907]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.8051541150754247  |
| 1.117649066628701  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.001944729827769956

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.08211373982097021

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.023683366900715812

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.04275212721966826  |
| 0.026469692911210096  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.963679938297505, array([5.02711483, 0.97724699])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02711483 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711493 0.97724699]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.963679938297505, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.963679938297505, array([5.02711493, 0.97724699])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02711483 0.97724709]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.963679938297505, array([5.02711483, 0.97724699]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.963679938297505, array([5.02711483, 0.97724709])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.06491188 -0.50953458]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.06491187809487542  |
| -0.5095345834149612   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7816016441859807e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40272978880644833

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.4238139161893915e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.871580696420992e-07  |
| -2.2540861808908694e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| -7.99227952537232e-08  |
| -1.834717610841139e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                        â€”
| -0.00021533260874892382  |
| -3.0444485610391165e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                      â€”
| 8.015856556741813e-08  |
| -3.417488341817689e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                       â€”
| 1.0743683950148907e-07  |
| -6.303650660179475e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                   â€”
| -0.14110517479062457  |
| 0.1070513326197512  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| -4.8910908330816796e-05  |
| -2.319316002517076e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                      â€”
| 9.342341714664298e-08  |
| -4.7153232518391827e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| -1.016411303534671e-05  |
| -1.0472239553090315e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| 5.7400778375568964e-08  |
| -1.629972398967813e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| 2.0494885304196774e-07  |
| -2.709310651047138e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                      â€”
| -0.0034480623569238564  |
| 0.0010324550689629233  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                        â€”
| -1.9919875184986182e-05  |
| -1.4985846732280658e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                     â€”
| 5.77601470166455e-08  |
| -1.492268448041247e-06  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| -2.6719251604182228e-05  |
| -1.742307411759899e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                       â€”
| -0.0018247692824927798  |
| 0.00038296361270483646  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                      â€”
| -0.0026580991799323326  |
| 0.0007001092389314928  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                        â€”
| -7.266047397465375e-05   |
| -2.7149162994300894e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| -9.460729596308086e-06  |
| -1.0075621885912511e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                      â€”
| 4.280626306600727e-08  |
| -7.729672313067682e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                      â€”
| -0.000821423980405126  |
| 7.362551675003652e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                      â€”
| 6.308029575217383e-09  |
| 1.774092350278092e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                     â€”
| 1.13046812799767e-10  |
| 1.0253595964274399e-10  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| -2.846491179655779e-05  |
| -1.798415408747731e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                      â€”
| -0.0007414604487500348  |
| 5.444089736260172e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                      â€”
| 4.053289743869125e-08  |
| -6.605626286149014e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                     â€”
| 6.69516933472632e-08  |
| -2.3136741147182123e-08  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                        â€”
| -0.00013411659454104502  |
| -3.1663662011257565e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                        â€”
| -5.6605128755532316e-05  |
| -2.4669863862560333e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                       â€”
| -1.963137388459626e-06  |
| -4.492050243861271e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                        â€”
| -3.455302332628375e-05   |
| -1.9774094741526013e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                     â€”
| -0.013049043389801434  |
| 0.006183722952516962  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                      â€”
| 2.960680602403211e-08  |
| -2.051779944491946e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                      â€”
| 2.088685595742724e-07  |
| -2.862592087503652e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                     â€”
| -0.04275212721966826  |
| 0.026469692911210096  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| -2.871580696420992e-07  |
| -2.2540861808908694e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.2070584   0.14170968]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.02918541 0.97582989]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.02711483 0.97724699]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.02918541 0.97582989]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 11              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.02918541 0.97582989]

[5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245, 5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1100

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [3.7489606133168345, 3.569847757823836, 6.062707085052321, 2.737701062915051, 4.95555015932942, 5.03554097046952, 5.536335885482347, 5.721829412707804, 4.5804843268719955, 5.0053447581300405, 4.622945444386177, 5.407374007814408, 5.640430077423015, 6.197331007915066, 3.0500257779373445, 4.5173219458210845, 6.868663826183948, 5.214690338255851, 3.9271192815166494, 5.731481560043303, 4.6949464273938535, 5.313634285275245, 3.6742655068676946, 3.9904471584232315, 4.707226273961055, 3.9549964228576555, 5.652356047020777, 6.604675659370592, 4.884017788369837, 5.1727262744056235, 3.899847746811868, 5.649035187203196, 4.415717692938217, 5.362768994549103, 4.6538732335691355]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.7489606133168345, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7489606133168345, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7489606133168345, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7489606133168345, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7489606133168345, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.31193445  0.34820148]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.3119344499656904  |
| 0.34820147876146734  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00035397248518933313

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.17438487375619385

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.002029834799113479

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0026630102006961597  |
| 0.0007067914786927993  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.569847757823836, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.569847757823836, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.569847757823836, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.569847757823836, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.569847757823836, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.49548371  0.60585121]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.4954837102010288  |
| 0.6058512092010915  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0006817517016596471

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.13561804510924264

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.005026998443389186

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.007517794283294456  |
| 0.003045613085579343  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.062707085052321, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.062707085052321, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.062707085052321, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.062707085052321, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.062707085052321, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.05912069 0.04848395]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.059120693369664  |
| 0.04848395462886401  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.162511858410669e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23362987322874795

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7816693562706924e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.8870028839688985e-08  |
| 8.63823762330656e-10    |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (2.737701062915051, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.737701062915051, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.737701062915051, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.737701062915051, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.737701062915051, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.34824166  2.24473467]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.348241663341355  |
| 2.2447346736953477  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.009405674626799851

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0274007615214686

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.3432632563671805

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.8060650800956379  |
| 0.7705349337729854  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.95555015932942, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.95555015932942, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.95555015932942, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.95555015932942, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.95555015932942, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.07545916 -0.50953732]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.07545915670092995  |
| -0.5095373234453859   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.8549047257239706e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4027322791866814

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.605800978928121e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.4754985780223366e-07  |
| -2.3468275031251733e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.03554097046952, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.03554097046952, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.03554097046952, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.03554097046952, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.03554097046952, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.00651293 -0.51236315]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| 0.006512927974711147  |
| -0.5123631496850578   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.2438800814485445e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40384435739071356

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.0800977126074058e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0060454557284587e-08  |
| -1.5781285653692724e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.536335885482347, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.536335885482347, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.536335885482347, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.536335885482347, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.536335885482347, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.51971192 -0.37733411]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5197119223865343  |
| -0.3773341084034598  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.813059824524748e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35398840538983545

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.489646465911567e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.2938989508617415e-07  |
| -9.394285294545658e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.721829412707804, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.721829412707804, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.721829412707804, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.721829412707804, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.721829412707804, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.7097999 -0.2604764]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7097998966365537  |
| -0.26047640266924077  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.10209067876997e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3158384687805524

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.821763291682283e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.971486569224781e-08  |
| -2.558337570086202e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.5804843268719955, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5804843268719955, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5804843268719955, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5804843268719955, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5804843268719955, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.45981491 -0.40666952]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.45981491059166046  |
| -0.4066695202809001  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.109121873638561e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.36426824543790076

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.044794289728009e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.4000418141012827e-05  |
| -1.2382250331577132e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.0053447581300405, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.0053447581300405, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.0053447581300405, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.0053447581300405, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.0053447581300405, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.02443121 -0.51208592]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.024431205947195167  |
| -0.5120859192242477    |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4474942395761436e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40373511995357025

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.5852571848161394e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.759215665590427e-08  |
| -1.8359597211419115e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.622945444386177, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.622945444386177, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.622945444386177, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.622945444386177, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.622945444386177, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.41630208 -0.42573068]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4163020828062969  |
| -0.4257306784882786  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.122644543013082e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37110720582277074

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4582234998071617e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.023363562973106e-05  |
| -1.0465411584487338e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.407374007814408, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.407374007814408, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.407374007814408, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.407374007814408, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.407374007814408, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.38755582 -0.43728459]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3875558163279891  |
| -0.4372845940903147  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7848070025409587e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3753149880379851

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.7554908794644806e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.8430181498311635e-07  |
| -2.0795028989268193e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.640430077423015, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.640430077423015, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.640430077423015, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.640430077423015, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.640430077423015, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.6263844  -0.31620564]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6263843999043672  |
| -0.3162056394501178  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.9259467103288684e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3334900964114495

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4770893538773613e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.252257285336004e-08  |
| -4.6706398366775235e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (6.197331007915066, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.197331007915066, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.197331007915066, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.197331007915066, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.197331007915066, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.19707909 0.20411479]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.197079086967534  |
| 0.20411479173532143  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.8237074561548869e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.20071156666424225

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.08621005985994e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0876912042452359e-08  |
| 1.854629874031694e-09   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (3.0500257779373445, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.0500257779373445, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.0500257779373445, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.0500257779373445, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.0500257779373445, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.02818105  1.54437449]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.0281810453326443  |
| 1.5443744949550364  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0038094625749144524

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.05427171972579268

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.07019240580843437

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.1423629069869636  |
| 0.1084033612700798  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.5173219458210845, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5173219458210845, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5173219458210845, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5173219458210845, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5173219458210845, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.52454175 -0.37481238]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -0.5245417478150216  |
| -0.374812381132017  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4782931580624086e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35311838974263854

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.186395274230338e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.1959390941893283e-05  |
| -1.569112781094096e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (6.868663826183948, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.868663826183948, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.868663826183948, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.868663826183948, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.868663826183948, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.88504    1.26430344]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.885039999649507  |
| 1.264303439718617  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.2710132809432487e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.07132914494308357

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.18385042012684e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.001685394839981e-10  |
| 4.0253530377159275e-10  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.214690338255851, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.214690338255851, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.214690338255851, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.214690338255851, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.214690338255851, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.19009961 -0.49431542]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1900996082770945  |
| -0.49431542192834854  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.96622232643124e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3967943096507775

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2515860751133402e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.3792602260411214e-07  |
| -6.186782987992964e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.9271192815166494, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9271192815166494, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9271192815166494, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9271192815166494, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9271192815166494, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.12936301  0.12534592]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.1293630119979525  |
| 0.1253459158689907  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00017865187845393117

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2167476810848488

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0008242389379196887

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.000930864969534973   |
| 0.00010331498456842754  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.731481560043303, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.731481560043303, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.731481560043303, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.731481560043303, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.731481560043303, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.71969112 -0.2534067 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7196911155915586  |
| -0.25340670006812616  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.935276844925096e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.31366705179063986

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.35793806894412e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 6.73482488847511e-08  |
| -2.3713642054930224e-08  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.6949464273938535, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6949464273938535, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6949464273938535, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6949464273938535, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6949464273938535, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.34251772 -0.45372519]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.34251772418336657  |
| -0.4537251885317062   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.522847715144428e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3813848108374759

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7103061081066724e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -5.8581015580560835e-06  |
| -7.760089613476286e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.313634285275245, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.313634285275245, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.313634285275245, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.313634285275245, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.313634285275245, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.29149428 -0.46989989]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.29149427538399664  |
| -0.46989989455070713  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.9499920013089745e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38745225559663726

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.613820693252347e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.219385145883272e-07  |
| -3.5777335408872696e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.6742655068676946, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.6742655068676946, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.6742655068676946, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.6742655068676946, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.6742655068676946, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.38847966  0.45155335]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.3884796645768915  |
| 0.45155335248736606  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00046706036814022896

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.15765528655581285

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0029625417475289107

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.00411342897190398   |
| 0.0013377456579804596  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (3.9904471584232315, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9904471584232315, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9904471584232315, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9904471584232315, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9904471584232315, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.06446658  0.05416007]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.0644665793257957  |
| 0.05416007198277839  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0001390366771447839

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23233939186972968

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005984205950867786

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0006369987238501303  |
| 3.241050250587701e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.707226273961055, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.707226273961055, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.707226273961055, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.707226273961055, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.707226273961055, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.32993372 -0.45795625]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3299337203443997  |
| -0.45795625291944475  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.1569555570619945e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38296272751746296

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.6077166561284322e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.304399376161116e-06  |
| -7.362638955967563e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.9549964228576555, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9549964228576555, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9549964228576555, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9549964228576555, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9549964228576555, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.10079538  0.09349076]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.100795383734976   |
| 0.09349075957842956  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00016006398993133802

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22359114441698927

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0007158780386794951

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0007880352402956369  |
| 6.692798160166237e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.652356047020777, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.652356047020777, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.652356047020777, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.652356047020777, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.652356047020777, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.63860576 -0.30847569]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6386057616047935  |
| -0.30847568943670467  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.605161637849747e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.33098400511747283

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.39135473818902e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.885271522436373e-08  |
| -4.2919911211388366e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.604675659370592, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.604675659370592, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.604675659370592, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.604675659370592, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.604675659370592, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.61451316 0.79094196]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.614513163161746  |
| 0.7909419608154167  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3445267790332618e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.11320783004327258

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1876623538489605e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 1.91749650368081e-09  |
| 9.3937199093995e-10   |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.884017788369837, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.884017788369837, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.884017788369837, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.884017788369837, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.884017788369837, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.1487633  -0.50131911]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.14876329723634285  |
| -0.5013191095581959   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.6373039023102018e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3995154522379496

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.601256315711752e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -9.820246554275128e-07  |
| -3.3093359381580326e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.1727262744056235, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.1727262744056235, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.1727262744056235, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.1727262744056235, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.1727262744056235, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.14709614 -0.50156572]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.14709614415409078  |
| -0.5015657156270947  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.175615556386081e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3996116054071374

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.5454044559326953e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.2732303662624997e-07  |
| -7.751218918731833e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.899847746811868, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.899847746811868, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.899847746811868, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.899847746811868, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.899847746811868, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.15731003  0.15729876]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.1573100278106097  |
| 0.15729875624259648  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0001987734632396922

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.21009364858884336

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.000946118383753214

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0010949522930135613  |
| 0.00014882324502263618  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.649035187203196, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.649035187203196, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.649035187203196, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.649035187203196, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.649035187203196, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.63520265 -0.31064315]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6352026460376692  |
| -0.3106431489818817  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.6923941765866685e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3316848002270096

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4147148658531022e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.986306261787168e-08  |
| -4.3947148084008804e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.415717692938217, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.415717692938217, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.415717692938217, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.415717692938217, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.415717692938217, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.62866261 -0.31477608]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6286626130780348  |
| -0.3147760763155816  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.3272924654558755e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3330251970967745

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.988337476396968e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -4.393306498982877e-05   |
| -2.1997614507893707e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.362768994549103, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.362768994549103, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.362768994549103, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.362768994549103, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.362768994549103, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.34184599 -0.45395501]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.34184599040365526  |
| -0.45395500802847266  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.2693909086436316e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3814703516136858

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.94906235581275e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.033663112995912e-07  |
| -2.7006066494948613e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.6538732335691355, array([5.02918541, 0.97582989])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.02918541 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918551 0.97582989]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6538732335691355, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6538732335691355, array([5.02918551, 0.97582989])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.02918541 0.97582999]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6538732335691355, array([5.02918541, 0.97582989]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6538732335691355, array([5.02918541, 0.97582999])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.38460825 -0.43842263]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -0.384608250802998  |
| -0.4384226348630449  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.903486472054307e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3757320200710732

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1034902669618867e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.090197121573425e-06  |
| -9.222177452502e-06     |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| -0.0026630102006961597  |
| 0.0007067914786927993  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                     â€”
| -0.007517794283294456  |
| 0.003045613085579343  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| 1.8870028839688985e-08  |
| 8.63823762330656e-10    |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                   â€”
| -0.8060650800956379  |
| 0.7705349337729854  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                        â€”
| -3.4754985780223366e-07  |
| -2.3468275031251733e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| 2.0060454557284587e-08  |
| -1.5781285653692724e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 1.2938989508617415e-07  |
| -9.394285294545658e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                      â€”
| 6.971486569224781e-08  |
| -2.558337570086202e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                        â€”
| -1.4000418141012827e-05  |
| -1.2382250331577132e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| -8.759215665590427e-08  |
| -1.8359597211419115e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                       â€”
| -1.023363562973106e-05  |
| -1.0465411584487338e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                       â€”
| 1.8430181498311635e-07  |
| -2.0795028989268193e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                      â€”
| 9.252257285336004e-08  |
| -4.6706398366775235e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| 1.0876912042452359e-08  |
| 1.854629874031694e-09   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                   â€”
| -0.1423629069869636  |
| 0.1084033612700798  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| -2.1959390941893283e-05  |
| -1.569112781094096e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                      â€”
| 6.001685394839981e-10  |
| 4.0253530377159275e-10  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| 2.3792602260411214e-07  |
| -6.186782987992964e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                       â€”
| -0.000930864969534973   |
| 0.00010331498456842754  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                     â€”
| 6.73482488847511e-08  |
| -2.3713642054930224e-08  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                        â€”
| -5.8581015580560835e-06  |
| -7.760089613476286e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                      â€”
| 2.219385145883272e-07  |
| -3.5777335408872696e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                      â€”
| -0.00411342897190398   |
| 0.0013377456579804596  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                      â€”
| -0.0006369987238501303  |
| 3.241050250587701e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                       â€”
| -5.304399376161116e-06  |
| -7.362638955967563e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                      â€”
| -0.0007880352402956369  |
| 6.692798160166237e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                      â€”
| 8.885271522436373e-08  |
| -4.2919911211388366e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                     â€”
| 1.91749650368081e-09  |
| 9.3937199093995e-10   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                       â€”
| -9.820246554275128e-07  |
| -3.3093359381580326e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                       â€”
| 2.2732303662624997e-07  |
| -7.751218918731833e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                       â€”
| -0.0010949522930135613  |
| 0.00014882324502263618  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                      â€”
| 8.986306261787168e-08  |
| -4.3947148084008804e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                        â€”
| -4.393306498982877e-05   |
| -2.1997614507893707e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                      â€”
| 2.033663112995912e-07  |
| -2.7006066494948613e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| -8.090197121573425e-06  |
| -9.222177452502e-06     |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.9662822   0.88428347]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.03884823 0.96698705]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.02918541 0.97582989]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.03884823 0.96698705]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 12              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.03884823 0.96698705]

[4.490416274328351, 3.5564524988044712, 4.091629946863637, 4.879829892905114, 4.1926418390225075, 4.210216335775425, 4.187504980422656, 4.530710047380715, 4.009372181787566, 3.9432288559544406, 4.411784013963423, 5.289816410463862, 3.4893621991347263, 4.70812136860741, 4.748987698855443, 3.7989359743023168, 4.515802235408018, 6.849494774956813, 3.8533509627750253, 5.262683037408525, 5.174250961590373, 4.811616579786701, 5.4144120224117165, 5.534109611204314, 4.757576389142094, 5.526458646858388, 4.68648626135325, 6.510397251598002, 4.053569161498328, 4.923389693719144, 5.171489048917507, 4.38273385252844, 3.5798745643728505, 5.776623075584453, 5.2610244417351675, 4.290574225520197, 4.856593546566716, 4.745192874359674, 3.6986305219136373, 4.379386401302874, 5.416012422669541, 4.659283968868628, 5.063523399259809, 5.7467416662796, 4.506312504147869, 4.476540161369129, 4.887133675349967, 7.101314492424933, 4.975301110000693, 4.862850942851848, 7.180855356630156, 2.9847168625895826, 6.5661988659086585, 4.130266496729279, 4.635203270500523, 5.2680387343232065, 5.599672587970324, 4.400983390586819, 2.518320319314273, 4.116474903067948, 4.804012214397902, 6.84000545726421, 5.297420062264238, 5.198714774949839, 6.736278646645381, 2.7653837751939716, 3.7074394406887117, 4.321519160329914, 4.295293892672745, 6.226190166839307, 4.668913001705613, 6.094720511993962, 5.253733943180695, 5.065868524410168, 5.120953283929446, 4.379225050802219, 5.923121760179944, 4.267138358789313, 4.759398905325478, 5.266646686893115, 6.473242173540947, 5.087655203318958, 3.965053905052038, 4.353966609391555, 5.343060515208491, 5.783994897105108, 5.01033882071759, 7.4186928993875325, 4.410049001959045, 7.374056817127453, 4.509450562163232, 5.746852509456756, 3.887434446403901, 4.02234765799981, 4.249898068401506, 4.9494169910636625, 4.938595751187503, 3.9514719173927, 4.478787355638231, 6.566513640431479]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245, 5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287, 4.490416274328351, 3.5564524988044712, 4.091629946863637, 4.879829892905114, 4.1926418390225075, 4.210216335775425, 4.187504980422656, 4.530710047380715, 4.009372181787566, 3.9432288559544406, 4.411784013963423, 5.289816410463862, 3.4893621991347263, 4.70812136860741, 4.748987698855443, 3.7989359743023168, 4.515802235408018, 6.849494774956813, 3.8533509627750253, 5.262683037408525, 5.174250961590373, 4.811616579786701, 5.4144120224117165, 5.534109611204314, 4.757576389142094, 5.526458646858388, 4.68648626135325, 6.510397251598002, 4.053569161498328, 4.923389693719144, 5.171489048917507, 4.38273385252844, 3.5798745643728505, 5.776623075584453, 5.2610244417351675, 4.290574225520197, 4.856593546566716, 4.745192874359674, 3.6986305219136373, 4.379386401302874, 5.416012422669541, 4.659283968868628, 5.063523399259809, 5.7467416662796, 4.506312504147869, 4.476540161369129, 4.887133675349967, 7.101314492424933, 4.975301110000693, 4.862850942851848, 7.180855356630156, 2.9847168625895826, 6.5661988659086585, 4.130266496729279, 4.635203270500523, 5.2680387343232065, 5.599672587970324, 4.400983390586819, 2.518320319314273, 4.116474903067948, 4.804012214397902, 6.84000545726421, 5.297420062264238, 5.198714774949839, 6.736278646645381, 2.7653837751939716, 3.7074394406887117, 4.321519160329914, 4.295293892672745, 6.226190166839307, 4.668913001705613, 6.094720511993962, 5.253733943180695, 5.065868524410168, 5.120953283929446, 4.379225050802219, 5.923121760179944, 4.267138358789313, 4.759398905325478, 5.266646686893115, 6.473242173540947, 5.087655203318958, 3.965053905052038, 4.353966609391555, 5.343060515208491, 5.783994897105108, 5.01033882071759, 7.4186928993875325, 4.410049001959045, 7.374056817127453, 4.509450562163232, 5.746852509456756, 3.887434446403901, 4.02234765799981, 4.249898068401506, 4.9494169910636625, 4.938595751187503, 3.9514719173927, 4.478787355638231, 6.566513640431479]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1200

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [5.310447194589536, 4.872723657480988, 4.54283903123057, 6.067302259045171, 5.127142048521152, 5.966834716727361, 5.203379054351345, 6.0186986796766915, 3.874129227058206, 5.683552641208301, 4.515629132401616, 5.830168658572052, 6.766802278820994, 5.018158035019807, 5.341174560288266, 5.11569859154836, 5.549917468523984, 5.198714774949839, 4.394530396440322, 5.013468469826582, 4.622296354773813, 5.3373694994337075, 3.987109903381666, 6.352534184519216, 5.0053447581300405, 4.5365608187111865, 4.758835667048313, 6.926584061561935, 4.627178804348573, 5.174250961590373, 2.7221024564318363, 4.384296078267645, 3.8154026296329313, 3.5453903236144364, 4.242708927436795]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.310447194589536, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.310447194589536, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.310447194589536, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.2808713  -0.47762562]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.28087130177212316  |
| -0.4776256223859576  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.000360399116986e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39051237031166564

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.68313791627757e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.157972948239639e-07  |
| -3.669663529139224e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.872723657480988, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.872723657480988, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.872723657480988, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.872723657480988, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.872723657480988, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.17179612 -0.50231303]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.17179611955242535  |
| -0.5023130333903225   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.7866888632340503e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39994700743529865

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.9676452415633245e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.1970144149185004e-06  |
| -3.49993901687732e-06    |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.54283903123057, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.54283903123057, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.54283903123057, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.54283903123057, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.54283903123057, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.51294301 -0.38551475]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.5129430125094814   |
| -0.38551475345371955  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.316917396621449e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3572334958917984

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.686433136214995e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.8909301183048948e-05  |
| -1.421174361631546e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.067302259045171, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.067302259045171, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.067302259045171, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.067302259045171, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.067302259045171, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.06356541 0.04851571]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0635654090762614  |
| 0.04851570700736829  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.0481053321489764e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2347891183597681

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7241452075926502e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.8337412030201525e-08  |
| 8.364812372972319e-10   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.127142048521152, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.127142048521152, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.127142048521152, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.127142048521152, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.127142048521152, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.09130812 -0.51290138]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.09130811884361378  |
| -0.5129013846971731  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.809682616212261e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40406302077656875

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.9327882569414124e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.764792598643476e-07  |
| -9.91329773311686e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.966834716727361, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.966834716727361, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.966834716727361, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.966834716727361, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.966834716727361, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.9596679  -0.05658873]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9596679029222344  |
| -0.056588733698959004  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.409545010790841e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.25990661322059033

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.8508489718582667e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.7358682543712307e-08  |
| -1.6132593328443852e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.203379054351345, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.203379054351345, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.203379054351345, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.203379054351345, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.203379054351345, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.17014785 -0.50259482]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.17014785469449123  |
| -0.5025948235370947  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.267628841603684e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40005600253266554

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3167228608633537e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.2403757000309272e-07  |
| -6.617780939028758e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.0186986796766915, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 1.0133025 -0.003679 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0133025041270116  |
| -0.003678997106959514  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.430115505065784e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24694344729183945

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1989307935142077e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.2281820794699433e-08  |
| -8.089860027742958e-11  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (3.874129227058206, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.874129227058206, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.874129227058206, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.874129227058206, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.874129227058206, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.20448257  0.20831902]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.2044825736801101  |
| 0.20831901981210876  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00021967153122829608

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.20117194509661987

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0010919590757189884

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0013152456778753614  |
| 0.0002274758443287159  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.683552641208301, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.683552641208301, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.683552641208301, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.683552641208301, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.683552641208301, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.66671457 -0.29481581]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6667145746597214  |
| -0.2948158051019334  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.858802484066658e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.32723707304693456

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1792070036982644e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 7.861944959064528e-08  |
| -3.476488621771424e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.515629132401616, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.515629132401616, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.515629132401616, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.515629132401616, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.515629132401616, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.54108185 -0.37068523]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5410818548767793  |
| -0.3706852336193833  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4896388396333622e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3521473389376096

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.230157876891648e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.2888616704501514e-05  |
| -1.568057060842455e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.830168658572052, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.830168658572052, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.830168658572052, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.830168658572052, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.830168658572052, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.81833606 -0.18223301]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                 â€”
| 0.81833606335735  |
| -0.1822330131240335  |
 â€”                 â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6591557971060096e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.29348272824171734

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.653333697169057e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.626326842586779e-08  |
| -1.0302240338307496e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.766802278820994, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.766802278820994, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.766802278820994, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.766802278820994, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.766802278820994, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.78694636 1.0795186 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.786946364212838  |
| 1.079518598956497  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.54801534276319e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.08663649858503988

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.249537339391654e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 9.38064166242545e-10  |
| 5.666973193789896e-10  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.018158035019807, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.018158035019807, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.018158035019807, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.018158035019807, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.018158035019807, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.02139661 -0.51684107]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.02139661225974976  |
| -0.5168410699418757   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3574622887892675e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40560528476680585

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.346756908184053e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -7.160925989205311e-08  |
| -1.7297414212612099e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.341174560288266, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.341174560288266, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.341174560288266, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.341174560288266, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.341174560288266, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.3126477  -0.46819567]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3126476999515404  |
| -0.4681956744523319  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.5474281357931464e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3869676150934999

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.583052525409992e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.0581762307296136e-07  |
| -3.082156717089458e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.11569859154836, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.11569859154836, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.11569859154836, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.11569859154836, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.11569859154836, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.07947398 -0.51391192]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.0794739796194932  |
| -0.513911916355525  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.28106081520121e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40445805176463484

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.0474461514788153e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.6271869371463716e-07  |
| -1.0522069753412226e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.549917468523984, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.549917468523984, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.549917468523984, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.549917468523984, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.549917468523984, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.5285171 -0.3774048]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5285170967539443  |
| -0.37740480296477585  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.173933561113373e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35444295070711024

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.3061351748726994e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.2188318673458688e-07  |
| -8.7034649128297e-08    |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.198714774949839, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.198714774949839, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.198714774949839, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.198714774949839, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.198714774949839, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.16532434 -0.5034039 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.16532433755322984  |
| -0.5034039007867364  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.396979591154976e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40036911611979314

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3480009755647993e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 2.22857368306358e-07  |
| -6.785889493636461e-07  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.394530396440322, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.394530396440322, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.394530396440322, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.394530396440322, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.394530396440322, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.6663149  -0.29508226]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.6663149032526405   |
| -0.29508225862784343  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.554964244279498e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3273213991696775

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.805674333424961e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -5.2010371382976725e-05  |
| -2.3033160124204238e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.013468469826582, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.013468469826582, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.013468469826582, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.013468469826582, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.013468469826582, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.02624628 -0.51672554]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                      â€”
| -0.026246278483910146  |
| -0.5167255434646023    |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3897709866835918e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4055599763296304

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.426795215990485e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.994062154621728e-08  |
| -1.7707126203245825e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.622296354773813, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.622296354773813, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.622296354773813, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.622296354773813, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.622296354773813, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.43077301 -0.42428731]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.4307730139263555   |
| -0.42428731417132326  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.15005810080441e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.370881345736216

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4671119769157267e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.0627652619897968e-05  |
| -1.0467643144454773e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.3373694994337075, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.3373694994337075, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.3373694994337075, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.3373694994337075, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.3373694994337075, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.30871274 -0.46941819]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.30871273537513844  |
| -0.46941818987455974  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.599711639549258e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.387425342196103

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.710226091078375e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.071532251562428e-07  |
| -3.1499021853230535e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.987109903381666, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.987109903381666, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.987109903381666, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.987109903381666, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.987109903381666, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.08764474  0.07441544]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.0876447387175858  |
| 0.07441543825592589  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00014089984712821682

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22898191608752513

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0006153317673975609

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0006692623593757504  |
| 4.5790183143682945e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (6.352534184519216, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.352534184519216, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.352534184519216, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.352534184519216, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.352534184519216, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.35853516 0.40573888]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.3585351554823433  |
| 0.40573887583050805  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.886528514424369e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.16621091022160162

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.143246977736219e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.628746677100624e-09  |
| 1.6810763710348436e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.0053447581300405, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.0053447581300405, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.0053447581300405, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.0053447581300405, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.0053447581300405, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.03464733 -0.51646976]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.03464733278057963  |
| -0.5164697591819589   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4474942395761436e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4054596772643208

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.5700078719110614e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.2369125076739144e-07  |
| -1.8438011058836035e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.5365608187111865, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5365608187111865, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5365608187111865, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5365608187111865, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5365608187111865, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.51943556 -0.38216337]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5194355612303525  |
| -0.3821633653977585  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.354991120038136e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.35607766685923714

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.8053246416428145e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.9766209408954255e-05  |
| -1.4542556714812375e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.758835667048313, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.758835667048313, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.758835667048313, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.758835667048313, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.758835667048313, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.28957225 -0.47514395]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.28957224840198137  |
| -0.4751439530714663   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.822601784644709e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3895763648435166

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.23790923162955e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.5846415952053794e-06  |
| -5.8818508586012585e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.926584061561935, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.926584061561935, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.926584061561935, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.926584061561935, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.926584061561935, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.9521831  1.38843935]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.9521830996183098  |
| 1.3884393457175292  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5230487898959383e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.06426394835016888

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.369989440419958e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 4.6266533318616974e-10  |
| 3.2905865880141403e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.627178804348573, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.627178804348573, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.627178804348573, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.627178804348573, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.627178804348573, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.42572388 -0.4264496 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.42572387726202976  |
| -0.4264495978567595   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.945764508976631e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3716576332678127

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4069906570519446e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.0247133950536344e-05  |
| -1.026460197744779e-05   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.174250961590373, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.174250961590373, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.174250961590373, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.174250961590373, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.174250961590373, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.14002533 -0.50726643]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1400253291095055  |
| -0.5072664255312276  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.1270942171393e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4018672948398173

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.52465609812352e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.1349047191856038e-07  |
| -7.734068490595065e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (2.7221024564318363, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.7221024564318363, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.7221024564318363, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.7221024564318363, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.7221024564318363, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.39583955  2.35295317]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                 â€”
| -2.3958395489742657  |
| 2.35295316564077  |
 â€”                 â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.009814843220298694

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.02528790756285394

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.38812397569484836

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.9298827708748443  |
| 0.9132375372722746  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.384296078267645, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.384296078267645, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.384296078267645, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.384296078267645, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.384296078267645, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.67689862 -0.28797417]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.6768986238192554   |
| -0.28797416673498333  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.6723570702159817e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3250792905577607

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.220631543863765e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.5645341789665425e-05  |
| -2.367329518879487e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (3.8154026296329313, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.8154026296329313, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.8154026296329313, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.8154026296329313, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.8154026296329313, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.2652141   0.28331323]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.2652140979341198  |
| 0.283313228433002  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0002753170689931575

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.18709968556037052

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.001471499367669019

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0018617617450759855  |
| 0.00041689523649143074  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.5453903236144364, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.5453903236144364, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.5453903236144364, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.5453903236144364, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.5453903236144364, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.54444463  0.67558442]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.544444629963948  |
| 0.6755844150774237  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0007437282787342444

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.12803713661947871

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0058086919027608005

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.008971203016333985  |
| 0.003924261721491623  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.242708927436795, array([5.03884823, 0.96698705])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.03884823 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884833 0.96698705]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.242708927436795, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.242708927436795, array([5.03884833, 0.96698705])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.03884823 0.96698715]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.242708927436795, array([5.03884823, 0.96698705]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.242708927436795, array([5.03884823, 0.96698715])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.82331956 -0.17814251]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8233195591955678   |
| -0.17814250963255063  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.92191366061201e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.29232415960025915

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0001683717715067587

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00013862377269792143  |
| -2.9994169927492377e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| 2.157972948239639e-07  |
| -3.669663529139224e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                        â€”
| -1.1970144149185004e-06  |
| -3.49993901687732e-06    |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| -1.8909301183048948e-05  |
| -1.421174361631546e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                       â€”
| 1.8337412030201525e-08  |
| 8.364812372972319e-10   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                      â€”
| 1.764792598643476e-07  |
| -9.91329773311686e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| 2.7358682543712307e-08  |
| -1.6132593328443852e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 2.2403757000309272e-07  |
| -6.617780939028758e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| 2.2281820794699433e-08  |
| -8.089860027742958e-11  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                      â€”
| -0.0013152456778753614  |
| 0.0002274758443287159  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                      â€”
| 7.861944959064528e-08  |
| -3.476488621771424e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                       â€”
| -2.2888616704501514e-05  |
| -1.568057060842455e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                      â€”
| 4.626326842586779e-08  |
| -1.0302240338307496e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                     â€”
| 9.38064166242545e-10  |
| 5.666973193789896e-10  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| -7.160925989205311e-08  |
| -1.7297414212612099e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                       â€”
| 2.0581762307296136e-07  |
| -3.082156717089458e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| 1.6271869371463716e-07  |
| -1.0522069753412226e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                       â€”
| 1.2188318673458688e-07  |
| -8.7034649128297e-08    |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                     â€”
| 2.22857368306358e-07  |
| -6.785889493636461e-07  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                        â€”
| -5.2010371382976725e-05  |
| -2.3033160124204238e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                       â€”
| -8.994062154621728e-08  |
| -1.7707126203245825e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                        â€”
| -1.0627652619897968e-05  |
| -1.0467643144454773e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                      â€”
| 2.071532251562428e-07  |
| -3.1499021853230535e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| -0.0006692623593757504  |
| 4.5790183143682945e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                      â€”
| 5.628746677100624e-09  |
| 1.6810763710348436e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                        â€”
| -1.2369125076739144e-07  |
| -1.8438011058836035e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                        â€”
| -1.9766209408954255e-05  |
| -1.4542556714812375e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                        â€”
| -3.5846415952053794e-06  |
| -5.8818508586012585e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| 4.6266533318616974e-10  |
| 3.2905865880141403e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                        â€”
| -1.0247133950536344e-05  |
| -1.026460197744779e-05   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                       â€”
| 2.1349047191856038e-07  |
| -7.734068490595065e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                   â€”
| -0.9298827708748443  |
| 0.9132375372722746  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                       â€”
| -5.5645341789665425e-05  |
| -2.367329518879487e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                       â€”
| -0.0018617617450759855  |
| 0.00041689523649143074  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                     â€”
| -0.008971203016333985  |
| 0.003924261721491623  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                        â€”
| -0.00013862377269792143  |
| -2.9994169927492377e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.94303208  0.91769009]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.04827855 0.95781015]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.03884823 0.96698705]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.04827855 0.95781015]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 13              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.04827855 0.95781015]

[4.525491827767309, 4.960166360955473, 6.62730134189487, 5.43112841504713, 4.9471173548619625, 5.43463957307096, 5.245356487899543, 6.597668615285253, 4.871918938790327, 3.6960369330631475, 3.951051628276191, 5.436916857828227, 4.683112858710698, 3.389905575467676, 6.773437205377399, 4.9731129801028136, 5.713770100318756, 5.78071325942541, 3.6774005993445718, 4.707968235738519, 5.894022120518272, 6.0115826438236235, 4.855906884524213, 5.036320434323684, 4.621962251875588, 3.8085309752160392, 6.632916483405368, 6.551570420264441, 5.328097831866994, 4.723646058902793, 4.684965447556247, 5.114917614624182, 6.0797356432271865, 6.334257343429244, 5.846049293573029, 5.846581982171187, 3.1574490721836947, 4.459369571447204, 4.773824744492791, 4.186583225587993, 5.495703629611403, 5.8122130284460605, 3.5691417657236753, 4.26265547659675, 4.523434629227358, 3.405932251233569, 5.626621309337312, 6.824971295892024, 5.9038830058265, 6.122729257147154, 5.230008982894638, 4.196378447740866, 6.177331674922204, 5.517840725857978, 5.210828974954717, 4.789102216128037, 4.781349903460918, 5.339287534498736, 4.9245071078156295, 6.701207863258989, 4.439469034941763, 3.9240069642493474, 3.571461598287253, 3.099771624686494, 5.999122977423209, 4.478828210834569, 4.961603054597413, 5.73423339159917, 4.750660014139897, 6.938201923584461, 5.502791389523651, 4.181805872180821, 6.488535024376849, 5.4999230250685995, 4.9746832551558136, 3.679784846830486, 6.046822572598391, 5.708897408940855, 7.061608116868901, 3.961717066019777, 5.603384170844793, 7.380843307721037, 6.781190249117925, 4.717267839259414, 3.9193847974155704, 4.790649456158062, 6.371011706777922, 4.689633998710858, 4.804061300502477, 4.894834442420745, 4.263084366604324, 5.144180615113666, 4.6066336477791845, 5.381709193101656, 3.5755635239163714, 4.492620532918825, 5.577956818303537, 6.8208208608907395, 5.82923381759672, 4.620459934894899]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245, 5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287, 4.490416274328351, 3.5564524988044712, 4.091629946863637, 4.879829892905114, 4.1926418390225075, 4.210216335775425, 4.187504980422656, 4.530710047380715, 4.009372181787566, 3.9432288559544406, 4.411784013963423, 5.289816410463862, 3.4893621991347263, 4.70812136860741, 4.748987698855443, 3.7989359743023168, 4.515802235408018, 6.849494774956813, 3.8533509627750253, 5.262683037408525, 5.174250961590373, 4.811616579786701, 5.4144120224117165, 5.534109611204314, 4.757576389142094, 5.526458646858388, 4.68648626135325, 6.510397251598002, 4.053569161498328, 4.923389693719144, 5.171489048917507, 4.38273385252844, 3.5798745643728505, 5.776623075584453, 5.2610244417351675, 4.290574225520197, 4.856593546566716, 4.745192874359674, 3.6986305219136373, 4.379386401302874, 5.416012422669541, 4.659283968868628, 5.063523399259809, 5.7467416662796, 4.506312504147869, 4.476540161369129, 4.887133675349967, 7.101314492424933, 4.975301110000693, 4.862850942851848, 7.180855356630156, 2.9847168625895826, 6.5661988659086585, 4.130266496729279, 4.635203270500523, 5.2680387343232065, 5.599672587970324, 4.400983390586819, 2.518320319314273, 4.116474903067948, 4.804012214397902, 6.84000545726421, 5.297420062264238, 5.198714774949839, 6.736278646645381, 2.7653837751939716, 3.7074394406887117, 4.321519160329914, 4.295293892672745, 6.226190166839307, 4.668913001705613, 6.094720511993962, 5.253733943180695, 5.065868524410168, 5.120953283929446, 4.379225050802219, 5.923121760179944, 4.267138358789313, 4.759398905325478, 5.266646686893115, 6.473242173540947, 5.087655203318958, 3.965053905052038, 4.353966609391555, 5.343060515208491, 5.783994897105108, 5.01033882071759, 7.4186928993875325, 4.410049001959045, 7.374056817127453, 4.509450562163232, 5.746852509456756, 3.887434446403901, 4.02234765799981, 4.249898068401506, 4.9494169910636625, 4.938595751187503, 3.9514719173927, 4.478787355638231, 6.566513640431479, 4.525491827767309, 4.960166360955473, 6.62730134189487, 5.43112841504713, 4.9471173548619625, 5.43463957307096, 5.245356487899543, 6.597668615285253, 4.871918938790327, 3.6960369330631475, 3.951051628276191, 5.436916857828227, 4.683112858710698, 3.389905575467676, 6.773437205377399, 4.9731129801028136, 5.713770100318756, 5.78071325942541, 3.6774005993445718, 4.707968235738519, 5.894022120518272, 6.0115826438236235, 4.855906884524213, 5.036320434323684, 4.621962251875588, 3.8085309752160392, 6.632916483405368, 6.551570420264441, 5.328097831866994, 4.723646058902793, 4.684965447556247, 5.114917614624182, 6.0797356432271865, 6.334257343429244, 5.846049293573029, 5.846581982171187, 3.1574490721836947, 4.459369571447204, 4.773824744492791, 4.186583225587993, 5.495703629611403, 5.8122130284460605, 3.5691417657236753, 4.26265547659675, 4.523434629227358, 3.405932251233569, 5.626621309337312, 6.824971295892024, 5.9038830058265, 6.122729257147154, 5.230008982894638, 4.196378447740866, 6.177331674922204, 5.517840725857978, 5.210828974954717, 4.789102216128037, 4.781349903460918, 5.339287534498736, 4.9245071078156295, 6.701207863258989, 4.439469034941763, 3.9240069642493474, 3.571461598287253, 3.099771624686494, 5.999122977423209, 4.478828210834569, 4.961603054597413, 5.73423339159917, 4.750660014139897, 6.938201923584461, 5.502791389523651, 4.181805872180821, 6.488535024376849, 5.4999230250685995, 4.9746832551558136, 3.679784846830486, 6.046822572598391, 5.708897408940855, 7.061608116868901, 3.961717066019777, 5.603384170844793, 7.380843307721037, 6.781190249117925, 4.717267839259414, 3.9193847974155704, 4.790649456158062, 6.371011706777922, 4.689633998710858, 4.804061300502477, 4.894834442420745, 4.263084366604324, 5.144180615113666, 4.6066336477791845, 5.381709193101656, 3.5755635239163714, 4.492620532918825, 5.577956818303537, 6.8208208608907395, 5.82923381759672, 4.620459934894899]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1300

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [6.491611225535168, 4.9731129801028136, 3.099771624686494, 7.374056817127453, 3.9642028572820385, 6.013585291449488, 5.303860627982663, 5.41454797409735, 5.157837638063537, 4.979376339681217, 5.965579597057303, 6.423126226420643, 5.005974997688087, 5.491125018176283, 5.602419780339141, 3.849921136787044, 6.129881283109167, 3.3104116950608886, 4.692982558398637, 5.980251346533316, 4.064795358316771, 4.459369571447204, 4.655306173206099, 4.6519615102397385, 2.590797475935505, 4.505686099511901, 5.526941880340413, 4.941645846213782, 6.37469851742308, 5.017597776501736, 6.177331674922204, 4.6622497962803156, 6.067302259045171, 4.963679938297505, 3.787735755199361]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (6.491611225535168, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.491611225535168, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.491611225535168, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.491611225535168, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.491611225535168, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.50690889 0.61336305]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.5069088865615754  |
| 0.613363053680871   |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8190621113072977e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1373983187759122

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.051744254531241e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.091791650104782e-09  |
| 1.2584641213314642e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.9731129801028136, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9731129801028136, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9731129801028136, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9731129801028136, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9731129801028136, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.07847654 -0.51894481]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.07847653527015552  |
| -0.5189448115672235   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7000296096064665e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40643346017419013

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.182799341564703e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.2825160005628573e-07  |
| -2.1706420161318017e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.099771624686494, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.099771624686494, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.099771624686494, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.099771624686494, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.099771624686494, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.03433527  1.54723558]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.0343352735707754  |
| 1.5472355840984164  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0032691325783877857

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.05617160942064524

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.05819901925733773

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.11839631776242698  |
| 0.09004759355458193  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (7.374056817127453, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.374056817127453, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.374056817127453, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.374056817127453, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.374056817127453, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.42822465 2.42611318]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 2.428224648021171  |
| 2.426113177023126  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.210930381470194e-13

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.02420631344896987

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.5658307674828975e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 6.23041351225305e-11  |
| 6.224995835001618e-11  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.9642028572820385, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9642028572820385, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9642028572820385, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9642028572820385, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9642028572820385, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.13182737  0.11849238]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.131827371825267   |
| 0.11849238035210874  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00015433412236450736

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2207158989158671

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0006992433400701086

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0007914227518578724  |
| 8.285500781026622e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.013585291449488, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.013585291449488, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.013585291449488, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.013585291449488, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.013585291449488, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 1.00782675 -0.01416672]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0078267487223513  |
| -0.014166716688635006  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.5997573977533685e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.25062018630785254

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.23436008098519e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.2518478558943136e-08  |
| -3.1653546247712755e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (5.303860627982663, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.303860627982663, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.303860627982663, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.303860627982663, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.303860627982663, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.26683996 -0.4864223 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2668399634586649  |
| -0.4864222957756681  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.1070954075247806e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3939680553902186

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.886668385961536e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.1044783039205838e-07  |
| -3.8362513423207933e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.41454797409735, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.41454797409735, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.41454797409735, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.41454797409735, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.41454797409735, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.38240289 -0.44890809]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3824028915744293  |
| -0.4489080918101962  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7168517502110845e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38006349781806037

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.5172760869368096e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.727419437684659e-07  |
| -2.0278417883666331e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.157837638063537, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.157837638063537, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.157837638063537, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.157837638063537, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.157837638063537, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.11438492 -0.51548213]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.11438491820392471  |
| -0.5154821292041589  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.669284397506398e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40508771926436266

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.646380297486625e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.8832107566056083e-07  |
| -8.48679621228182e-07   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (4.979376339681217, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.979376339681217, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.979376339681217, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.979376339681217, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.979376339681217, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.07193729 -0.51943661]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.07193728612797656  |
| -0.5194366070604417   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.647860300320297e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40662495477571003

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.052531161618546e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.915280937158946e-07  |
| -2.1050330365978483e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.965579597057303, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.965579597057303, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.965579597057303, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.965579597057303, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.965579597057303, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.95770649 -0.06342323]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9577064852273054  |
| -0.06342323333186073  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.465238199538937e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2627273985961713

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.841438783860332e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.7212643506794277e-08  |
| -1.8021323498697244e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.423126226420643, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.423126226420643, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.423126226420643, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.423126226420643, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.423126226420643, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.43540724 0.50817284]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4354072397360085  |
| 0.5081728438049993  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.3869627733278286e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.15196289899402995

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.8868643612150196e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 4.1438260042239065e-09  |
| 1.4670260721179392e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (5.005974997688087, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.005974997688087, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.005974997688087, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.005974997688087, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.005974997688087, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.044167   -0.52104873]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.04416700338794044  |
| -0.5210487297180322   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.442934931155192e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4072533125197622

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.5430894895058043e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.5648764548677904e-07  |
| -1.8461222777843104e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.491125018176283, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.491125018176283, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.491125018176283, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.491125018176283, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.491125018176283, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.46235302 -0.41513892]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.46235301920383165  |
| -0.4151389187967425  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.1308054146176354e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.36796722768418083

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.0731144774343433e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.420863757000774e-07  |
| -1.2757694215007095e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.602419780339141, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.602419780339141, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.602419780339141, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.602419780339141, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.602419780339141, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.57855012 -0.35466396]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5785501211441613  |
| -0.3546639559814935  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.099388456861389e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3472588056668967

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.756438816619137e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.016187890137309e-07  |
| -6.229455391415962e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (3.849921136787044, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.849921136787044, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.849921136787044, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.849921136787044, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.849921136787044, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.251143    0.26065516]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.2511430025341497  |
| 0.26065515967843567  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00024119980225630077

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1926184672464209

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0012522153545523169

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0015667004785139505  |
| 0.00032639639319262306  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.129881283109167, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.129881283109167, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.129881283109167, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.129881283109167, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.129881283109167, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.12924537 0.11557346]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.1292453727662632  |
| 0.1155734552504839  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.7637958179549005e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.221333834396368

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2487001029428934e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.4100888132210189e-08  |
| 1.4431658546874514e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.3104116950608886, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3104116950608886, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3104116950608886, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3104116950608886, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3104116950608886, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.81441689  1.12402994]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.8144168878464484  |
| 1.124029940235971  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0016642988945206605

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.08424777498867686

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0197548112664619

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.03584346317808775  |
| 0.022204999327214054  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.692982558398637, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.692982558398637, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.692982558398637, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.692982558398637, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.692982558398637, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.37094621 -0.45322357]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3709462070844438  |
| -0.4532235697851661  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.583255488271073e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38163770772413774

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.725001318011715e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -6.3988269613211215e-06  |
| -7.81811255233386e-06    |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.980251346533316, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.980251346533316, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.980251346533316, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.980251346533316, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.980251346533316, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.9730245  -0.04863575]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9730245009720306  |
| -0.04863574876168286  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.838881671065959e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2590324610236282

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.6401639563012667e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.568944216064382e-08  |
| -1.2840635086831906e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (4.064795358316771, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.064795358316771, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.064795358316771, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.064795358316771, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.064795358316771, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.02680395  0.00513897]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -1.0268039463134926   |
| 0.005138971470586284  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00010305794184667538

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.24602850550055405

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0004188861840907427

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.00043011398688057474  |
| 2.152644149465081e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.459369571447204, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.459369571447204, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.459369571447204, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.459369571447204, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.459369571447204, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.61484944 -0.33300423]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6148494358093615  |
| -0.3330042286364687  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9174543451210014e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3401288204698255

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.637435670615593e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.466174141489567e-05   |
| -1.8772899169810595e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.655306173206099, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.655306173206099, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.655306173206099, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.655306173206099, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.655306173206099, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.41028217 -0.43785839]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -0.410282171570131  |
| -0.4378583895459087  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.850947629278976e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3760622998703363

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.0876720777344417e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.565346335792141e-06  |
| -9.141047338567637e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (4.6519615102397385, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6519615102397385, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6519615102397385, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6519615102397385, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6519615102397385, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.41377416 -0.43641959]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.41377416160059965  |
| -0.4364195937966997   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.974102302486762e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37554440716557663

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1233447097964637e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.785851770851003e-06  |
| -9.26669235739744e-06   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (2.590797475935505, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.590797475935505, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.590797475935505, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.590797475935505, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.590797475935505, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.56572884  2.76945765]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -2.565728838277437  |
| 2.769457649876017  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.013911293122369292

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.01742236333892778

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.7984733673465813

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -2.048666145197617  |
| 2.2113381754202526  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.505686099511901, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.505686099511901, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.505686099511901, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.505686099511901, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.505686099511901, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.56649275 -0.36156712]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5664927460458102  |
| -0.3615671206880222  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5579694177722562e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.34956246107970074

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.456912830285392e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.524808788115176e-05   |
| -1.6114731392037932e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (5.526941880340413, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.526941880340413, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.526941880340413, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.526941880340413, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.526941880340413, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.49974755 -0.39715027]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.49974755000903315  |
| -0.3971502704480656  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.283131503904689e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3616815633275072

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.566658753213444e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.2826814236276582e-07  |
| -1.0193492179866142e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.941645846213782, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.941645846213782, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.941645846213782, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.941645846213782, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.941645846213782, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.11132974 -0.51582694]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.11132974098160275  |
| -0.515826940050701    |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.987028268234638e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4052215270281249

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.9035604865501775e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.459121188552532e-07  |
| -2.5293886011307043e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.37469851742308, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.37469851742308, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.37469851742308, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.37469851742308, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.37469851742308, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.38484638 0.43687562]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.384846377572302  |
| 0.4368756179040645  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.980605414480303e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.16270286423776908

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.6757837315884157e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.090395785429416e-09  |
| 1.605860289019397e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (5.017597776501736, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.017597776501736, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.017597776501736, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.017597776501736, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.017597776501736, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.03203226 -0.52151106]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.03203226128789538  |
| -0.5215110576717308   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3612839038739524e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40743369358713605

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.341117647607661e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.0702355348176696e-07  |
| -1.7424297982095563e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.177331674922204, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.177331674922204, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.177331674922204, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.177331674922204, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.177331674922204, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.17878587 0.17274396]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.1787858689160657  |
| 0.17274396135746883  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.0639342525828008e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.20953978563689027

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.8498442494323e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.1610857212254968e-08  |
| 1.70150111440102e-09    |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.6622497962803156, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6622497962803156, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6622497962803156, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.6622497962803156, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.6622497962803156, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.40303269 -0.44080644]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4030326927750849  |
| -0.4408064424588076  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.601042761201103e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.37712567840874833

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.0155198111338098e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.123203768227899e-06  |
| -8.884541176511425e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61630>

args = (6.067302259045171, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.067302259045171, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.067302259045171, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.067302259045171, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.067302259045171, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.06390985 0.04392799]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0639098513287593  |
| 0.04392799013430704  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.0481053321489764e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23705564071964272

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7076604124921568e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.816796735574538e-08  |
| 7.501408975270215e-10  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.963679938297505, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.963679938297505, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.963679938297505, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.963679938297505, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.963679938297505, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.08832509 -0.51812343]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.08832508724410104  |
| -0.518123433046469    |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7816016441859807e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40611383543393054

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.3869513637287155e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -3.874778619369668e-07  |
| -2.272982301183011e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.787735755199361, array([5.04827855, 0.95781015])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.04827855 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827865 0.95781015]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.787735755199361, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.787735755199361, array([5.04827865, 0.95781015])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.04827855 0.95781025]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.787735755199361, array([5.04827855, 0.95781015]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.787735755199361, array([5.04827855, 0.95781025])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.31606754  0.34399263]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.3160675393919519  |
| 0.3439926299364515  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0003058518851010263

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1778410010796669

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0017198052375110887

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.00226337984716461  |
| 0.000591600326629923  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| 3.091791650104782e-09  |
| 1.2584641213314642e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                        â€”
| -3.2825160005628573e-07  |
| -2.1706420161318017e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                    â€”
| -0.11839631776242698  |
| 0.09004759355458193  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                     â€”
| 6.23041351225305e-11  |
| 6.224995835001618e-11  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                      â€”
| -0.0007914227518578724  |
| 8.285500781026622e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| 2.2518478558943136e-08  |
| -3.1653546247712755e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 2.1044783039205838e-07  |
| -3.8362513423207933e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                      â€”
| 1.727419437684659e-07  |
| -2.0278417883666331e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| 1.8832107566056083e-07  |
| -8.48679621228182e-07   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| -2.915280937158946e-07  |
| -2.1050330365978483e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                       â€”
| 2.7212643506794277e-08  |
| -1.8021323498697244e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                       â€”
| 4.1438260042239065e-09  |
| 1.4670260721179392e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                        â€”
| -1.5648764548677904e-07  |
| -1.8461222777843104e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                      â€”
| 1.420863757000774e-07  |
| -1.2757694215007095e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                      â€”
| 1.016187890137309e-07  |
| -6.229455391415962e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| -0.0015667004785139505  |
| 0.00032639639319262306  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                       â€”
| 1.4100888132210189e-08  |
| 1.4431658546874514e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                     â€”
| -0.03584346317808775  |
| 0.022204999327214054  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                        â€”
| -6.3988269613211215e-06  |
| -7.81811255233386e-06    |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                      â€”
| 2.568944216064382e-08  |
| -1.2840635086831906e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                      â€”
| -0.00043011398688057474  |
| 2.152644149465081e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                        â€”
| -3.466174141489567e-05   |
| -1.8772899169810595e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| -8.565346335792141e-06  |
| -9.141047338567637e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                       â€”
| -8.785851770851003e-06  |
| -9.26669235739744e-06   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                   â€”
| -2.048666145197617  |
| 2.2113381754202526  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                        â€”
| -2.524808788115176e-05   |
| -1.6114731392037932e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                       â€”
| 1.2826814236276582e-07  |
| -1.0193492179866142e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| -5.459121188552532e-07  |
| -2.5293886011307043e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                      â€”
| 5.090395785429416e-09  |
| 1.605860289019397e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                        â€”
| -1.0702355348176696e-07  |
| -1.7424297982095563e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                       â€”
| 1.1610857212254968e-08  |
| 1.70150111440102e-09    |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                       â€”
| -8.123203768227899e-06  |
| -8.884541176511425e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                      â€”
| 1.816796735574538e-08  |
| 7.501408975270215e-10  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                       â€”
| -3.874778619369668e-07  |
| -2.272982301183011e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                     â€”
| -0.00226337984716461  |
| 0.000591600326629923  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-2.20805007  2.32450939]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.07035905 0.93456506]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.04827855 0.95781015]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.07035905 0.93456506]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 14              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.07035905 0.93456506]

[6.9904670323299865, 6.164042199831909, 6.630221735394573, 3.8222843450965946, 6.125515905657428, 5.107895262736628, 6.803408080581033, 5.773340456981987, 6.008548120608629, 5.862181357017108, 7.043883173632293, 5.543745390710634, 3.869386773297068, 5.196806275202913, 4.413733794800476, 3.2790332151915615, 5.08138520994593, 4.336847519534556, 5.33269272348815, 5.188281310309852, 3.626726904604922, 6.085726993257404, 5.551210898534239, 4.614813888109768, 4.840177833878102, 5.5778413375269755, 6.115050488749432, 6.353907630856576, 7.664892913421985, 5.905035360960175, 4.321066304382384, 5.454549164953521, 7.487136297287175, 5.214240530104311, 4.200050491923914, 4.800876295931128, 3.8660799414320355, 5.261182328504775, 3.7654295503122177, 5.441861123290136, 5.651112180674322, 6.274835394016853, 5.28945498068728, 5.356461099182567, 5.1832124360021945, 6.665867026962602, 4.839347695362241, 4.919572087574838, 4.6206366192879305, 4.604594413140994, 4.984678484651757, 5.107017553153507, 5.686489608032262, 5.186002447477617, 4.7029937764974115, 6.303807780759701, 3.9430285221898838, 5.58503002718914, 6.18498041549213, 5.762318895721471, 5.686051595888033, 5.164742310256726, 5.479490034757343, 4.60408251234389, 5.379064196745426, 3.955130636735653, 6.114620893503636, 3.958982574562085, 3.838940291278325, 4.865896401991737, 7.062290581115448, 3.8008119492608645, 5.786056747809032, 3.74746781054968, 4.947768377377417, 5.67667560463442, 4.666177588305013, 5.334813027008264, 5.793369424289828, 4.4223341037659605, 5.330722476384998, 6.046541903653283, 4.197513571891748, 6.5968003456033655, 5.936296382786533, 5.170981254862737, 5.880836879058082, 4.096208966409434, 4.696039761279678, 4.652970837455283, 4.341410437282872, 5.258579423394452, 6.543783961505602, 5.099884384474556, 5.867561782235126, 4.424638904638933, 3.288712428222356, 5.059452664266722, 4.743613133176822, 4.434570194608235]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245, 5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287, 4.490416274328351, 3.5564524988044712, 4.091629946863637, 4.879829892905114, 4.1926418390225075, 4.210216335775425, 4.187504980422656, 4.530710047380715, 4.009372181787566, 3.9432288559544406, 4.411784013963423, 5.289816410463862, 3.4893621991347263, 4.70812136860741, 4.748987698855443, 3.7989359743023168, 4.515802235408018, 6.849494774956813, 3.8533509627750253, 5.262683037408525, 5.174250961590373, 4.811616579786701, 5.4144120224117165, 5.534109611204314, 4.757576389142094, 5.526458646858388, 4.68648626135325, 6.510397251598002, 4.053569161498328, 4.923389693719144, 5.171489048917507, 4.38273385252844, 3.5798745643728505, 5.776623075584453, 5.2610244417351675, 4.290574225520197, 4.856593546566716, 4.745192874359674, 3.6986305219136373, 4.379386401302874, 5.416012422669541, 4.659283968868628, 5.063523399259809, 5.7467416662796, 4.506312504147869, 4.476540161369129, 4.887133675349967, 7.101314492424933, 4.975301110000693, 4.862850942851848, 7.180855356630156, 2.9847168625895826, 6.5661988659086585, 4.130266496729279, 4.635203270500523, 5.2680387343232065, 5.599672587970324, 4.400983390586819, 2.518320319314273, 4.116474903067948, 4.804012214397902, 6.84000545726421, 5.297420062264238, 5.198714774949839, 6.736278646645381, 2.7653837751939716, 3.7074394406887117, 4.321519160329914, 4.295293892672745, 6.226190166839307, 4.668913001705613, 6.094720511993962, 5.253733943180695, 5.065868524410168, 5.120953283929446, 4.379225050802219, 5.923121760179944, 4.267138358789313, 4.759398905325478, 5.266646686893115, 6.473242173540947, 5.087655203318958, 3.965053905052038, 4.353966609391555, 5.343060515208491, 5.783994897105108, 5.01033882071759, 7.4186928993875325, 4.410049001959045, 7.374056817127453, 4.509450562163232, 5.746852509456756, 3.887434446403901, 4.02234765799981, 4.249898068401506, 4.9494169910636625, 4.938595751187503, 3.9514719173927, 4.478787355638231, 6.566513640431479, 4.525491827767309, 4.960166360955473, 6.62730134189487, 5.43112841504713, 4.9471173548619625, 5.43463957307096, 5.245356487899543, 6.597668615285253, 4.871918938790327, 3.6960369330631475, 3.951051628276191, 5.436916857828227, 4.683112858710698, 3.389905575467676, 6.773437205377399, 4.9731129801028136, 5.713770100318756, 5.78071325942541, 3.6774005993445718, 4.707968235738519, 5.894022120518272, 6.0115826438236235, 4.855906884524213, 5.036320434323684, 4.621962251875588, 3.8085309752160392, 6.632916483405368, 6.551570420264441, 5.328097831866994, 4.723646058902793, 4.684965447556247, 5.114917614624182, 6.0797356432271865, 6.334257343429244, 5.846049293573029, 5.846581982171187, 3.1574490721836947, 4.459369571447204, 4.773824744492791, 4.186583225587993, 5.495703629611403, 5.8122130284460605, 3.5691417657236753, 4.26265547659675, 4.523434629227358, 3.405932251233569, 5.626621309337312, 6.824971295892024, 5.9038830058265, 6.122729257147154, 5.230008982894638, 4.196378447740866, 6.177331674922204, 5.517840725857978, 5.210828974954717, 4.789102216128037, 4.781349903460918, 5.339287534498736, 4.9245071078156295, 6.701207863258989, 4.439469034941763, 3.9240069642493474, 3.571461598287253, 3.099771624686494, 5.999122977423209, 4.478828210834569, 4.961603054597413, 5.73423339159917, 4.750660014139897, 6.938201923584461, 5.502791389523651, 4.181805872180821, 6.488535024376849, 5.4999230250685995, 4.9746832551558136, 3.679784846830486, 6.046822572598391, 5.708897408940855, 7.061608116868901, 3.961717066019777, 5.603384170844793, 7.380843307721037, 6.781190249117925, 4.717267839259414, 3.9193847974155704, 4.790649456158062, 6.371011706777922, 4.689633998710858, 4.804061300502477, 4.894834442420745, 4.263084366604324, 5.144180615113666, 4.6066336477791845, 5.381709193101656, 3.5755635239163714, 4.492620532918825, 5.577956818303537, 6.8208208608907395, 5.82923381759672, 4.620459934894899, 6.9904670323299865, 6.164042199831909, 6.630221735394573, 3.8222843450965946, 6.125515905657428, 5.107895262736628, 6.803408080581033, 5.773340456981987, 6.008548120608629, 5.862181357017108, 7.043883173632293, 5.543745390710634, 3.869386773297068, 5.196806275202913, 4.413733794800476, 3.2790332151915615, 5.08138520994593, 4.336847519534556, 5.33269272348815, 5.188281310309852, 3.626726904604922, 6.085726993257404, 5.551210898534239, 4.614813888109768, 4.840177833878102, 5.5778413375269755, 6.115050488749432, 6.353907630856576, 7.664892913421985, 5.905035360960175, 4.321066304382384, 5.454549164953521, 7.487136297287175, 5.214240530104311, 4.200050491923914, 4.800876295931128, 3.8660799414320355, 5.261182328504775, 3.7654295503122177, 5.441861123290136, 5.651112180674322, 6.274835394016853, 5.28945498068728, 5.356461099182567, 5.1832124360021945, 6.665867026962602, 4.839347695362241, 4.919572087574838, 4.6206366192879305, 4.604594413140994, 4.984678484651757, 5.107017553153507, 5.686489608032262, 5.186002447477617, 4.7029937764974115, 6.303807780759701, 3.9430285221898838, 5.58503002718914, 6.18498041549213, 5.762318895721471, 5.686051595888033, 5.164742310256726, 5.479490034757343, 4.60408251234389, 5.379064196745426, 3.955130636735653, 6.114620893503636, 3.958982574562085, 3.838940291278325, 4.865896401991737, 7.062290581115448, 3.8008119492608645, 5.786056747809032, 3.74746781054968, 4.947768377377417, 5.67667560463442, 4.666177588305013, 5.334813027008264, 5.793369424289828, 4.4223341037659605, 5.330722476384998, 6.046541903653283, 4.197513571891748, 6.5968003456033655, 5.936296382786533, 5.170981254862737, 5.880836879058082, 4.096208966409434, 4.696039761279678, 4.652970837455283, 4.341410437282872, 5.258579423394452, 6.543783961505602, 5.099884384474556, 5.867561782235126, 4.424638904638933, 3.288712428222356, 5.059452664266722, 4.743613133176822, 4.434570194608235]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1400

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [6.416668405673112, 5.332826186388074, 5.526458646858388, 3.5706906099409914, 7.4186928993875325, 4.306155737356559, 5.854834685558841, 3.812411132294809, 7.062290581115448, 5.6286381872351585, 4.901625130182659, 5.352245128503238, 5.894022120518272, 5.2610244417351675, 5.2986142577406, 5.319509945892983, 4.98582664005507, 3.677069888579797, 4.379323623302777, 6.490140411593667, 5.9869670759644285, 3.379193638784313, 5.229495417953454, 6.442058704688819, 3.3104116950608886, 3.5287015832287567, 2.932310514318679, 6.328013047797941, 4.8594588994096695, 4.521677738182754, 5.417373682275772, 4.3798901053116595, 3.846893827083139, 5.67667560463442, 4.016764105980929]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (6.416668405673112, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.416668405673112, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.416668405673112, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.416668405673112, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.416668405673112, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.44057312 0.50261722]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4405731230127117  |
| 0.5026172211763935  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.5726626982568036e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1564795288430744

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.9222114432888546e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.209659264962109e-09  |
| 1.4687537953157023e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.332826186388074, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.332826186388074, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.332826186388074, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.332826186388074, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.332826186388074, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.2808441  -0.49557148]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2808441035284659  |
| -0.4955714849597115  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.663496013450769e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3977396251440546

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.696581997547003e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.8806955678059514e-07  |
| -3.3186350846788393e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.526458646858388, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.526458646858388, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.526458646858388, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.526458646858388, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.526458646858388, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.48803402 -0.41591959]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.4880340220836388  |
| -0.41591958543918395  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.307956974704596e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3692070845625313

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.521066730269673e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.2303663363147567e-07  |
| -1.0485610293182814e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.5706906099409914, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.5706906099409914, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.5706906099409914, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.5706906099409914, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.5706906099409914, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.60466998  0.75247444]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.6046699791161245  |
| 0.7524744427556129  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0006797032519961499

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.12389288239205373

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.005486217116535057

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.008803567905816835  |
| 0.004128238167601023  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (7.4186928993875325, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.4186928993875325, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.4186928993875325, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.4186928993875325, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.4186928993875325, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.51275583 2.6219625 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.512755825634372   |
| 2.6219625048895523  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.4645369752818147e-13

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.02159061923339841

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.067813306797448e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.195909932979562e-11  |
| 5.421728957534585e-11  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.306155737356559, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.306155737356559, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.306155737356559, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.306155737356559, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.306155737356559, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.81771019 -0.20068331]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.8177101862294478  |
| -0.2006833099699179  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.752785313629968e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3019330691219886

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0001242919606170647

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00010163480226300315  |
| -2.4943322059283222e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.854834685558841, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.854834685558841, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.854834685558841, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.854834685558841, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.854834685558841, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.83940179 -0.18271051]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8394017902979556  |
| -0.18271051338558664  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.43648278178186e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2969039296855351

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.8382073733524057e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.061199931024778e-08  |
| -8.839913530511487e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.812411132294809, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.812411132294809, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.812411132294809, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.812411132294809, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.812411132294809, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.34602504  0.37088333]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.3460250403163343  |
| 0.3708833307136672  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00027847622952710885

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1769804187347002

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0015734861038189452

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0021179516963300876  |
| 0.0005835797670160415  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (7.062290581115448, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.062290581115448, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.062290581115448, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (7.062290581115448, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [7.062290581115448, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.13139948 1.73642353]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.1313994835736594  |
| 1.736423529408171   |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.89502312423543e-12

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.04939517771049362

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1934410194424865e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.5436995725153375e-10  |
| 2.072319067120808e-10   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.6286381872351585, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.6286381872351585, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.6286381872351585, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.6286381872351585, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.6286381872351585, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.59736781 -0.35658404]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5973678129933546  |
| -0.35658403563232355  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.264353201633081e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.34929077997360114

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.5071549274879094e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.003258428756104e-08  |
| -5.374273863667807e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.901625130182659, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.901625130182659, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.901625130182659, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.901625130182659, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.901625130182659, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.18054813 -0.5187094 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.18054812866985515  |
| -0.518709398766859    |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.4196128943002635e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40643396604163556

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.953274323663184e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.0748525385956853e-06  |
| -3.088019345121509e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.352245128503238, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.352245128503238, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.352245128503238, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.352245128503238, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.352245128503238, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.30162269 -0.48952007]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3016226890473206  |
| -0.4895200667043298  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.401018830447462e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3954965790742914

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.070896582891672e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.831120152259976e-07  |
| -2.9718257002122196e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.894022120518272, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.894022120518272, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.894022120518272, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.894022120518272, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.894022120518272, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.88133299 -0.14663428]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8813329910317691  |
| -0.14663427583272437  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.1411012811249178e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2870605084028553

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.9751245738181364e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 3.5034084303670247e-08  |
| -5.8288951322668955e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.2610244417351675, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.2610244417351675, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.2610244417351675, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.2610244417351675, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.2610244417351675, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.20401505 -0.51419712]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.20401505262945818  |
| -0.514197120438098   |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.896068740484769e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.40472363577711123

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.626491749126324e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.963949220835052e-07  |
| -4.949914337321865e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.2986142577406, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.2986142577406, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.2986142577406, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.2986142577406, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.2986142577406, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.24423677 -0.50518239]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2442367741384288  |
| -0.5051823903645669  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.1947236564588437e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4013282213218558

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.960376287359942e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.9442166253528346e-07  |
| -4.021441921049912e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.319509945892983, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.319509945892983, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.319509945892983, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.319509945892983, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.319509945892983, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.26659551 -0.49947161]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.26659550567131873  |
| -0.49947160851715466  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8592635324256003e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39919199931664784

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.162627350548602e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.9095242604547223e-07  |
| -3.577529003987476e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.98582664005507, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.98582664005507, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.98582664005507, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.98582664005507, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.98582664005507, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.09045113 -0.5309175 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.0904511276988984  |
| -0.5309175021839252  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5957412828630928e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.411097634313466

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.881660096458556e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -3.511005330684912e-07  |
| -2.060841282738791e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.677069888579797, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.677069888579797, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.677069888579797, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.677069888579797, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.677069888579797, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.4908424   0.57629713]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.4908423984216768  |
| 0.576297127796721  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00046227064997408516

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.14606716082434598

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0031647815112254545

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0047181904586759355  |
| 0.0018238544950233957  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.379323623302777, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.379323623302777, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.379323623302777, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.379323623302777, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.379323623302777, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.73941934 -0.26163779]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.739419341311276    |
| -0.26163778921173275  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.731222286406084e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.31963230910661994

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.544888012228541e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -6.318255465580645e-05  |
| -2.235665608581313e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.490140411593667, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.490140411593667, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.490140411593667, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.490140411593667, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.490140411593667, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.51918938 0.61895994]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.5191893787402933  |
| 0.6189599366379639  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8461042109514336e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.14035786611424453

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.0277482764199635e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.080533644296145e-09  |
| 1.2550949446906411e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (5.9869670759644285, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9869670759644285, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9869670759644285, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9869670759644285, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9869670759644285, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.98078562 -0.05403798]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9807856238275292  |
| -0.05403798297720641  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.569514408906515e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2632635847231726

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.4954132626487265e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.447465453514421e-08  |
| -1.34847099408107e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.379193638784313, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.379193638784313, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.379193638784313, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.379193638784313, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.379193638784313, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.80957489  1.10227217]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.8095748943736112  |
| 1.1022721668751956  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0013222593755967138

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.08934529441546925

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.014799429385144851

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.026780675866413212  |
| 0.016312999096880058  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.229495417953454, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.229495417953454, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.229495417953454, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.229495417953454, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.229495417953454, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.17027848 -0.52051081]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1702784779844535  |
| -0.520510813339925  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.596733555173521e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4071187898682834

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.129089020101656e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 1.92259559851868e-07  |
| -5.877030441862918e-07  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (6.442058704688819, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.442058704688819, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.442058704688819, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.442058704688819, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.442058704688819, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.46774116 0.54212381]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4677411597219248  |
| 0.5421238147285123  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.883948222656517e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.15080742766083824

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.57543562866904e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.780072876411861e-09  |
| 1.3962049876017843e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.3104116950608886, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3104116950608886, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3104116950608886, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.3104116950608886, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.3104116950608886, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.88317271  1.23816122]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -1.8831727066626058  |
| 1.238161217997913  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0016642988945206605

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.07868963159692231

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.021150167572849002

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.039829418314529734  |
| 0.026187317242858683  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.5287015832287567, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.5287015832287567, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.5287015832287567, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.5287015832287567, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.5287015832287567, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.64959893  0.82557988]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.6495989285658652  |
| 0.8255798755740784  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0007889512982477657

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.11571098238150859

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.006818292283151901

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.011247447644936284  |
| 0.005629044894752246  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (2.932310514318679, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.932310514318679, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.932310514318679, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.932310514318679, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.932310514318679, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.2877472  2.081885 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -2.2877471961280094  |
| 2.081884997728878  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.005417282302987947

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.03576595822787876

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.15146476066633935

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.34651308152661786  |
| 0.3153322129158469  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (6.328013047797941, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.328013047797941, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.328013047797941, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.328013047797941, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.328013047797941, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.34571043 0.37046005]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.3457104319769542  |
| 0.3704600515241907  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.044895815840499e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.17705044331890057

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.543843926643072e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.114698173358708e-09  |
| 1.6833126551820736e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.8594588994096695, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.8594588994096695, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.8594588994096695, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.8594588994096695, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.8594588994096695, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.22566669 -0.50954549]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.22566669199974854  |
| -0.509545489135732    |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.972494880321663e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4029680180026647

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.376503214957388e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.6646310790449439e-06  |
| -3.7586639387767626e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (4.521677738182754, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.521677738182754, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.521677738182754, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.521677738182754, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.521677738182754, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.58709809 -0.36266616]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5870980923639024  |
| -0.3626661637667894  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.4494760521346097e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.351281852919047

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.126248025879782e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.4225123446143382e-05  |
| -1.4964505422961083e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.417373682275772, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.417373682275772, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.417373682275772, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.417373682275772, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.417373682275772, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.37131131 -0.46607214]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.371311313918099  |
| -0.4660721431015702  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6907772104286126e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3869240903844225

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.369790489779964e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 1.622552648307012e-07  |
| -2.0366376184766078e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.3798901053116595, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3798901053116595, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3798901053116595, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3798901053116595, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3798901053116595, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.7388132 -0.2620858]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.7388131972874135   |
| -0.26208580194975184  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.724454608630777e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3197661659086039

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.520146591773831e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -6.294796744825883e-05   |
| -2.2330094522344895e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb616c0>

args = (3.846893827083139, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.846893827083139, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.846893827083139, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.846893827083139, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.846893827083139, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.30912799  0.32189968]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.3091279904564601  |
| 0.3218996824649878  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00024402629023030056

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.18527060974817747

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0013171343828467163

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0017242974877772314  |
| 0.00042398513960207565  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.67667560463442, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.67667560463442, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.67667560463442, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.67667560463442, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.67667560463442, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.64876864 -0.32455781]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.648768641031694  |
| -0.32455781395768213  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.012519108235683e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.33899120556720785

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1836646621914112e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 7.67924514327161e-08  |
| -3.841676152198027e-08  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.016764105980929, array([5.07035905, 0.93456506])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07035905 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035915 0.93456506]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.016764105980929, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.016764105980929, array([5.07035915, 0.93456506])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07035905 0.93456516]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.016764105980929, array([5.07035905, 0.93456506]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.016764105980929, array([5.07035905, 0.93456516])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.127364    0.10046646]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.1273639954900716  |
| 0.10046645737560311  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00012513273866697442

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22786680932497097

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0005491485971022531

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0006190903565469636  |
| 5.517101412364576e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| 4.209659264962109e-09  |
| 1.4687537953157023e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                       â€”
| 1.8806955678059514e-07  |
| -3.3186350846788393e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| 1.2303663363147567e-07  |
| -1.0485610293182814e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                     â€”
| -0.008803567905816835  |
| 0.004128238167601023  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                      â€”
| 5.195909932979562e-11  |
| 5.421728957534585e-11  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                        â€”
| -0.00010163480226300315  |
| -2.4943322059283222e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                      â€”
| 4.061199931024778e-08  |
| -8.839913530511487e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                      â€”
| -0.0021179516963300876  |
| 0.0005835797670160415  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| 2.5436995725153375e-10  |
| 2.072319067120808e-10   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                      â€”
| 9.003258428756104e-08  |
| -5.374273863667807e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                        â€”
| -1.0748525385956853e-06  |
| -3.088019345121509e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                      â€”
| 1.831120152259976e-07  |
| -2.9718257002122196e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                       â€”
| 3.5034084303670247e-08  |
| -5.8288951322668955e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                      â€”
| 1.963949220835052e-07  |
| -4.949914337321865e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                       â€”
| 1.9442166253528346e-07  |
| -4.021441921049912e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                       â€”
| 1.9095242604547223e-07  |
| -3.577529003987476e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                       â€”
| -3.511005330684912e-07  |
| -2.060841282738791e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                      â€”
| -0.0047181904586759355  |
| 0.0018238544950233957  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                       â€”
| -6.318255465580645e-05  |
| -2.235665608581313e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                      â€”
| 3.080533644296145e-09  |
| 1.2550949446906411e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                      â€”
| 2.447465453514421e-08  |
| -1.34847099408107e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                     â€”
| -0.026780675866413212  |
| 0.016312999096880058  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                     â€”
| 1.92259559851868e-07  |
| -5.877030441862918e-07  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                      â€”
| 3.780072876411861e-09  |
| 1.3962049876017843e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                     â€”
| -0.039829418314529734  |
| 0.026187317242858683  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                     â€”
| -0.011247447644936284  |
| 0.005629044894752246  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                   â€”
| -0.34651308152661786  |
| 0.3153322129158469  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                      â€”
| 6.114698173358708e-09  |
| 1.6833126551820736e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                        â€”
| -1.6646310790449439e-06  |
| -3.7586639387767626e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                        â€”
| -2.4225123446143382e-05  |
| -1.4964505422961083e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| 1.622552648307012e-07  |
| -2.0366376184766078e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                        â€”
| -6.294796744825883e-05   |
| -2.2330094522344895e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                       â€”
| -0.0017242974877772314  |
| 0.00042398513960207565  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                     â€”
| 7.67924514327161e-08  |
| -3.841676152198027e-08  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                      â€”
| -0.0006190903565469636  |
| 5.517101412364576e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.44260709  0.37038002]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.07478512 0.93086126]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.07035905 0.93456506]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.07478512 0.93086126]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 15              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.07478512 0.93086126]

[7.187580171723515, 3.471231530182341, 2.3686724515993025, 4.451990509861954, 5.240620487169285, 3.5222411354344105, 4.262005646710565, 5.764483440284738, 4.366434388777134, 4.544654910204675, 6.50567498172823, 4.557750874682837, 4.409615615112985, 5.2441318488689666, 4.550096218292411, 2.801886920218563, 5.00974937216955, 4.33700861055893, 4.488644138023066, 5.279866203663008, 4.525009197400127, 4.33089532384023, 5.995821397467381, 5.324686370964817, 5.857152175945536, 6.967777459049267, 6.706329641211171, 6.2579808766866964, 3.367196124043231, 4.752186044392098, 5.905041372943237, 4.623757033113152, 6.962174237591437, 5.391922240508943, 5.363421243592593, 4.63532822652848, 4.732178835003656, 6.166618800239452, 5.554262945916173, 5.787732891753274, 4.237139419347655, 4.305337043410611, 5.189681390640036, 4.555349944623174, 4.8727662881095295, 5.810353711986783, 4.688964563194054, 4.375800494352086, 4.584832850079451, 5.056132493256727, 3.938853459154085, 4.274378702255976, 5.921418762273382, 4.881950411456388, 4.746217319898652, 4.404092978125731, 3.224623480090594, 6.142760791698945, 6.10674645073701, 5.904647704527291, 6.556375537556776, 6.439555229509324, 5.067414047378016, 4.283845851825088, 6.703802567329461, 4.303873543090519, 6.402834934538062, 5.474713186940502, 5.248566051124064, 4.0775674282289405, 4.939483048591898, 4.638291829692265, 6.804293191051115, 4.918860393464179, 4.404598814093878, 4.691344343334003, 5.101307511848818, 5.66061948488715, 6.349660721726601, 4.742700283719975, 4.087363140683709, 2.70317368568934, 5.570654830830955, 4.9211169911141734, 5.835060040858386, 5.559141551232454, 4.9491463545467544, 5.005910660608396, 5.518166257261246, 4.391522418913977, 5.1673192431057045, 6.109181199181052, 4.08530560033833, 5.599002355304142, 4.745915489373403, 5.580638883138358, 5.080767424792324, 3.3889055446620335, 5.192864981501029, 5.871047988936063]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245, 5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287, 4.490416274328351, 3.5564524988044712, 4.091629946863637, 4.879829892905114, 4.1926418390225075, 4.210216335775425, 4.187504980422656, 4.530710047380715, 4.009372181787566, 3.9432288559544406, 4.411784013963423, 5.289816410463862, 3.4893621991347263, 4.70812136860741, 4.748987698855443, 3.7989359743023168, 4.515802235408018, 6.849494774956813, 3.8533509627750253, 5.262683037408525, 5.174250961590373, 4.811616579786701, 5.4144120224117165, 5.534109611204314, 4.757576389142094, 5.526458646858388, 4.68648626135325, 6.510397251598002, 4.053569161498328, 4.923389693719144, 5.171489048917507, 4.38273385252844, 3.5798745643728505, 5.776623075584453, 5.2610244417351675, 4.290574225520197, 4.856593546566716, 4.745192874359674, 3.6986305219136373, 4.379386401302874, 5.416012422669541, 4.659283968868628, 5.063523399259809, 5.7467416662796, 4.506312504147869, 4.476540161369129, 4.887133675349967, 7.101314492424933, 4.975301110000693, 4.862850942851848, 7.180855356630156, 2.9847168625895826, 6.5661988659086585, 4.130266496729279, 4.635203270500523, 5.2680387343232065, 5.599672587970324, 4.400983390586819, 2.518320319314273, 4.116474903067948, 4.804012214397902, 6.84000545726421, 5.297420062264238, 5.198714774949839, 6.736278646645381, 2.7653837751939716, 3.7074394406887117, 4.321519160329914, 4.295293892672745, 6.226190166839307, 4.668913001705613, 6.094720511993962, 5.253733943180695, 5.065868524410168, 5.120953283929446, 4.379225050802219, 5.923121760179944, 4.267138358789313, 4.759398905325478, 5.266646686893115, 6.473242173540947, 5.087655203318958, 3.965053905052038, 4.353966609391555, 5.343060515208491, 5.783994897105108, 5.01033882071759, 7.4186928993875325, 4.410049001959045, 7.374056817127453, 4.509450562163232, 5.746852509456756, 3.887434446403901, 4.02234765799981, 4.249898068401506, 4.9494169910636625, 4.938595751187503, 3.9514719173927, 4.478787355638231, 6.566513640431479, 4.525491827767309, 4.960166360955473, 6.62730134189487, 5.43112841504713, 4.9471173548619625, 5.43463957307096, 5.245356487899543, 6.597668615285253, 4.871918938790327, 3.6960369330631475, 3.951051628276191, 5.436916857828227, 4.683112858710698, 3.389905575467676, 6.773437205377399, 4.9731129801028136, 5.713770100318756, 5.78071325942541, 3.6774005993445718, 4.707968235738519, 5.894022120518272, 6.0115826438236235, 4.855906884524213, 5.036320434323684, 4.621962251875588, 3.8085309752160392, 6.632916483405368, 6.551570420264441, 5.328097831866994, 4.723646058902793, 4.684965447556247, 5.114917614624182, 6.0797356432271865, 6.334257343429244, 5.846049293573029, 5.846581982171187, 3.1574490721836947, 4.459369571447204, 4.773824744492791, 4.186583225587993, 5.495703629611403, 5.8122130284460605, 3.5691417657236753, 4.26265547659675, 4.523434629227358, 3.405932251233569, 5.626621309337312, 6.824971295892024, 5.9038830058265, 6.122729257147154, 5.230008982894638, 4.196378447740866, 6.177331674922204, 5.517840725857978, 5.210828974954717, 4.789102216128037, 4.781349903460918, 5.339287534498736, 4.9245071078156295, 6.701207863258989, 4.439469034941763, 3.9240069642493474, 3.571461598287253, 3.099771624686494, 5.999122977423209, 4.478828210834569, 4.961603054597413, 5.73423339159917, 4.750660014139897, 6.938201923584461, 5.502791389523651, 4.181805872180821, 6.488535024376849, 5.4999230250685995, 4.9746832551558136, 3.679784846830486, 6.046822572598391, 5.708897408940855, 7.061608116868901, 3.961717066019777, 5.603384170844793, 7.380843307721037, 6.781190249117925, 4.717267839259414, 3.9193847974155704, 4.790649456158062, 6.371011706777922, 4.689633998710858, 4.804061300502477, 4.894834442420745, 4.263084366604324, 5.144180615113666, 4.6066336477791845, 5.381709193101656, 3.5755635239163714, 4.492620532918825, 5.577956818303537, 6.8208208608907395, 5.82923381759672, 4.620459934894899, 6.9904670323299865, 6.164042199831909, 6.630221735394573, 3.8222843450965946, 6.125515905657428, 5.107895262736628, 6.803408080581033, 5.773340456981987, 6.008548120608629, 5.862181357017108, 7.043883173632293, 5.543745390710634, 3.869386773297068, 5.196806275202913, 4.413733794800476, 3.2790332151915615, 5.08138520994593, 4.336847519534556, 5.33269272348815, 5.188281310309852, 3.626726904604922, 6.085726993257404, 5.551210898534239, 4.614813888109768, 4.840177833878102, 5.5778413375269755, 6.115050488749432, 6.353907630856576, 7.664892913421985, 5.905035360960175, 4.321066304382384, 5.454549164953521, 7.487136297287175, 5.214240530104311, 4.200050491923914, 4.800876295931128, 3.8660799414320355, 5.261182328504775, 3.7654295503122177, 5.441861123290136, 5.651112180674322, 6.274835394016853, 5.28945498068728, 5.356461099182567, 5.1832124360021945, 6.665867026962602, 4.839347695362241, 4.919572087574838, 4.6206366192879305, 4.604594413140994, 4.984678484651757, 5.107017553153507, 5.686489608032262, 5.186002447477617, 4.7029937764974115, 6.303807780759701, 3.9430285221898838, 5.58503002718914, 6.18498041549213, 5.762318895721471, 5.686051595888033, 5.164742310256726, 5.479490034757343, 4.60408251234389, 5.379064196745426, 3.955130636735653, 6.114620893503636, 3.958982574562085, 3.838940291278325, 4.865896401991737, 7.062290581115448, 3.8008119492608645, 5.786056747809032, 3.74746781054968, 4.947768377377417, 5.67667560463442, 4.666177588305013, 5.334813027008264, 5.793369424289828, 4.4223341037659605, 5.330722476384998, 6.046541903653283, 4.197513571891748, 6.5968003456033655, 5.936296382786533, 5.170981254862737, 5.880836879058082, 4.096208966409434, 4.696039761279678, 4.652970837455283, 4.341410437282872, 5.258579423394452, 6.543783961505602, 5.099884384474556, 5.867561782235126, 4.424638904638933, 3.288712428222356, 5.059452664266722, 4.743613133176822, 4.434570194608235, 7.187580171723515, 3.471231530182341, 2.3686724515993025, 4.451990509861954, 5.240620487169285, 3.5222411354344105, 4.262005646710565, 5.764483440284738, 4.366434388777134, 4.544654910204675, 6.50567498172823, 4.557750874682837, 4.409615615112985, 5.2441318488689666, 4.550096218292411, 2.801886920218563, 5.00974937216955, 4.33700861055893, 4.488644138023066, 5.279866203663008, 4.525009197400127, 4.33089532384023, 5.995821397467381, 5.324686370964817, 5.857152175945536, 6.967777459049267, 6.706329641211171, 6.2579808766866964, 3.367196124043231, 4.752186044392098, 5.905041372943237, 4.623757033113152, 6.962174237591437, 5.391922240508943, 5.363421243592593, 4.63532822652848, 4.732178835003656, 6.166618800239452, 5.554262945916173, 5.787732891753274, 4.237139419347655, 4.305337043410611, 5.189681390640036, 4.555349944623174, 4.8727662881095295, 5.810353711986783, 4.688964563194054, 4.375800494352086, 4.584832850079451, 5.056132493256727, 3.938853459154085, 4.274378702255976, 5.921418762273382, 4.881950411456388, 4.746217319898652, 4.404092978125731, 3.224623480090594, 6.142760791698945, 6.10674645073701, 5.904647704527291, 6.556375537556776, 6.439555229509324, 5.067414047378016, 4.283845851825088, 6.703802567329461, 4.303873543090519, 6.402834934538062, 5.474713186940502, 5.248566051124064, 4.0775674282289405, 4.939483048591898, 4.638291829692265, 6.804293191051115, 4.918860393464179, 4.404598814093878, 4.691344343334003, 5.101307511848818, 5.66061948488715, 6.349660721726601, 4.742700283719975, 4.087363140683709, 2.70317368568934, 5.570654830830955, 4.9211169911141734, 5.835060040858386, 5.559141551232454, 4.9491463545467544, 5.005910660608396, 5.518166257261246, 4.391522418913977, 5.1673192431057045, 6.109181199181052, 4.08530560033833, 5.599002355304142, 4.745915489373403, 5.580638883138358, 5.080767424792324, 3.3889055446620335, 5.192864981501029, 5.871047988936063]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1500

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [3.1323925616990453, 4.525009197400127, 4.9746832551558136, 6.991232443315761, 5.837729023802759, 6.551147266661519, 6.248769371351054, 5.94425815480839, 5.837426140988665, 6.665867026962602, 3.6386479232645037, 4.2451914202734065, 4.404066951844861, 4.818517562928384, 2.190464992016754, 6.42544810019025, 3.0500257779373445, 4.63532822652848, 5.834628726160632, 4.210013662823948, 6.0186986796766915, 5.419524728917479, 4.676748312626479, 4.742700283719975, 4.445583036041847, 3.2566050881687696, 4.828453374610873, 5.587894222072076, 4.514892773948391, 5.660006359268667, 4.233542366661202, 3.178338978589365, 6.571746260902078, 6.748502320899499, 5.905041372943237]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (3.1323925616990453, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.1323925616990453, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.1323925616990453, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.1323925616990453, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.1323925616990453, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.08666179  1.6399414 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.0866617855830327  |
| 1.6399414004553137  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.002953158174953947

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.05449348518155989

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.05419286663561151

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.11308218385972826  |
| 0.08887312560509278  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.525009197400127, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.525009197400127, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.525009197400127, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.525009197400127, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.525009197400127, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.59061001 -0.3627269 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.590610007566994    |
| -0.36272690406846664  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.427797166393018e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3515274128103577

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.061695089376393e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.3988777674714137e-05  |
| -1.4732860850395929e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.9746832551558136, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9746832551558136, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9746832551558136, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9746832551558136, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9746832551558136, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.10753688 -0.53135486]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.10753688273190676  |
| -0.5313548578911309   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.686803436530439e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4112728837452787

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.101421472695872e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.410540799434203e-07  |
| -2.1793102237759478e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.991232443315761, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.991232443315761, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.991232443315761, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.991232443315761, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.991232443315761, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.05878937 1.58216978]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.0587893745371844  |
| 1.5821697774143217  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.712532330972105e-12

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.05750423076397825

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.6890117826002154e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 3.4773195114854323e-10  |
| 2.6723033961267493e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.837729023802759, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.837729023802759, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.837729023802759, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.837729023802759, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.837729023802759, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.8196107  -0.20125609]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.819610703750584  |
| -0.20125608513055226  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.587566170424398e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.30246959210771013

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.248680237116404e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.301874502904758e-08  |
| -1.0563288366241463e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (6.551147266661519, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.551147266661519, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.551147266661519, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.551147266661519, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.551147266661519, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.58601734 0.7205885 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.586017335242218  |
| 0.7205884955396868  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9119948899253253e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.12823586314824945

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4909985732422826e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 2.364749583983674e-09  |
| 1.074396418744476e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.248769371351054, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.248769371351054, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.248769371351054, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.248769371351054, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.248769371351054, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.26118064 0.25815134]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.2611806421425342  |
| 0.258151338083934   |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3241463339967665e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.19722320951390762

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.713947801885819e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.467501000093814e-09  |
| 1.7332146088825116e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.94425815480839, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.94425815480839, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.94425815480839, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.94425815480839, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.94425815480839, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.93405218 -0.10091021]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9340521756051601  |
| -0.10091020907765369  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.475882674601692e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2754958744586102

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.076591506590814e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.8736969901795068e-08  |
| -3.1045949217661257e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.837426140988665, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.837426140988665, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.837426140988665, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.837426140988665, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.837426140988665, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.81928533 -0.20152272]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 0.819285326247865  |
| -0.20152271629214624  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5903756322029082e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3025446733903508

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.256663799038251e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.306707515570395e-08  |
| -1.0593371674167812e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.665867026962602, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.665867026962602, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.665867026962602, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.665867026962602, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.665867026962602, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.70925779 0.92364407]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.7092577886401727  |
| 0.9236440723370265  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.958527564436651e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.10615015593552646

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.439486014394446e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.4425257202223517e-09  |
| 7.795081230766668e-10   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.6386479232645037, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.6386479232645037, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.6386479232645037, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.6386479232645037, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.6386479232645037, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.54280484  0.65298622]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.5428048394383609  |
| 0.6529862184301294  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0005320261647648437

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.13656485027146995

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.003895776722247763

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.006010423180455164  |
| 0.0025438885097086913  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.2451914202734065, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.2451914202734065, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.2451914202734065, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.2451914202734065, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.2451914202734065, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.89121096 -0.14000855]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8912109628056442   |
| -0.14000855141915736  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.8703306710259943e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.28570727912704036

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00017046575382702767

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.00015192094859357524  |
| -2.3866663259896822e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.404066951844861, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.404066951844861, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.404066951844861, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.404066951844861, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.404066951844861, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.72053511 -0.27755159]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.7205351137962168  |
| -0.2775515883790547  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.4499903201741734e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.32473234372669163

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.544645205517896e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -5.4361817917099186e-05  |
| -2.0940282605479114e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.818517562928384, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.818517562928384, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.818517562928384, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.818517562928384, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.818517562928384, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.27530162 -0.49924147]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.27530161927913355  |
| -0.49924147260682616  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.623774900253276e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39916059655430375

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.078488537032438e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.4993225948520825e-06  |
| -4.532357986272265e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (2.190464992016754, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.190464992016754, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.190464992016754, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.190464992016754, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.190464992016754, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-3.09855005  4.26336851]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -3.098550047653248  |
| 4.2633685115589515  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.03622495714386665

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.004739986225139951

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.642418231457431

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -23.680415375268474  |
| 32.582445240159664  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (6.42544810019025, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.42544810019025, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.42544810019025, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.42544810019025, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.42544810019025, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.450982  0.5155374]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4509819989960704  |
| 0.5155373972165478  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.322010781380066e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.15520477870488686

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.7847150180846724e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.040571363574876e-09  |
| 1.4356247324132037e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.0500257779373445, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.0500257779373445, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.0500257779373445, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.0500257779373445, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.0500257779373445, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.17514628  1.82849333]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -2.1751462764285634  |
| 1.828493330080505  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0038094625749144524

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.04572135384032193

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.08331911142042485

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.18123125496147371  |
| 0.15234843950048127  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.63532822652848, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.63532822652848, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.63532822652848, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.63532822652848, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.63532822652848, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.47209715 -0.42569912]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.4720971502614191   |
| -0.42569912150902667  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.61442514844752e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3727493095383737

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.3110505983541423e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.091040401592938e-05  |
| -9.838122094822687e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.834628726160632, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.834628726160632, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.834628726160632, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.834628726160632, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.834628726160632, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.81628014 -0.20398031]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8162801368172268  |
| -0.20398030819990254  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6165528709136442e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3032375903696618

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.330977828121459e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.351571310908587e-08  |
| -1.0874145003870623e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.210013662823948, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.210013662823948, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.210013662823948, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.210013662823948, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.210013662823948, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.92900151 -0.10561514]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9290015068330604   |
| -0.10561514329765487  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.651264984629786e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.27670509298702806

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00020423422365032932

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -0.0001897339015180362   |
| -2.1570226797114822e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.0186986796766915, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.0186986796766915, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.0186986796766915, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 1.01402169 -0.02301695]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0140216910592414  |
| -0.02301695278816851  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.430115505065784e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.25622727136935064

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1192574373702373e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.1489730104320425e-08  |
| -4.877884838192574e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.419524728917479, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.419524728917479, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.419524728917479, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.419524728917479, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.419524728917479, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.37034472 -0.46855932]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3703447248870617  |
| -0.4685593246733788  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6711850308268045e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38792150311388146

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.308049482722791e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.5954634004788204e-07  |
| -2.0185767562840897e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.676748312626479, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.676748312626479, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.676748312626479, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.676748312626479, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.676748312626479, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.42760063 -0.44571583]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.42760063156954686  |
| -0.4457158275794626   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.1034787932590645e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3797597738791171

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.870519018035913e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -7.99835113475005e-06  |
| -8.337199321270006e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.742700283719975, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.742700283719975, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.742700283719975, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.742700283719975, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.742700283719975, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.35675015 -0.47350164]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -0.356750148311491  |
| -0.47350163789872113  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.206818591176409e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.38971028929156537

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3360741900455391e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.766446654539013e-06  |
| -6.3263331734077e-06    |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.445583036041847, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.445583036041847, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.445583036041847, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.445583036041847, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.445583036041847, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.67593547 -0.30869263]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6759354675978102  |
| -0.3086926292361625  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.0388433598455126e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3342834474128035

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.09914542770574e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -4.122628716623325e-05   |
| -1.8827612381722034e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.2566050881687696, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.2566050881687696, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.2566050881687696, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.2566050881687696, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.2566050881687696, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.9532235   1.37040376]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.9532234984964703  |
| 1.3704037638007094  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.001985912280339395

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0700341743789832

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.02835633171875236

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.055386253444227915  |
| 0.03885962371495968  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (4.828453374610873, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.828453374610873, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.828453374610873, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.828453374610873, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.828453374610873, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.26462783 -0.50212302]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.26462783409009205  |
| -0.5021230164992119   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.4542005092747025e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4002327094959995

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.630480286392556e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.2838653053452994e-06  |
| -4.333562795240413e-06   |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.587894222072076, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.587894222072076, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.587894222072076, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.587894222072076, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.587894222072076, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.55121969 -0.38521536]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5512196854517981  |
| -0.38521536183111493  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.615803735917226e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3589637227597461

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.843028505792819e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0159135932418152e-07  |
| -7.0996289272404e-08    |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.514892773948391, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.514892773948391, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.514892773948391, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.514892773948391, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.514892773948391, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.60147782 -0.35624921]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.6014778208829341   |
| -0.35624921235211104  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.494599909017428e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.34941413928507603

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.2774454178513674e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.572788548874932e-05   |
| -1.5238365609886962e-05  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (5.660006359268667, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.660006359268667, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.660006359268667, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.660006359268667, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.660006359268667, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.62868788 -0.33951271]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6286878773131832  |
| -0.3395127090577432  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.410139039825353e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3440126744093625

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2819699295664464e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.059589537984601e-08  |
| -4.3524508371766846e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.233542366661202, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.233542366661202, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.233542366661202, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.233542366661202, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.233542366661202, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.90372523 -0.12877739]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9037252346466573   |
| -0.12877739097305607  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.116888024813712e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2827358681750617

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00018097767566036178

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0001635540924019671  |
| -2.330583289590934e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.178338978589365, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.178338978589365, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.178338978589365, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.178338978589365, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.178338978589365, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.03730275  1.53816395]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.037302748725267  |
| 1.5381639473233122  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.002554604492882893

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.059908706828223184

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.04264162303165273

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.08687389581249277  |
| 0.06558980720263963  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61b40>

args = (6.571746260902078, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.571746260902078, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.571746260902078, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.571746260902078, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.571746260902078, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.60814631 0.75593026]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.6081463050099387  |
| 0.7559302606097162  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6702744226945034e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.12408576365285787

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3460645069383306e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.1646686631379013e-09  |
| 1.0175308935273814e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (6.748502320899499, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.748502320899499, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.748502320899499, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.748502320899499, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.748502320899499, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.79803073 1.07932023]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.7980307331910694  |
| 1.0793202287473491  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.14669984325922e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.09183024818037352

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.604580130449001e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0077207321179317e-09  |
| 6.049136708429064e-10   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.905041372943237, array([5.07478512, 0.93086126])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.07478512 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478522 0.93086126]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.905041372943237, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.905041372943237, array([5.07478522, 0.93086126])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.07478512 0.93086136]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.905041372943237, array([5.07478512, 0.93086126]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.905041372943237, array([5.07478512, 0.93086136])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.89192261 -0.13937397]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8919226068826447  |
| -0.13937397236318816  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0692798694210279e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2855385605213956

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.744782727308405e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 3.3400563723500126e-08  |
| -5.21925244342026e-09   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                    â€”
| -0.11308218385972826  |
| 0.08887312560509278  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                        â€”
| -2.3988777674714137e-05  |
| -1.4732860850395929e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| -4.410540799434203e-07  |
| -2.1793102237759478e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                       â€”
| 3.4773195114854323e-10  |
| 2.6723033961267493e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                      â€”
| 4.301874502904758e-08  |
| -1.0563288366241463e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                      â€”
| 2.364749583983674e-09  |
| 1.074396418744476e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                      â€”
| 8.467501000093814e-09  |
| 1.7332146088825116e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| 2.8736969901795068e-08  |
| -3.1045949217661257e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                      â€”
| 4.306707515570395e-08  |
| -1.0593371674167812e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| 1.4425257202223517e-09  |
| 7.795081230766668e-10   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                      â€”
| -0.006010423180455164  |
| 0.0025438885097086913  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                        â€”
| -0.00015192094859357524  |
| -2.3866663259896822e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                        â€”
| -5.4361817917099186e-05  |
| -2.0940282605479114e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                        â€”
| -2.4993225948520825e-06  |
| -4.532357986272265e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                   â€”
| -23.680415375268474  |
| 32.582445240159664  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                      â€”
| 4.040571363574876e-09  |
| 1.4356247324132037e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                    â€”
| -0.18123125496147371  |
| 0.15234843950048127  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| -1.091040401592938e-05  |
| -9.838122094822687e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                      â€”
| 4.351571310908587e-08  |
| -1.0874145003870623e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                        â€”
| -0.0001897339015180362   |
| -2.1570226797114822e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                       â€”
| 2.1489730104320425e-08  |
| -4.877884838192574e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                       â€”
| 1.5954634004788204e-07  |
| -2.0185767562840897e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                      â€”
| -7.99835113475005e-06  |
| -8.337199321270006e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                       â€”
| -4.766446654539013e-06  |
| -6.3263331734077e-06    |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                        â€”
| -4.122628716623325e-05   |
| -1.8827612381722034e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                    â€”
| -0.055386253444227915  |
| 0.03885962371495968  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                        â€”
| -2.2838653053452994e-06  |
| -4.333562795240413e-06   |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| 1.0159135932418152e-07  |
| -7.0996289272404e-08    |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                        â€”
| -2.572788548874932e-05   |
| -1.5238365609886962e-05  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                      â€”
| 8.059589537984601e-08  |
| -4.3524508371766846e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                       â€”
| -0.0001635540924019671  |
| -2.330583289590934e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                    â€”
| -0.08687389581249277  |
| 0.06558980720263963  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                       â€”
| 2.1646686631379013e-09  |
| 1.0175308935273814e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                       â€”
| 1.0077207321179317e-09  |
| 6.049136708429064e-10   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| 3.3400563723500126e-08  |
| -5.21925244342026e-09   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-24.12367822  32.93048575]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.07478512 0.93086126]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.31602191 0.6015564 ]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 16              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.31602191 0.6015564 ]

[4.420798334956712, 5.704526635653845, 4.40551057329567, 5.68868091239222, 6.214698275642181, 5.491639083642935, 5.662377060672991, 5.076303314832115, 4.6879231927353855, 5.865231353401842, 5.635352716963373, 5.6603819948737595, 4.645907622701246, 5.703068184959901, 5.606409469121143, 5.35006571491482, 5.070573094465618, 5.022775912555119, 4.812150328463806, 6.0653513278246844, 5.100669618303849, 5.966054107360217, 5.640553456126811, 4.882034350080855, 6.0906605618087655, 5.635658258677284, 5.763931175758994, 5.1295760834957065, 5.643866865635124, 4.463595356358185, 4.762146724205129, 5.689140381483792, 5.643689876618276, 5.657135899915088, 5.09689029921085, 5.630351334023029, 4.483832227800276, 5.506559651096729, 5.052386323956622, 5.592271176795211, 4.6473303023765435, 5.631373347049518, 7.298080000200956, 3.494586881189134, 6.155857006230028, 5.248663624105395, 4.927322423036279, 6.031884461282644, 4.072045706267471, 5.432854015596811, 5.336305912813326, 5.691035743087922, 4.729040479752855, 5.144003774847542, 4.893172725037447, 4.850837419197311, 4.067924956739809, 5.357436860141771, 5.473700241978306, 4.885379869716033, 6.273080296810322, 5.617639568881801, 5.780279338259978, 4.516177687884646, 4.8392404912159295, 5.545366023647382, 6.025021874563144, 4.948578250656107, 5.156982120647383, 5.62497318974314, 6.277570128675382, 4.645792963953393, 4.703531983900468, 4.641046980791542, 5.190800369539718, 5.282296816584589, 4.507150043207611, 5.886459540660359, 5.332673701929155, 6.139120253116274, 5.564337393879438, 5.509122032768994, 6.111724592603875, 4.4387163156444895, 5.753968787060908, 4.806699736324585, 7.08799867042233, 5.571188252755882, 5.265302228773493, 6.08823374732821, 5.688739418706673, 5.039624719542464, 5.150241447087976, 5.234994764757508, 5.805524483325444, 5.5466495856619344, 5.571596375487135, 5.6457686536791885, 4.6846548904766365, 5.391762187040244]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245, 5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287, 4.490416274328351, 3.5564524988044712, 4.091629946863637, 4.879829892905114, 4.1926418390225075, 4.210216335775425, 4.187504980422656, 4.530710047380715, 4.009372181787566, 3.9432288559544406, 4.411784013963423, 5.289816410463862, 3.4893621991347263, 4.70812136860741, 4.748987698855443, 3.7989359743023168, 4.515802235408018, 6.849494774956813, 3.8533509627750253, 5.262683037408525, 5.174250961590373, 4.811616579786701, 5.4144120224117165, 5.534109611204314, 4.757576389142094, 5.526458646858388, 4.68648626135325, 6.510397251598002, 4.053569161498328, 4.923389693719144, 5.171489048917507, 4.38273385252844, 3.5798745643728505, 5.776623075584453, 5.2610244417351675, 4.290574225520197, 4.856593546566716, 4.745192874359674, 3.6986305219136373, 4.379386401302874, 5.416012422669541, 4.659283968868628, 5.063523399259809, 5.7467416662796, 4.506312504147869, 4.476540161369129, 4.887133675349967, 7.101314492424933, 4.975301110000693, 4.862850942851848, 7.180855356630156, 2.9847168625895826, 6.5661988659086585, 4.130266496729279, 4.635203270500523, 5.2680387343232065, 5.599672587970324, 4.400983390586819, 2.518320319314273, 4.116474903067948, 4.804012214397902, 6.84000545726421, 5.297420062264238, 5.198714774949839, 6.736278646645381, 2.7653837751939716, 3.7074394406887117, 4.321519160329914, 4.295293892672745, 6.226190166839307, 4.668913001705613, 6.094720511993962, 5.253733943180695, 5.065868524410168, 5.120953283929446, 4.379225050802219, 5.923121760179944, 4.267138358789313, 4.759398905325478, 5.266646686893115, 6.473242173540947, 5.087655203318958, 3.965053905052038, 4.353966609391555, 5.343060515208491, 5.783994897105108, 5.01033882071759, 7.4186928993875325, 4.410049001959045, 7.374056817127453, 4.509450562163232, 5.746852509456756, 3.887434446403901, 4.02234765799981, 4.249898068401506, 4.9494169910636625, 4.938595751187503, 3.9514719173927, 4.478787355638231, 6.566513640431479, 4.525491827767309, 4.960166360955473, 6.62730134189487, 5.43112841504713, 4.9471173548619625, 5.43463957307096, 5.245356487899543, 6.597668615285253, 4.871918938790327, 3.6960369330631475, 3.951051628276191, 5.436916857828227, 4.683112858710698, 3.389905575467676, 6.773437205377399, 4.9731129801028136, 5.713770100318756, 5.78071325942541, 3.6774005993445718, 4.707968235738519, 5.894022120518272, 6.0115826438236235, 4.855906884524213, 5.036320434323684, 4.621962251875588, 3.8085309752160392, 6.632916483405368, 6.551570420264441, 5.328097831866994, 4.723646058902793, 4.684965447556247, 5.114917614624182, 6.0797356432271865, 6.334257343429244, 5.846049293573029, 5.846581982171187, 3.1574490721836947, 4.459369571447204, 4.773824744492791, 4.186583225587993, 5.495703629611403, 5.8122130284460605, 3.5691417657236753, 4.26265547659675, 4.523434629227358, 3.405932251233569, 5.626621309337312, 6.824971295892024, 5.9038830058265, 6.122729257147154, 5.230008982894638, 4.196378447740866, 6.177331674922204, 5.517840725857978, 5.210828974954717, 4.789102216128037, 4.781349903460918, 5.339287534498736, 4.9245071078156295, 6.701207863258989, 4.439469034941763, 3.9240069642493474, 3.571461598287253, 3.099771624686494, 5.999122977423209, 4.478828210834569, 4.961603054597413, 5.73423339159917, 4.750660014139897, 6.938201923584461, 5.502791389523651, 4.181805872180821, 6.488535024376849, 5.4999230250685995, 4.9746832551558136, 3.679784846830486, 6.046822572598391, 5.708897408940855, 7.061608116868901, 3.961717066019777, 5.603384170844793, 7.380843307721037, 6.781190249117925, 4.717267839259414, 3.9193847974155704, 4.790649456158062, 6.371011706777922, 4.689633998710858, 4.804061300502477, 4.894834442420745, 4.263084366604324, 5.144180615113666, 4.6066336477791845, 5.381709193101656, 3.5755635239163714, 4.492620532918825, 5.577956818303537, 6.8208208608907395, 5.82923381759672, 4.620459934894899, 6.9904670323299865, 6.164042199831909, 6.630221735394573, 3.8222843450965946, 6.125515905657428, 5.107895262736628, 6.803408080581033, 5.773340456981987, 6.008548120608629, 5.862181357017108, 7.043883173632293, 5.543745390710634, 3.869386773297068, 5.196806275202913, 4.413733794800476, 3.2790332151915615, 5.08138520994593, 4.336847519534556, 5.33269272348815, 5.188281310309852, 3.626726904604922, 6.085726993257404, 5.551210898534239, 4.614813888109768, 4.840177833878102, 5.5778413375269755, 6.115050488749432, 6.353907630856576, 7.664892913421985, 5.905035360960175, 4.321066304382384, 5.454549164953521, 7.487136297287175, 5.214240530104311, 4.200050491923914, 4.800876295931128, 3.8660799414320355, 5.261182328504775, 3.7654295503122177, 5.441861123290136, 5.651112180674322, 6.274835394016853, 5.28945498068728, 5.356461099182567, 5.1832124360021945, 6.665867026962602, 4.839347695362241, 4.919572087574838, 4.6206366192879305, 4.604594413140994, 4.984678484651757, 5.107017553153507, 5.686489608032262, 5.186002447477617, 4.7029937764974115, 6.303807780759701, 3.9430285221898838, 5.58503002718914, 6.18498041549213, 5.762318895721471, 5.686051595888033, 5.164742310256726, 5.479490034757343, 4.60408251234389, 5.379064196745426, 3.955130636735653, 6.114620893503636, 3.958982574562085, 3.838940291278325, 4.865896401991737, 7.062290581115448, 3.8008119492608645, 5.786056747809032, 3.74746781054968, 4.947768377377417, 5.67667560463442, 4.666177588305013, 5.334813027008264, 5.793369424289828, 4.4223341037659605, 5.330722476384998, 6.046541903653283, 4.197513571891748, 6.5968003456033655, 5.936296382786533, 5.170981254862737, 5.880836879058082, 4.096208966409434, 4.696039761279678, 4.652970837455283, 4.341410437282872, 5.258579423394452, 6.543783961505602, 5.099884384474556, 5.867561782235126, 4.424638904638933, 3.288712428222356, 5.059452664266722, 4.743613133176822, 4.434570194608235, 7.187580171723515, 3.471231530182341, 2.3686724515993025, 4.451990509861954, 5.240620487169285, 3.5222411354344105, 4.262005646710565, 5.764483440284738, 4.366434388777134, 4.544654910204675, 6.50567498172823, 4.557750874682837, 4.409615615112985, 5.2441318488689666, 4.550096218292411, 2.801886920218563, 5.00974937216955, 4.33700861055893, 4.488644138023066, 5.279866203663008, 4.525009197400127, 4.33089532384023, 5.995821397467381, 5.324686370964817, 5.857152175945536, 6.967777459049267, 6.706329641211171, 6.2579808766866964, 3.367196124043231, 4.752186044392098, 5.905041372943237, 4.623757033113152, 6.962174237591437, 5.391922240508943, 5.363421243592593, 4.63532822652848, 4.732178835003656, 6.166618800239452, 5.554262945916173, 5.787732891753274, 4.237139419347655, 4.305337043410611, 5.189681390640036, 4.555349944623174, 4.8727662881095295, 5.810353711986783, 4.688964563194054, 4.375800494352086, 4.584832850079451, 5.056132493256727, 3.938853459154085, 4.274378702255976, 5.921418762273382, 4.881950411456388, 4.746217319898652, 4.404092978125731, 3.224623480090594, 6.142760791698945, 6.10674645073701, 5.904647704527291, 6.556375537556776, 6.439555229509324, 5.067414047378016, 4.283845851825088, 6.703802567329461, 4.303873543090519, 6.402834934538062, 5.474713186940502, 5.248566051124064, 4.0775674282289405, 4.939483048591898, 4.638291829692265, 6.804293191051115, 4.918860393464179, 4.404598814093878, 4.691344343334003, 5.101307511848818, 5.66061948488715, 6.349660721726601, 4.742700283719975, 4.087363140683709, 2.70317368568934, 5.570654830830955, 4.9211169911141734, 5.835060040858386, 5.559141551232454, 4.9491463545467544, 5.005910660608396, 5.518166257261246, 4.391522418913977, 5.1673192431057045, 6.109181199181052, 4.08530560033833, 5.599002355304142, 4.745915489373403, 5.580638883138358, 5.080767424792324, 3.3889055446620335, 5.192864981501029, 5.871047988936063, 4.420798334956712, 5.704526635653845, 4.40551057329567, 5.68868091239222, 6.214698275642181, 5.491639083642935, 5.662377060672991, 5.076303314832115, 4.6879231927353855, 5.865231353401842, 5.635352716963373, 5.6603819948737595, 4.645907622701246, 5.703068184959901, 5.606409469121143, 5.35006571491482, 5.070573094465618, 5.022775912555119, 4.812150328463806, 6.0653513278246844, 5.100669618303849, 5.966054107360217, 5.640553456126811, 4.882034350080855, 6.0906605618087655, 5.635658258677284, 5.763931175758994, 5.1295760834957065, 5.643866865635124, 4.463595356358185, 4.762146724205129, 5.689140381483792, 5.643689876618276, 5.657135899915088, 5.09689029921085, 5.630351334023029, 4.483832227800276, 5.506559651096729, 5.052386323956622, 5.592271176795211, 4.6473303023765435, 5.631373347049518, 7.298080000200956, 3.494586881189134, 6.155857006230028, 5.248663624105395, 4.927322423036279, 6.031884461282644, 4.072045706267471, 5.432854015596811, 5.336305912813326, 5.691035743087922, 4.729040479752855, 5.144003774847542, 4.893172725037447, 4.850837419197311, 4.067924956739809, 5.357436860141771, 5.473700241978306, 4.885379869716033, 6.273080296810322, 5.617639568881801, 5.780279338259978, 4.516177687884646, 4.8392404912159295, 5.545366023647382, 6.025021874563144, 4.948578250656107, 5.156982120647383, 5.62497318974314, 6.277570128675382, 4.645792963953393, 4.703531983900468, 4.641046980791542, 5.190800369539718, 5.282296816584589, 4.507150043207611, 5.886459540660359, 5.332673701929155, 6.139120253116274, 5.564337393879438, 5.509122032768994, 6.111724592603875, 4.4387163156444895, 5.753968787060908, 4.806699736324585, 7.08799867042233, 5.571188252755882, 5.265302228773493, 6.08823374732821, 5.688739418706673, 5.039624719542464, 5.150241447087976, 5.234994764757508, 5.805524483325444, 5.5466495856619344, 5.571596375487135, 5.6457686536791885, 4.6846548904766365, 5.391762187040244]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1600

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [4.497423308210756, 4.509213114986299, 4.939483048591898, 3.5047445905543126, 3.367196124043231, 5.067893487853502, 4.812856458542559, 6.112267701173042, 5.215515562044457, 5.107895262736628, 5.416012422669541, 3.9432288559544406, 5.402274786830667, 6.18498041549213, 6.427940779193832, 4.322057445540274, 4.906538074218057, 5.42795546724073, 5.7340549665814295, 4.746358885990728, 4.262005646710565, 6.210986605519358, 4.093561787638524, 6.006003343642173, 5.68868091239222, 4.825133460880819, 4.742700283719975, 4.3200161395055625, 6.284719622689429, 4.106993251266785, 5.704526635653845, 4.186583225587993, 5.039339884578826, 4.604594413140994, 5.805524483325444]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.497423308210756, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.497423308210756, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.497423308210756, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.497423308210756, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.497423308210756, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.36080116  0.09471244]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.360801160643632   |
| 0.09471244011294289  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6170099357709413e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.29470041857798485

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.486961788427326e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -7.466663970099165e-05  |
| 5.196835397884291e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.509213114986299, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.509213114986299, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.509213114986299, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.509213114986299, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.509213114986299, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.34120232  0.06823438]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.3412023225356506  |
| 0.0682343781477357  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5333969047442615e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2994320035473733

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.121018750761763e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -6.868322242270293e-05  |
| 3.4942952994112317e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.939483048591898, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.939483048591898, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.939483048591898, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.939483048591898, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.939483048591898, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.62594115 -0.63527611]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6259411544640159  |
| -0.6352761106054317  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.008374384864729e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4571852490901312

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.392911601723158e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.7497041594409625e-06  |
| -2.7907117965761648e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.5047445905543126, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.5047445905543126, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.5047445905543126, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.5047445905543126, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.5047445905543126, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-3.01098511  3.70183745]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -3.010985105689201  |
| 3.701837445468925  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0008583008699094368

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.033651909618253976

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.02550526492095011

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.07679597279363805  |
| 0.09441634474097814  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (3.367196124043231, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.367196124043231, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.367196124043231, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.367196124043231, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.367196124043231, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-3.23963942  4.41645344]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -3.2396394233913384  |
| 4.416453442956936  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0013768687191472032

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.021893458786393805

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.06288950195493505

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.20373930985065422  |
| 0.2777485574347198  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.067893487853502, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.067893487853502, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.067893487853502, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.067893487853502, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.067893487853502, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.41247748 -0.7461084 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4124774799496578  |
| -0.7461084006177288  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0563291403373597e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.48870574182005877

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1614829741990212e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -8.915630501517035e-07  |
| -1.6127006048420832e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.812856458542559, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.812856458542559, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.812856458542559, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.812856458542559, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.812856458542559, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.83643944 -0.48136185]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.8364394377124995   |
| -0.48136185037961354  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.723925595511428e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4167558441942916

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.93550899738633e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -7.474012121448802e-06  |
| -4.301213145065569e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (6.112267701173042, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.112267701173042, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.112267701173042, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.112267701173042, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.112267701173042, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.32364271 0.04483778]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 1.323642710993056  |
| 0.044837784596296615  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.0784280038731676e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.30367611783047527

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.0137208107987192e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.3418041621956953e-08  |
| 4.5452995355376123e-10  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.215515562044457, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.215515562044457, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.215515562044457, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.215515562044457, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.215515562044457, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.16707726 -0.8172198 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.1670772553641342  |
| -0.8172197973888018  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.944895482175327e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5100649534014011

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.694638789040438e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -1.61975364061955e-07  |
| -7.922650746937246e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.107895262736628, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.107895262736628, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.107895262736628, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.107895262736628, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.107895262736628, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.34598035 -0.77132603]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.34598034881483386  |
| -0.771326025272856    |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.618060938234414e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4961758398193425

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.7368965287330894e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -6.009320668663484e-07  |
| -1.3397134958179147e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.416012422669541, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.416012422669541, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.416012422669541, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.416012422669541, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.416012422669541, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.1662196 -0.8173627]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1662196047469422  |
| -0.8173626986351934  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.703290275230342e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5101088013644149

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.339072508991144e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 5.5501931266588854e-08  |
| -2.7292333168875875e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (3.9432288559544406, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9432288559544406, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9432288559544406, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9432288559544406, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9432288559544406, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.28206881  1.7727412 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.2820688139191248  |
| 1.7727412027213063  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00016767794063580948

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.10739677940426628

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00156129393791811

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0035629802050839007  |
| 0.0027677700933064348  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.402274786830667, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.402274786830667, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.402274786830667, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.402274786830667, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.402274786830667, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.14338278 -0.82089787]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1433827845342961  |
| -0.8208978652390897  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.8346811047750917e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5111947548460413

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.589006122192413e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.146016915105843e-08  |
| -2.9462074640377757e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (6.18498041549213, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.18498041549213, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.18498041549213, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.18498041549213, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.18498041549213, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.44451702 0.21213746]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4445170215537928  |
| 0.21213746315140725  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9686261894800643e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.27460165544784326

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.169025205873084e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0355778937831854e-08  |
| 1.5208188204424112e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (6.427940779193832, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.427940779193832, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.427940779193832, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.427940779193832, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.427940779193832, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.84840328 0.87712002]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.8484032815457851  |
| 0.8771200166179938  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.25332515450338e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.18406647064825502

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.3107549895012355e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 4.27120710544238e-09  |
| 2.0268094547914356e-09  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.322057445540274, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.322057445540274, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.322057445540274, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.322057445540274, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.322057445540274, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.65232139  0.53390543]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.6523213930419445  |
| 0.5339054331976456  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.5039701369941476e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22627699884661745

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00015485312934388547

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.00025586713839439323  |
| 8.267692710435821e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.906538074218057, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.906538074218057, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.906538074218057, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.906538074218057, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.906538074218057, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.68070738 -0.59949602]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.680707380462664   |
| -0.5994960150523809  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.362012684230835e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.447450053582676

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.2788298164644255e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.5933384162737043e-06  |
| -3.1646374391101144e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.42795546724073, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.42795546724073, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.42795546724073, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.42795546724073, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.42795546724073, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.18607318 -0.81386556]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1860731790870318  |
| -0.8138655605005596  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5964889960126862e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5090368006225469

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.136293867280708e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 5.8358017043608275e-08  |
| -2.552521566188881e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.7340549665814295, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.7340549665814295, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.7340549665814295, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.7340549665814295, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.7340549665814295, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.69491906 -0.58972092]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6949190634042424  |
| -0.5897209187644137  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8922913158904048e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.444826645934152

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.502063989032152e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.518408217452675e-08  |
| -3.834403149477049e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.746358885990728, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.746358885990728, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.746358885990728, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.746358885990728, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.746358885990728, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.94698198 -0.38278991]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9469819772789378   |
| -0.38278991087992154  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.1172167485579585e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3927620481336287

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3028796373973835e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -1.2338035351790398e-05  |
| -4.98729180286609e-06  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.262005646710565, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.262005646710565, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.262005646710565, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.262005646710565, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.262005646710565, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.75214877  0.70383507]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.7521487705174366  |
| 0.7038350657850856  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.5341646478718016e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.20428940364751877

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00022194810728876844

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.00038888610330468774  |
| 0.00015621486069446557  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.210986605519358, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.210986605519358, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.210986605519358, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.210986605519358, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.210986605519358, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.48774853 0.27552059]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4877485288167236  |
| 0.2755205907867264  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6755642877825145e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2643285910574008

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.3389445730396e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 9.430755462790418e-09  |
| 1.7465097537281838e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.093561787638524, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.093561787638524, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.093561787638524, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.093561787638524, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.093561787638524, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.03216218  1.23366386]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.0321621829744174  |
| 1.2336638577359338  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.164734180973534e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.14853426583814106

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0006170114437405586

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0012538673224320104  |
| 0.0007611847179521956  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (6.006003343642173, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.006003343642173, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.006003343642173, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.006003343642173, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.006003343642173, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 1.14699368 -0.17337996]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.1469936778141232  |
| -0.17337995705801745  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.8608182199593755e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.34627457660483724

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.6925349465224078e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.9413268831406668e-08  |
| -2.934516363472489e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.68868091239222, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.68868091239222, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.68868091239222, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.68868091239222, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.68868091239222, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.6194913  -0.63929243]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6194913004797797  |
| -0.6392924323250782  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.747904745279703e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.45829116510757756

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.177999120711686e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 5.0661993106121775e-08  |
| -5.228132949432124e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.825133460880819, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.825133460880819, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.825133460880819, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.825133460880819, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.825133460880819, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.81603071 -0.49822425]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.8160307063409533  |
| -0.4982242540751969  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.5099983489875487e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.42100479941873037

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 8.337193195502061e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -6.803405652226537e-06   |
| -4.1537918609098214e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.742700283719975, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.742700283719975, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.742700283719975, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.742700283719975, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.742700283719975, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.95306387 -0.37701197]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9530638733767915   |
| -0.37701197386219576  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.206818591176409e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39139927290865506

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3303087030495272e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.2678691653152384e-05  |
| -5.015423099827599e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.3200161395055625, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3200161395055625, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3200161395055625, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3200161395055625, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3200161395055625, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.65571477  0.53951814]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.6557147675122508  |
| 0.5395181368328394  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.535013820543432e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.22551429524192418

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00015675342517649214

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0002595389609228447  |
| 8.457131589338694e-05  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (6.284719622689429, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.284719622689429, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.284719622689429, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.284719622689429, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.284719622689429, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.61031894 0.46538627]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.6103189426353026  |
| 0.4653862717418633  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0570446369371937e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23579860108461184

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.482828278348833e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 7.218783293206327e-09  |
| 2.086246739319759e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (4.106993251266785, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.106993251266785, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.106993251266785, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.106993251266785, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.106993251266785, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.00983433  1.18853931]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.0098343278540654  |
| 1.1885393136346067  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.673653943594703e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1526214452461216

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.000568311611098121

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.001142212184903053  |
| 0.0006754606921851383  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.704526635653845, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.704526635653845, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.704526635653845, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.704526635653845, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.704526635653845, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.64583251 -0.62262736]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.6458325096936335  |
| -0.6226273552289996  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.424413247041058e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4537197546992813

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.547419330045941e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.874368767633812e-08  |
| -4.6992297362707323e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (4.186583225587993, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.186583225587993, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.186583225587993, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.186583225587993, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.186583225587993, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.87752758  0.93137725]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.8775275756333087  |
| 0.9313772508789953  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.235428984291831e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.17815574710161314

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00034999875590515663

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0006571323156492829  |
| 0.0003259808790860133  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb613f0>

args = (5.039339884578826, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.039339884578826, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.039339884578826, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.039339884578826, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.039339884578826, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.45994369 -0.72540315]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4599436931318479  |
| -0.7254031453296506  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.2203025119297011e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.48265647647820176

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.5283044388710563e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.162877680975998e-06  |
| -1.8340399923079816e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.604594413140994, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.604594413140994, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.604594413140994, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.604594413140994, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.604594413140994, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.18264479 -0.13185306]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -1.182644788766396    |
| -0.13185305514085144  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.928676339172573e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3377315189435113

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.9398133672069963e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -3.476754958673146e-05  |
| -3.876233740101562e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61360>

args = (5.805524483325444, array([5.31602191, 0.6015564 ])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31602191 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602201 0.6015564 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.805524483325444, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.805524483325444, array([5.31602201, 0.6015564 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31602191 0.6015565 ]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.805524483325444, array([5.31602191, 0.6015564 ]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.805524483325444, array([5.31602191, 0.6015565 ])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.81372674 -0.50010157]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.8137267393237835  |
| -0.5001015734862335  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.914937428147376e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4214805145625369

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.543359329754373e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.697052972977316e-08  |
| -2.272141149723521e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| -7.466663970099165e-05  |
| 5.196835397884291e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                       â€”
| -6.868322242270293e-05  |
| 3.4942952994112317e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                        â€”
| -2.7497041594409625e-06  |
| -2.7907117965761648e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                    â€”
| -0.07679597279363805  |
| 0.09441634474097814  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                   â€”
| -0.20373930985065422  |
| 0.2777485574347198  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| -8.915630501517035e-07  |
| -1.6127006048420832e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| -7.474012121448802e-06  |
| -4.301213145065569e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                       â€”
| 1.3418041621956953e-08  |
| 4.5452995355376123e-10  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                      â€”
| -1.61975364061955e-07  |
| -7.922650746937246e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| -6.009320668663484e-07  |
| -1.3397134958179147e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                       â€”
| 5.5501931266588854e-08  |
| -2.7292333168875875e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                      â€”
| -0.0035629802050839007  |
| 0.0027677700933064348  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                      â€”
| 5.146016915105843e-08  |
| -2.9462074640377757e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| 1.0355778937831854e-08  |
| 1.5208188204424112e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                     â€”
| 4.27120710544238e-09  |
| 2.0268094547914356e-09  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                      â€”
| -0.00025586713839439323  |
| 8.267692710435821e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                        â€”
| -3.5933384162737043e-06  |
| -3.1646374391101144e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| 5.8358017043608275e-08  |
| -2.552521566188881e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                      â€”
| 4.518408217452675e-08  |
| -3.834403149477049e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                      â€”
| -1.2338035351790398e-05  |
| -4.98729180286609e-06  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                       â€”
| -0.00038888610330468774  |
| 0.00015621486069446557  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                      â€”
| 9.430755462790418e-09  |
| 1.7465097537281838e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                      â€”
| -0.0012538673224320104  |
| 0.0007611847179521956  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                       â€”
| 1.9413268831406668e-08  |
| -2.934516363472489e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                       â€”
| 5.0661993106121775e-08  |
| -5.228132949432124e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                        â€”
| -6.803405652226537e-06   |
| -4.1537918609098214e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                       â€”
| -1.2678691653152384e-05  |
| -5.015423099827599e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                      â€”
| -0.0002595389609228447  |
| 8.457131589338694e-05  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                      â€”
| 7.218783293206327e-09  |
| 2.086246739319759e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                      â€”
| -0.001142212184903053  |
| 0.0006754606921851383  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                      â€”
| 4.874368767633812e-08  |
| -4.6992297362707323e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                      â€”
| -0.0006571323156492829  |
| 0.0003259808790860133  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                       â€”
| -1.162877680975998e-06  |
| -1.8340399923079816e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                       â€”
| -3.476754958673146e-05  |
| -3.876233740101562e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                      â€”
| 3.697052972977316e-08  |
| -2.272141149723521e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-0.28828193  0.37699261]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.31890473 0.59778648]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.31602191 0.6015564 ]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.31890473 0.59778648]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 17              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.31890473 0.59778648]

[4.988814880700932, 5.801538893565885, 5.0597762553799965, 5.8867796140020845, 4.726414172879279, 5.671094389102945, 5.617999727632223, 5.475483110051719, 5.069282489857713, 5.87111306059723, 4.781123508560691, 5.05147682900074, 4.641678506660446, 4.945440662445091, 5.7083534611631555, 5.9863294310925825, 4.577400973729236, 6.604951691116456, 6.102918448259324, 6.27169659505453, 4.097987659844832, 4.791932910706026, 4.113250666031916, 4.091794682238024, 4.8639562821092355, 4.678339762528495, 4.381259810280782, 5.955747655926387, 5.6580770222674435, 4.674522965625474, 5.220705733663694, 5.335702517976408, 6.238195034736511, 5.609715893938521, 4.72323294262587, 5.7530320718381756, 4.753568669768855, 5.1449872939276915, 5.354122570909834, 5.32365074599894, 5.291778393055308, 5.882198641563992, 5.8221979802375445, 4.109545177934299, 4.924330645176363, 6.065834648556958, 4.908852418420793, 6.613117909707456, 5.805451294765698, 4.0944033025623, 4.050453480680038, 5.560211666220153, 5.042606603739396, 6.2005351155033415, 4.783126259591142, 4.732021265334232, 5.205760223640613, 4.345497142361654, 4.610532546046786, 5.141688029888546, 5.468103808401066, 5.278902749374555, 6.09363873620213, 4.332090151997182, 6.170183436232143, 6.321093116510872, 5.191947004919269, 4.475181838919639, 5.053410647023033, 5.2400997742242845, 5.253770164162756, 4.403502601430028, 5.698483215352255, 5.518211262276038, 5.408284495113768, 5.048531015805539, 6.586748892876148, 5.1960968575696285, 5.800175413825111, 6.133649800420464, 5.108531838544709, 4.996604554593744, 4.317001794504348, 4.2528434145294325, 7.0056981009043, 4.902698544344471, 5.018486950309088, 5.358317497733772, 5.028496845859937, 6.3507532218812335, 5.073849456823711, 5.8868780263200895, 5.174874241347814, 5.573813331874844, 5.828890508183712, 5.280872574143104, 4.999548890432387, 5.484573149864807, 5.331465739012567, 5.259520448780412]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245, 5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287, 4.490416274328351, 3.5564524988044712, 4.091629946863637, 4.879829892905114, 4.1926418390225075, 4.210216335775425, 4.187504980422656, 4.530710047380715, 4.009372181787566, 3.9432288559544406, 4.411784013963423, 5.289816410463862, 3.4893621991347263, 4.70812136860741, 4.748987698855443, 3.7989359743023168, 4.515802235408018, 6.849494774956813, 3.8533509627750253, 5.262683037408525, 5.174250961590373, 4.811616579786701, 5.4144120224117165, 5.534109611204314, 4.757576389142094, 5.526458646858388, 4.68648626135325, 6.510397251598002, 4.053569161498328, 4.923389693719144, 5.171489048917507, 4.38273385252844, 3.5798745643728505, 5.776623075584453, 5.2610244417351675, 4.290574225520197, 4.856593546566716, 4.745192874359674, 3.6986305219136373, 4.379386401302874, 5.416012422669541, 4.659283968868628, 5.063523399259809, 5.7467416662796, 4.506312504147869, 4.476540161369129, 4.887133675349967, 7.101314492424933, 4.975301110000693, 4.862850942851848, 7.180855356630156, 2.9847168625895826, 6.5661988659086585, 4.130266496729279, 4.635203270500523, 5.2680387343232065, 5.599672587970324, 4.400983390586819, 2.518320319314273, 4.116474903067948, 4.804012214397902, 6.84000545726421, 5.297420062264238, 5.198714774949839, 6.736278646645381, 2.7653837751939716, 3.7074394406887117, 4.321519160329914, 4.295293892672745, 6.226190166839307, 4.668913001705613, 6.094720511993962, 5.253733943180695, 5.065868524410168, 5.120953283929446, 4.379225050802219, 5.923121760179944, 4.267138358789313, 4.759398905325478, 5.266646686893115, 6.473242173540947, 5.087655203318958, 3.965053905052038, 4.353966609391555, 5.343060515208491, 5.783994897105108, 5.01033882071759, 7.4186928993875325, 4.410049001959045, 7.374056817127453, 4.509450562163232, 5.746852509456756, 3.887434446403901, 4.02234765799981, 4.249898068401506, 4.9494169910636625, 4.938595751187503, 3.9514719173927, 4.478787355638231, 6.566513640431479, 4.525491827767309, 4.960166360955473, 6.62730134189487, 5.43112841504713, 4.9471173548619625, 5.43463957307096, 5.245356487899543, 6.597668615285253, 4.871918938790327, 3.6960369330631475, 3.951051628276191, 5.436916857828227, 4.683112858710698, 3.389905575467676, 6.773437205377399, 4.9731129801028136, 5.713770100318756, 5.78071325942541, 3.6774005993445718, 4.707968235738519, 5.894022120518272, 6.0115826438236235, 4.855906884524213, 5.036320434323684, 4.621962251875588, 3.8085309752160392, 6.632916483405368, 6.551570420264441, 5.328097831866994, 4.723646058902793, 4.684965447556247, 5.114917614624182, 6.0797356432271865, 6.334257343429244, 5.846049293573029, 5.846581982171187, 3.1574490721836947, 4.459369571447204, 4.773824744492791, 4.186583225587993, 5.495703629611403, 5.8122130284460605, 3.5691417657236753, 4.26265547659675, 4.523434629227358, 3.405932251233569, 5.626621309337312, 6.824971295892024, 5.9038830058265, 6.122729257147154, 5.230008982894638, 4.196378447740866, 6.177331674922204, 5.517840725857978, 5.210828974954717, 4.789102216128037, 4.781349903460918, 5.339287534498736, 4.9245071078156295, 6.701207863258989, 4.439469034941763, 3.9240069642493474, 3.571461598287253, 3.099771624686494, 5.999122977423209, 4.478828210834569, 4.961603054597413, 5.73423339159917, 4.750660014139897, 6.938201923584461, 5.502791389523651, 4.181805872180821, 6.488535024376849, 5.4999230250685995, 4.9746832551558136, 3.679784846830486, 6.046822572598391, 5.708897408940855, 7.061608116868901, 3.961717066019777, 5.603384170844793, 7.380843307721037, 6.781190249117925, 4.717267839259414, 3.9193847974155704, 4.790649456158062, 6.371011706777922, 4.689633998710858, 4.804061300502477, 4.894834442420745, 4.263084366604324, 5.144180615113666, 4.6066336477791845, 5.381709193101656, 3.5755635239163714, 4.492620532918825, 5.577956818303537, 6.8208208608907395, 5.82923381759672, 4.620459934894899, 6.9904670323299865, 6.164042199831909, 6.630221735394573, 3.8222843450965946, 6.125515905657428, 5.107895262736628, 6.803408080581033, 5.773340456981987, 6.008548120608629, 5.862181357017108, 7.043883173632293, 5.543745390710634, 3.869386773297068, 5.196806275202913, 4.413733794800476, 3.2790332151915615, 5.08138520994593, 4.336847519534556, 5.33269272348815, 5.188281310309852, 3.626726904604922, 6.085726993257404, 5.551210898534239, 4.614813888109768, 4.840177833878102, 5.5778413375269755, 6.115050488749432, 6.353907630856576, 7.664892913421985, 5.905035360960175, 4.321066304382384, 5.454549164953521, 7.487136297287175, 5.214240530104311, 4.200050491923914, 4.800876295931128, 3.8660799414320355, 5.261182328504775, 3.7654295503122177, 5.441861123290136, 5.651112180674322, 6.274835394016853, 5.28945498068728, 5.356461099182567, 5.1832124360021945, 6.665867026962602, 4.839347695362241, 4.919572087574838, 4.6206366192879305, 4.604594413140994, 4.984678484651757, 5.107017553153507, 5.686489608032262, 5.186002447477617, 4.7029937764974115, 6.303807780759701, 3.9430285221898838, 5.58503002718914, 6.18498041549213, 5.762318895721471, 5.686051595888033, 5.164742310256726, 5.479490034757343, 4.60408251234389, 5.379064196745426, 3.955130636735653, 6.114620893503636, 3.958982574562085, 3.838940291278325, 4.865896401991737, 7.062290581115448, 3.8008119492608645, 5.786056747809032, 3.74746781054968, 4.947768377377417, 5.67667560463442, 4.666177588305013, 5.334813027008264, 5.793369424289828, 4.4223341037659605, 5.330722476384998, 6.046541903653283, 4.197513571891748, 6.5968003456033655, 5.936296382786533, 5.170981254862737, 5.880836879058082, 4.096208966409434, 4.696039761279678, 4.652970837455283, 4.341410437282872, 5.258579423394452, 6.543783961505602, 5.099884384474556, 5.867561782235126, 4.424638904638933, 3.288712428222356, 5.059452664266722, 4.743613133176822, 4.434570194608235, 7.187580171723515, 3.471231530182341, 2.3686724515993025, 4.451990509861954, 5.240620487169285, 3.5222411354344105, 4.262005646710565, 5.764483440284738, 4.366434388777134, 4.544654910204675, 6.50567498172823, 4.557750874682837, 4.409615615112985, 5.2441318488689666, 4.550096218292411, 2.801886920218563, 5.00974937216955, 4.33700861055893, 4.488644138023066, 5.279866203663008, 4.525009197400127, 4.33089532384023, 5.995821397467381, 5.324686370964817, 5.857152175945536, 6.967777459049267, 6.706329641211171, 6.2579808766866964, 3.367196124043231, 4.752186044392098, 5.905041372943237, 4.623757033113152, 6.962174237591437, 5.391922240508943, 5.363421243592593, 4.63532822652848, 4.732178835003656, 6.166618800239452, 5.554262945916173, 5.787732891753274, 4.237139419347655, 4.305337043410611, 5.189681390640036, 4.555349944623174, 4.8727662881095295, 5.810353711986783, 4.688964563194054, 4.375800494352086, 4.584832850079451, 5.056132493256727, 3.938853459154085, 4.274378702255976, 5.921418762273382, 4.881950411456388, 4.746217319898652, 4.404092978125731, 3.224623480090594, 6.142760791698945, 6.10674645073701, 5.904647704527291, 6.556375537556776, 6.439555229509324, 5.067414047378016, 4.283845851825088, 6.703802567329461, 4.303873543090519, 6.402834934538062, 5.474713186940502, 5.248566051124064, 4.0775674282289405, 4.939483048591898, 4.638291829692265, 6.804293191051115, 4.918860393464179, 4.404598814093878, 4.691344343334003, 5.101307511848818, 5.66061948488715, 6.349660721726601, 4.742700283719975, 4.087363140683709, 2.70317368568934, 5.570654830830955, 4.9211169911141734, 5.835060040858386, 5.559141551232454, 4.9491463545467544, 5.005910660608396, 5.518166257261246, 4.391522418913977, 5.1673192431057045, 6.109181199181052, 4.08530560033833, 5.599002355304142, 4.745915489373403, 5.580638883138358, 5.080767424792324, 3.3889055446620335, 5.192864981501029, 5.871047988936063, 4.420798334956712, 5.704526635653845, 4.40551057329567, 5.68868091239222, 6.214698275642181, 5.491639083642935, 5.662377060672991, 5.076303314832115, 4.6879231927353855, 5.865231353401842, 5.635352716963373, 5.6603819948737595, 4.645907622701246, 5.703068184959901, 5.606409469121143, 5.35006571491482, 5.070573094465618, 5.022775912555119, 4.812150328463806, 6.0653513278246844, 5.100669618303849, 5.966054107360217, 5.640553456126811, 4.882034350080855, 6.0906605618087655, 5.635658258677284, 5.763931175758994, 5.1295760834957065, 5.643866865635124, 4.463595356358185, 4.762146724205129, 5.689140381483792, 5.643689876618276, 5.657135899915088, 5.09689029921085, 5.630351334023029, 4.483832227800276, 5.506559651096729, 5.052386323956622, 5.592271176795211, 4.6473303023765435, 5.631373347049518, 7.298080000200956, 3.494586881189134, 6.155857006230028, 5.248663624105395, 4.927322423036279, 6.031884461282644, 4.072045706267471, 5.432854015596811, 5.336305912813326, 5.691035743087922, 4.729040479752855, 5.144003774847542, 4.893172725037447, 4.850837419197311, 4.067924956739809, 5.357436860141771, 5.473700241978306, 4.885379869716033, 6.273080296810322, 5.617639568881801, 5.780279338259978, 4.516177687884646, 4.8392404912159295, 5.545366023647382, 6.025021874563144, 4.948578250656107, 5.156982120647383, 5.62497318974314, 6.277570128675382, 4.645792963953393, 4.703531983900468, 4.641046980791542, 5.190800369539718, 5.282296816584589, 4.507150043207611, 5.886459540660359, 5.332673701929155, 6.139120253116274, 5.564337393879438, 5.509122032768994, 6.111724592603875, 4.4387163156444895, 5.753968787060908, 4.806699736324585, 7.08799867042233, 5.571188252755882, 5.265302228773493, 6.08823374732821, 5.688739418706673, 5.039624719542464, 5.150241447087976, 5.234994764757508, 5.805524483325444, 5.5466495856619344, 5.571596375487135, 5.6457686536791885, 4.6846548904766365, 5.391762187040244, 4.988814880700932, 5.801538893565885, 5.0597762553799965, 5.8867796140020845, 4.726414172879279, 5.671094389102945, 5.617999727632223, 5.475483110051719, 5.069282489857713, 5.87111306059723, 4.781123508560691, 5.05147682900074, 4.641678506660446, 4.945440662445091, 5.7083534611631555, 5.9863294310925825, 4.577400973729236, 6.604951691116456, 6.102918448259324, 6.27169659505453, 4.097987659844832, 4.791932910706026, 4.113250666031916, 4.091794682238024, 4.8639562821092355, 4.678339762528495, 4.381259810280782, 5.955747655926387, 5.6580770222674435, 4.674522965625474, 5.220705733663694, 5.335702517976408, 6.238195034736511, 5.609715893938521, 4.72323294262587, 5.7530320718381756, 4.753568669768855, 5.1449872939276915, 5.354122570909834, 5.32365074599894, 5.291778393055308, 5.882198641563992, 5.8221979802375445, 4.109545177934299, 4.924330645176363, 6.065834648556958, 4.908852418420793, 6.613117909707456, 5.805451294765698, 4.0944033025623, 4.050453480680038, 5.560211666220153, 5.042606603739396, 6.2005351155033415, 4.783126259591142, 4.732021265334232, 5.205760223640613, 4.345497142361654, 4.610532546046786, 5.141688029888546, 5.468103808401066, 5.278902749374555, 6.09363873620213, 4.332090151997182, 6.170183436232143, 6.321093116510872, 5.191947004919269, 4.475181838919639, 5.053410647023033, 5.2400997742242845, 5.253770164162756, 4.403502601430028, 5.698483215352255, 5.518211262276038, 5.408284495113768, 5.048531015805539, 6.586748892876148, 5.1960968575696285, 5.800175413825111, 6.133649800420464, 5.108531838544709, 4.996604554593744, 4.317001794504348, 4.2528434145294325, 7.0056981009043, 4.902698544344471, 5.018486950309088, 5.358317497733772, 5.028496845859937, 6.3507532218812335, 5.073849456823711, 5.8868780263200895, 5.174874241347814, 5.573813331874844, 5.828890508183712, 5.280872574143104, 4.999548890432387, 5.484573149864807, 5.331465739012567, 5.259520448780412]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1700

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [5.668938602235695, 4.5648405270958206, 5.783994897105108, 5.06493956557433, 5.188281310309852, 4.635203270500523, 6.191723201474966, 5.502791389523651, 5.1832124360021945, 4.306155737356559, 6.703802567329461, 4.8639562821092355, 5.383262810753363, 5.346106766030754, 5.635658258677284, 5.329016305586674, 4.745192874359674, 4.443479631755579, 4.870272803872543, 6.294709173583546, 5.497622685783328, 6.618789419021743, 3.6986305219136373, 6.281802728313567, 3.7874973100608047, 5.0259791147615065, 2.737701062915051, 5.426907341576591, 4.9353372020035975, 5.453116857370316, 4.765341336617399, 5.7646627309872605, 5.381709193101656, 5.362768994549103, 4.9494169910636625]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (5.668938602235695, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.668938602235695, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.668938602235695, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.668938602235695, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.668938602235695, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.58554993 -0.66498461]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5855499296547606  |
| -0.664984605158736  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.192552650053611e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.46572549858439477

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.00219692242999e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.271235774667183e-08  |
| -5.986322366023296e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.5648405270958206, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5648405270958206, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5648405270958206, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.5648405270958206, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.5648405270958206, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.26142742 -0.04081967]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -1.2614274158551098   |
| -0.04081966542557325  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.1913682217885285e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.32069129999429286

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.715000131932892e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -4.6862030163255e-05     |
| -1.5164506244146112e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.783994897105108, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.783994897105108, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.783994897105108, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.783994897105108, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.783994897105108, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.77802048 -0.53376104]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7780204835938775  |
| -0.5337610364808398  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.169385018727973e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.43058842894627447

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.03818698527696e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 3.9198126747215605e-08  |
| -2.689187907245708e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (5.06493956557433, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.06493956557433, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.06493956557433, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.06493956557433, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.06493956557433, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.42484268 -0.74617338]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4248426821451545  |
| -0.7461733830815831  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0722568397808336e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.48888630766600194

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.193264206763957e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -9.317922482545643e-07  |
| -1.6365553731528066e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.188281310309852, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.188281310309852, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.188281310309852, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.188281310309852, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.188281310309852, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.21851191 -0.81254528]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.21851190967758782  |
| -0.8125452799490063   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.697489506130174e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5086734158087227

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1200682656222415e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.447482556903797e-07  |
| -9.101061824520222e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.635203270500523, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.635203270500523, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.635203270500523, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.635203270500523, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.635203270500523, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.14372194 -0.18236926]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.1437219371579488  |
| -0.1823692596936155  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 8.61941610661938e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3490082027295179

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.469688689036185e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.8246371316015404e-05  |
| -4.503952978932249e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (6.191723201474966, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.191723201474966, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.191723201474966, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.191723201474966, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.191723201474966, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.46008393 0.2295035 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4600839315370706  |
| 0.22950349620742827  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.8881720691662342e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.27283942095589425

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.920451826759543e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.0104440511227975e-08  |
| 1.5882678895763988e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.502791389523651, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.502791389523651, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.502791389523651, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.502791389523651, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.502791389523651, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.30761254 -0.78910623]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.30761253877464867  |
| -0.7891062314691055  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0605637214451497e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5015958132616422

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.114379134364782e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.504095334540946e-08  |
| -1.6684697506155026e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.1832124360021945, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.1832124360021945, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.1832124360021945, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.1832124360021945, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.1832124360021945, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.22699132 -0.81065648]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.2269913168539972  |
| -0.810656479721672   |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.849238836677054e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5080993970295015

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1511997201479522e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.613123404383367e-07  |
| -9.332275125917129e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (4.306155737356559, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.306155737356559, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.306155737356559, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.306155737356559, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.306155737356559, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.69416519  0.59867847]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -1.6941651947988134  |
| 0.5986784734623996  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.752785313629968e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.21880820601941983

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00017151026380139042

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.0002905667194830785  |
| 0.00010267950291574985  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.703802567329461, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.703802567329461, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.703802567329461, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.703802567329461, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.703802567329461, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.31670982 1.84715292]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 2.316709815097795  |
| 1.8471529239505458  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.951886481025032e-11

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.10373860346257743

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.70134959309805e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 1.5525082376731865e-09  |
| 1.2378417495305862e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.8639562821092355, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.8639562821092355, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.8639562821092355, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.8639562821092355, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.8639562821092355, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.76105518 -0.5468166 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.7610551788772568  |
| -0.5468166042188471  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.9082067892805237e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4339620798839301

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.701522838258976e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.100228662421207e-06  |
| -3.664503961511824e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (5.383262810753363, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.383262810753363, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.383262810753363, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.383262810753363, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.383262810753363, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.10766058 -0.83062358]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.10766057711997234  |
| -0.8306235765864756  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.0327669061018828e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5142004422647131

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.953257794079072e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.256100156145817e-08  |
| -3.2836691280863196e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.346106766030754, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.346106766030754, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.346106766030754, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.346106766030754, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.346106766030754, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.04550453 -0.83538365]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.0455045268132892  |
| -0.8353836522534408  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.4811653297818673e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.515665687326604

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.811577327638608e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.1894854951974544e-08  |
| -4.019513041062591e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.635658258677284, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.635658258677284, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.635658258677284, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.635658258677284, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.635658258677284, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.5298773  -0.69603399]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.5298773009254631  |
| -0.6960339882233768  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.0602718464390236e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4744505061149382

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.0665542098111168e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 5.651428659854047e-08  |
| -7.423579803112638e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (5.329016305586674, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.329016305586674, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.329016305586674, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.329016305586674, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.329016305586674, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.01691495 -0.83627593]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| 0.016914952771784897  |
| -0.8362759285063248   |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.718145170681276e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5159408117504884

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.268327507295129e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 8.911351097219236e-09  |
| -4.405775477838646e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.745192874359674, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.745192874359674, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.745192874359674, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.745192874359674, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.745192874359674, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.95972714 -0.37588106]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9597271377614902   |
| -0.37588105961106066  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.145612024047509e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.39180888437969214

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.3132964129167184e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -1.2604062074009944e-05  |
| -4.9364324727054115e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.443479631755579, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.443479631755579, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.443479631755579, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.443479631755579, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.443479631755579, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.46444455  0.23587962]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.4644445522904448  |
| 0.23587962472149115  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.0579931580676096e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.27180145575212106

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 7.571678203020624e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.00011088302896109856  |
| 1.7860046130403993e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (4.870272803872543, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.870272803872543, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.870272803872543, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.870272803872543, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.870272803872543, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.75048866 -0.55480248]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.7504886612164086  |
| -0.5548024795132278  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.8201594691336478e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.43603870222878977

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.467681549180257e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -4.853921667018358e-06  |
| -3.588285760187161e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (6.294709173583546, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.294709173583546, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.294709173583546, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.294709173583546, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.294709173583546, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.63236279 0.49588506]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.6323627893832793  |
| 0.49588506190190174  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.926724918002678e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23267534114745392

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.266341619635486e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.964217306690159e-09  |
| 2.1156150781476024e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.497622685783328, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.497622685783328, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.497622685783328, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.497622685783328, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.497622685783328, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.29896613 -0.79172859]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2989661340713212  |
| -0.7917285926861695  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.0911470912064195e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5023827380902963

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.1719438357977597e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 6.493376520084927e-08  |
| -1.719590036509561e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (6.618789419021743, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.618789419021743, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.618789419021743, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.618789419021743, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.618789419021743, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.17449658 1.52779847]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 2.174496578355445  |
| 1.5277984743988782  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.2247364892720177e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.12555954536821995

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 9.754228447389772e-10

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.1210536383346405e-09  |
| 1.4902495340860233e-09  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (3.6986305219136373, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.6986305219136373, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.6986305219136373, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.6986305219136373, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.6986305219136373, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.71045653  2.83686745]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.7104565303304184  |
| 2.8368674520606874  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00042693772348133

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.05741101870691396

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.007436511894360694

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.020156342226949772  |
| 0.021096398549974017  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (6.281802728313567, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.281802728313567, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.281802728313567, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.281802728313567, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.281802728313567, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [1.6107724  0.46087479]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.6107723976865884  |
| 0.46087478500211887  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.076596358808372e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.23759624196300624

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.5312011247046496e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 7.298733700040674e-09  |
| 2.0883163441496147e-09  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (3.7874973100608047, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7874973100608047, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7874973100608047, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.7874973100608047, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.7874973100608047, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.56179678  2.4449816 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -2.5617967835955824  |
| 2.4449815994032065  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00030612823656296433

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.07256627986884219

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.004218601768152741

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.010807200440924328  |
| 0.010314403698343282  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.0259791147615065, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.0259791147615065, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.0259791147615065, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.0259791147615065, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.0259791147615065, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.49001721 -0.71636061]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.4900172101418576  |
| -0.7163606130511369  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3051774306938908e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4802507143795965

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.7177001337311103e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.331719837533072e-06  |
| -1.9468533338887747e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (2.737701062915051, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.737701062915051, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.737701062915051, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (2.737701062915051, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [2.737701062915051, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-4.31793595  8.4858645 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -4.317935946573925  |
| 8.485864499974127  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.009405674626799851

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.00196077860801792

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.796908018242664

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                  â€”
| -20.712741564378693  |
| 40.70591146164667  |
 â€”                  â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.426907341576591, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.426907341576591, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.426907341576591, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.426907341576591, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.426907341576591, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.18067081 -0.820098  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.18067080942429925  |
| -0.8200980028405525  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6055967120953538e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5109752258324285

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.1422202700330143e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 5.6770747957630484e-08  |
| -2.5769285679391764e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.9353372020035975, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9353372020035975, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9353372020035975, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9353372020035975, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9353372020035975, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.64164645 -0.63056399]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                 â€”
| -0.6416464537473132  |
| -0.6305639854709  |
 â€”                 â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.0499089469034908e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.45624056134496294

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.493044066184105e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -2.882945791597439e-06   |
| -2.8331517732694273e-06  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (5.453116857370316, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.453116857370316, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.453116857370316, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.453116857370316, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.453116857370316, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.22451509 -0.81121546]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.2245150865221035  |
| -0.8112154592510024  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3922419865075207e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5082692068891994

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.7391822436550316e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 6.14987738434019e-08  |
| -2.2220669817588074e-07  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.765341336617399, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.765341336617399, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.765341336617399, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.765341336617399, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.765341336617399, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.92602202 -0.40766075]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -0.9260220212858883   |
| -0.40766074738129987  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.6754854528074576e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3993234148765781

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1708518155021152e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.0842345648175206e-05  |
| -4.773103261803441e-06  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (5.7646627309872605, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.7646627309872605, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.7646627309872605, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.7646627309872605, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.7646627309872605, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.7456809  -0.55839897]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7456808992500896  |
| -0.5583989703961123  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.4255880982528514e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.4369771643941375

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 5.550834908308983e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.139151566016631e-08  |
| -3.0995804976385346e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61750>

args = (5.381709193101656, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.381709193101656, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.381709193101656, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.381709193101656, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.381709193101656, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.10506162 -0.83090001]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.1050616238451596  |
| -0.8309000065764849  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.0498368368376432e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5142854181377079

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.985796144600716e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 4.187542152675278e-08  |
| -3.311798042761263e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.362768994549103, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.362768994549103, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.362768994549103, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.362768994549103, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.362768994549103, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.07337774 -0.83372683]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| 0.07337773832993832  |
| -0.833726834237325   |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.2693909086436316e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5151552129381927

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.4052566132450433e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.232477670429251e-08  |
| -3.6727806501638295e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb617e0>

args = (4.9494169910636625, array([5.31890473, 0.59778648])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.31890473 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890483 0.59778648]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9494169910636625, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9494169910636625, array([5.31890483, 0.59778648])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.31890473 0.59778658]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9494169910636625, array([5.31890473, 0.59778648]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9494169910636625, array([5.31890473, 0.59778658])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.61809325 -0.64539944]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.6180932488053514  |
| -0.6453994383370798  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9121107738113126e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.46030469501730803

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.154011015984564e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.567566164443118e-06  |
| -2.68099637656248e-06   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                      â€”
| 5.271235774667183e-08  |
| -5.986322366023296e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                        â€”
| -4.6862030163255e-05     |
| -1.5164506244146112e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                       â€”
| 3.9198126747215605e-08  |
| -2.689187907245708e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                       â€”
| -9.317922482545643e-07  |
| -1.6365553731528066e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                       â€”
| -2.447482556903797e-07  |
| -9.101061824520222e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                       â€”
| -2.8246371316015404e-05  |
| -4.503952978932249e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 1.0104440511227975e-08  |
| 1.5882678895763988e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                      â€”
| 6.504095334540946e-08  |
| -1.6684697506155026e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                       â€”
| -2.613123404383367e-07  |
| -9.332275125917129e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| -0.0002905667194830785  |
| 0.00010267950291574985  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                       â€”
| 1.5525082376731865e-09  |
| 1.2378417495305862e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                       â€”
| -5.100228662421207e-06  |
| -3.664503961511824e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                      â€”
| 4.256100156145817e-08  |
| -3.2836691280863196e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                       â€”
| 2.1894854951974544e-08  |
| -4.019513041062591e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                      â€”
| 5.651428659854047e-08  |
| -7.423579803112638e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                      â€”
| 8.911351097219236e-09  |
| -4.405775477838646e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                        â€”
| -1.2604062074009944e-05  |
| -4.9364324727054115e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                       â€”
| -0.00011088302896109856  |
| 1.7860046130403993e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                       â€”
| -4.853921667018358e-06  |
| -3.588285760187161e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                      â€”
| 6.964217306690159e-09  |
| 2.1156150781476024e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                      â€”
| 6.493376520084927e-08  |
| -1.719590036509561e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                       â€”
| 2.1210536383346405e-09  |
| 1.4902495340860233e-09  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                     â€”
| -0.020156342226949772  |
| 0.021096398549974017  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                      â€”
| 7.298733700040674e-09  |
| 2.0883163441496147e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                     â€”
| -0.010807200440924328  |
| 0.010314403698343282  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                       â€”
| -1.331719837533072e-06  |
| -1.9468533338887747e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                  â€”
| -20.712741564378693  |
| 40.70591146164667  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| 5.6770747957630484e-08  |
| -2.5769285679391764e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                        â€”
| -2.882945791597439e-06   |
| -2.8331517732694273e-06  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                     â€”
| 6.14987738434019e-08  |
| -2.2220669817588074e-07  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                       â€”
| -1.0842345648175206e-05  |
| -4.773103261803441e-06  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                      â€”
| 4.139151566016631e-08  |
| -3.0995804976385346e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                      â€”
| 4.187542152675278e-08  |
| -3.311798042761263e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                      â€”
| 3.232477670429251e-08  |
| -3.6727806501638295e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                       â€”
| -2.567566164443118e-06  |
| -2.68099637656248e-06   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-20.74422267  40.73740601]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [5.52634695 0.19041242]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.31890473 0.59778648]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [5.52634695 0.19041242]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 18              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Nouveau Sample selon la distribution q:
    â€”> params : [5.52634695 0.19041242]

[5.622707225175159, 5.230204153613175, 5.520044282574538, 5.5073021683645775, 5.685969880213872, 5.419103249279589, 5.288782754769209, 5.4729576951251175, 5.494282533763233, 5.252713960121123, 5.32230464425795, 5.3147284450839445, 5.663022606386477, 5.3848320329443595, 5.681943564133652, 5.619566146840507, 5.405804497788205, 5.552482123662689, 5.5074122755521, 4.987828570333703, 5.562652404988264, 6.011270185696632, 5.546915822105463, 5.799058163815428, 5.31356373746576, 5.742277484358023, 5.457315789658875, 5.884075540796308, 5.371364534326759, 5.062847190671592, 5.529822186605968, 5.567466038593696, 5.804237514806786, 5.5558325969988935, 5.645629605335047, 5.306084814824554, 5.738220793428211, 5.64802187626193, 5.659061521724451, 5.2989672613391186, 5.744947969980105, 5.674752920454454, 5.439345205171108, 5.524339998089824, 5.449595955464017, 5.79410854500433, 5.694128966756079, 5.583111848495794, 5.641071376328007, 5.319324318610144, 5.317822295752544, 5.65181492323524, 5.420627924316974, 5.7971862208685145, 5.381085423482853, 5.746349908544809, 5.472887281702463, 5.392995567770747, 5.485018482818689, 5.210023928088271, 5.34746461816734, 5.500061474023206, 5.354820067014132, 5.698571717922029, 5.583911581910307, 5.615929904054369, 5.77440589447915, 5.367853779827831, 5.240726222062141, 5.1636840981998935, 5.393225097495435, 5.440611504614147, 5.6250416770320815, 5.436431367426142, 5.453775366772732, 5.493993457781619, 5.389952337595526, 5.148570303990999, 5.942741652237132, 5.641836944990432, 5.750807859857186, 5.54319571885393, 5.922281508973615, 5.249760000334818, 5.718905169377162, 5.514004719718613, 5.433952804217221, 5.3750918748098195, 5.456445297825246, 5.204716579109263, 5.289474940117951, 5.854007074583721, 5.489086241875628, 5.308338838894494, 5.616696342852475, 5.709602707273146, 5.474508190349741, 5.4596484227613695, 5.233758217917958, 5.640717904673551]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X = [4.8771155294868835, 3.7035403343567204, 4.964929274694722, 3.0500257779373445, 4.649149768708871, 4.361892743154881, 3.236238053622853, 4.825133460880819, 4.868395354141642, 5.38886877752516, 4.820574508670093, 4.804489890904643, 4.716700787345068, 7.194080636994697, 6.4744507224852565, 4.611942257619824, 3.9040201984263465, 5.303860627982663, 5.185153986903305, 4.5206883397532, 4.707226273961055, 4.895052026654668, 5.188899709528759, 4.27759954835913, 6.737748945656159, 5.407187058155758, 5.010802007794817, 4.3798901053116595, 5.988452552619513, 3.625452586511261, 5.319509945892983, 5.55683746821213, 3.0182273749133195, 4.6949464273938535, 4.04546333767802, 4.458072467891961, 4.062894541813671, 4.414296444090939, 4.5648405270958206, 4.3713069102964575, 4.88410549606166, 3.935260854841958, 4.613803930278985, 5.286197399318152, 5.315952471562018, 7.241251340705833, 5.859518344253065, 4.015317840162659, 6.178810442773789, 4.883661872420566, 5.980251346533316, 3.951907692366885, 4.012118393769083, 3.24759527534737, 6.246103631662809, 3.8344716745102545, 5.768397223276881, 5.143890922660086, 4.111326882270596, 5.945935256321971, 4.3220588543640375, 4.3200161395055625, 4.879702242433275, 5.602419780339141, 6.0186986796766915, 5.032936787330783, 5.215515562044457, 3.6893413985247774, 4.2451914202734065, 6.154830813145515, 3.6742655068676946, 4.81550566103036, 4.302667469861069, 5.232282220706864, 5.468809469326474, 5.631262219431296, 3.053180560159051, 5.447724251764727, 3.41026649461324, 5.507857020890545, 6.256502992307158, 4.068395433782213, 4.870937027448608, 3.1863717469629935, 2.9116718709146503, 4.861567822983643, 4.998327010425135, 6.323190275767644, 5.627978841350949, 4.579025627430244, 4.758835667048313, 4.82264258451891, 6.027084279817071, 4.870272803872543, 4.9968217609175785, 4.7112042920115424, 4.926155833330847, 5.310447194589536, 5.195623623321056, 5.08660075475377, 3.9116393763485187, 7.374530526546719, 5.428266151627575, 5.548405292854267, 3.26216134770916, 6.696992759267852, 4.304118999872094, 5.8119416483858455, 5.450345079840404, 5.4722858473660745, 6.584243588482547, 6.837919540774682, 3.9904471584232315, 5.049053701962508, 6.1141522712873195, 3.3920732473675006, 5.683552641208301, 5.115389674854448, 4.693095522658777, 4.236747701955112, 4.3832286222934815, 5.078898677793509, 5.497622685783328, 5.352430620406992, 6.223400843230536, 5.288946216263123, 5.099009240601548, 4.112684880044097, 4.629296744322371, 5.506812997555188, 3.0832354831714737, 7.401460739083532, 4.119495359701023, 5.069264393369226, 4.98528795580584, 5.804466301525952, 5.586355966218047, 5.606199292300382, 4.989940814723232, 4.6519615102397385, 4.773804421930737, 4.695996151221829, 5.9193141612464135, 4.515629132401616, 4.611195390717706, 5.86539785051527, 4.635866190414418, 4.013069392924608, 3.5287015832287567, 3.9501101176362394, 3.6386479232645037, 6.069938050586868, 5.186306410333221, 4.736229525144517, 6.766802278820994, 5.143963586671972, 6.991232443315761, 6.934783121380934, 3.53316674864284, 4.394530396440322, 4.8821063563572915, 4.465113639664778, 4.913065034204201, 4.828453374610873, 4.95555015932942, 5.158153815235092, 5.55177578013804, 5.2710888266117655, 5.426907341576591, 5.346806331997987, 4.600664009685107, 3.7944390494735445, 5.491125018176283, 3.9887648799226767, 5.661115785804077, 4.2400444120983325, 4.958388622977857, 4.035612679030055, 4.415717692938217, 5.22949440618468, 4.53195970305815, 7.405879356715559, 4.794212663476959, 4.536066331415189, 6.539771633182656, 4.064795358316771, 3.677069888579797, 6.0425090445084475, 5.37414252802516, 3.9065392919311197, 6.42544810019025, 4.353253507685603, 6.114315044446465, 5.021887110192536, 5.0558244965209145, 4.3569966403813005, 4.709619903202896, 4.486912586006225, 5.03554097046952, 4.228092373494506, 3.1833630437423293, 5.039339884578826, 3.7045246848469064, 3.9614236173154596, 4.6538732335691355, 6.0928704256143, 6.868663826183948, 4.996623467634061, 5.649035187203196, 4.379323623302777, 5.638997507425143, 3.1694112049284824, 5.399136939360868, 4.948275708067148, 6.402353722870568, 4.580613659052775, 5.7873815726343665, 5.087907720151822, 4.963679938297505, 4.514892773948391, 3.7887077317883, 5.098950049252682, 5.959537281936, 4.298546418050926, 3.5845223427129325, 5.234388006882316, 5.014176919759004, 7.410651171142116, 4.449538349639722, 4.782687120837187, 5.485773774388848, 3.9974063377558973, 4.70725843733819, 3.8154026296329313, 5.772940924220628, 5.47447557000941, 5.755206684399332, 5.479846669723631, 5.5079948041310836, 6.129881283109167, 6.168235860322557, 4.765757417029921, 5.22191806826484, 7.055361979976823, 4.497423308210756, 5.708252543421104, 4.125401624249385, 5.362768994549103, 4.167114631122481, 3.748202528473394, 5.782131775120994, 4.306155737356559, 5.003541235735333, 4.499964249709488, 3.9458548757218463, 3.6629724972002125, 4.454494962965196, 4.378033726834976, 6.011633478419664, 6.551147266661519, 4.590410757914798, 5.177066413593627, 5.262123257167052, 5.304433881181889, 3.9665884909206266, 5.267264980547312, 5.135103319965891, 4.757913804231076, 4.536949354973703, 6.328013047797941, 5.657633204861243, 4.035319767998339, 4.662272999799967, 5.971841707313747, 3.4768378803889717, 5.197374169448914, 5.511395240211079, 5.1065318129664385, 4.906320893269945, 5.0977386255091695, 3.674507657822029, 4.741199888999902, 4.2893002364320685, 5.076919952714801, 5.678817747329083, 6.926584061561935, 5.825006151740532, 4.530333967265844, 5.9869670759644285, 6.271859718716548, 4.880117339106297, 5.499094833641162, 4.093561787638524, 5.205855532588175, 6.14711352949421, 5.822092679004669, 3.3414842060083703, 2.932310514318679, 5.456232480847415, 6.598570317774973, 4.731262008177704, 3.846893827083139, 5.643354099459681, 4.784860694567975, 5.500366044448021, 4.905024099030723, 4.99361706657747, 3.787735755199361, 4.062658856269076, 4.941645846213782, 5.809400694679507, 4.380788354624879, 6.877149062445431, 4.443479631755579, 5.527730665018761, 4.383481509023446, 4.770160694819487, 5.903454622985064, 4.7007530321677224, 1.9935797366433166, 5.407374007814408, 6.112267701173042, 4.033024737129325, 2.7920118172842194, 4.002564336103206, 3.9396288656299276, 5.731481560043303, 5.1916247724180185, 5.148124799783533, 5.7652235553793725, 4.586700520536034, 4.901625130182659, 5.105401209919368, 5.049803194580772, 7.636426262795658, 7.66292892888184, 5.3362828086861995, 5.577036531533784, 5.2106314983131234, 5.402332750008211, 4.6904120189505525, 4.789511181850781, 5.217167449051036, 5.203379054351345, 5.640372014180977, 3.2136246945061395, 2.784718036601556, 7.080740775141724, 5.147057473169987, 3.9796857745812337, 5.934944323484632, 3.828332454959588, 5.543861664243698, 5.4741995747482814, 3.379193638784313, 4.458852548946068, 5.0081661366724015, 5.905524370638401, 4.205189815239311, 4.903301032798097, 5.477037398289465, 3.569847757823836, 5.637663953330408, 5.145453940904799, 5.1552073596116506, 6.352534184519216, 5.8064945924276605, 5.740953845116167, 3.4723592737079136, 3.9271192815166494, 4.298520730879047, 4.578242041451908, 5.642685255174476, 5.7646627309872605, 6.4970380596769335, 5.8717283373945675, 4.325591863846316, 3.417989168144456, 5.015151297280011, 2.931616402500867, 5.017597776501736, 5.017198826219964, 5.344093242395677, 5.854025341444138, 4.875774926518186, 4.021463590542871, 5.365148388265451, 3.849921136787044, 5.4325658622178485, 4.158940389859275, 5.204336062374974, 6.197331007915066, 5.652356047020777, 4.581933398444201, 6.032366311400969, 5.99799053118142, 5.545460014797378, 3.967421844437092, 3.7778839884891147, 4.210013662823948, 4.622296354773813, 4.505686099511901, 6.86315644484575, 4.233617611040928, 5.431872307175255, 5.651528666631276, 6.487728176663111, 5.076260842331797, 3.509387105833827, 6.934959410588143, 4.505249482187357, 6.510671006216665, 4.242708927436795, 6.281802728313567, 4.364652291999458, 4.184849798656908, 6.293629282018586, 5.72081756683573, 5.4731679931799855, 4.338418726764614, 2.190464992016754, 3.97634819707438, 6.10179038899805, 6.135959274378711, 4.827846043748334, 5.068160544417442, 5.214690338255851, 5.549917468523984, 6.491611225535168, 6.423688878462031, 6.396153095610848, 4.730867502247377, 4.816208061544749, 3.5481086814143286, 4.143778226942451, 3.9867445211820147, 5.837426140988665, 3.5229656039838515, 5.998626512156872, 4.85701345057956, 4.812856458542559, 4.257960496206052, 5.11569859154836, 2.737701062915051, 4.8082350479465115, 4.322057445540274, 5.0767833359361285, 3.899847746811868, 3.6296440357229653, 4.731871769988107, 6.067302259045171, 3.762701661104035, 4.789703644660185, 5.64886843926889, 4.013566328065052, 5.157837638063537, 4.42141675625603, 4.398934573638421, 5.383262810753363, 5.60662451125058, 4.2898270375680445, 6.237838483577062, 5.005974997688087, 6.700196142106886, 4.3859933966975175, 3.9124194995643924, 6.490140411593667, 4.75737187606253, 4.1777980316709495, 4.292401199282439, 7.648813685024926, 4.746987622976991, 5.046460597885442, 4.380862488192241, 4.6324562902621675, 5.547957074408476, 4.543324019776738, 4.670392956969452, 5.346106766030754, 5.656456978387318, 5.150372920068067, 5.1727262744056235, 3.975931513770335, 3.745350691591175, 4.615970925573878, 6.224735765340912, 4.061982886607598, 3.695477784606563, 5.426038598807297, 4.663359949758123, 5.6577165787939965, 5.178917987160397, 4.204576791023203, 6.565624088278566, 5.312327690403269, 4.345702639157525, 6.442058704688819, 5.077373356563462, 5.765727096705151, 3.9269158606086196, 4.746358885990728, 4.479009875440771, 6.248769371351054, 4.7405467798580005, 5.961895300422139, 6.0725640441180975, 6.189216951619629, 6.400651394067637, 5.419524728917479, 5.800613093214671, 6.267293257538643, 5.170620548673374, 5.276536813019522, 4.151023298764732, 4.857322360163403, 3.3104116950608886, 5.205770919120722, 5.063810893872629, 6.6959620473838335, 6.115364591580022, 5.189500835817977, 4.54283903123057, 4.210917534229328, 5.446508284371847, 3.4456366356424297, 4.888817497310663, 3.5608956806900705, 6.78329021653083, 7.62926466152618, 6.699545252108056, 5.914746417621921, 4.868513869969408, 6.188170057382675, 5.247808176707644, 2.7221024564318363, 4.768590287076733, 4.818517562928384, 4.835497711250678, 6.710834231586855, 5.854834685558841, 4.507534235827013, 6.297949577946734, 5.680442034038884, 4.574531404334611, 4.997714188363295, 6.004112738056949, 6.568285124126496, 3.232110632302119, 3.484533745664581, 6.203374790419099, 6.416668405673112, 5.519559049000368, 5.453116857370316, 6.294709173583546, 6.488978577529594, 4.067630145015588, 5.756269816909863, 4.484150493605244, 4.2036299513485105, 4.073503622470542, 5.6286381872351585, 6.610296423566688, 3.5453903236144364, 5.978095641838392, 5.1529366260096445, 3.9778280306171716, 3.2630334611788037, 3.986711251241448, 5.561744791862315, 6.182162023290769, 3.9642028572820385, 4.809169187716645, 4.106993251266785, 4.5173219458210845, 5.312176130509372, 5.0053447581300405, 6.697726951435652, 4.116758700053005, 4.592769374151632, 5.776381406378199, 5.5125308743688874, 4.689859057043255, 6.208502531304828, 3.8358173416924677, 4.692982558398637, 4.275766313228071, 5.067893487853502, 4.978823171240284, 4.029214745185353, 5.006792385634919, 6.604675659370592, 3.624302813965327, 4.330613617238767, 4.9353372020035975, 4.059651123910417, 5.094035289960832, 4.0302530162482695, 5.138789787930809, 4.321316299613069, 4.244530376578208, 4.726700856694704, 3.812411132294809, 5.2986142577406, 3.2367535026550094, 4.896075558611478, 4.864281656659538, 4.988062133291249, 4.294221005085495, 3.486520698041963, 3.513821223018519, 6.995218174422393, 5.459385056453483, 4.884017788369837, 5.728523544218265, 5.391373833190526, 5.381252112463946, 4.7350131234713535, 4.162310833550109, 7.217150518778553, 5.094773808282984, 4.422276143327911, 3.7931851831315924, 3.083540128825657, 5.739548322096601, 5.634771311039012, 4.398735026901141, 4.641194673730232, 5.245857196193984, 4.655306173206099, 5.487396231419734, 4.317734324641174, 4.590610828723053, 4.559373384026196, 4.016764105980929, 5.962140693066046, 4.797842753811596, 5.3523839007846234, 5.016017101069609, 5.040617579892614, 4.890786605095597, 5.1924352375113845, 5.6522484943069085, 4.452584530712797, 5.827663471239049, 5.966834716727361, 5.56950382986559, 6.423126226420643, 5.095509624227406, 4.280416359907861, 5.035522335466402, 4.301724704636541, 4.666506308948859, 4.950642276890436, 6.892024854761788, 4.363793788056196, 5.352245128503238, 4.854628661410772, 4.337513083756973, 3.559627330323787, 5.2542163722121185, 1.7492531185147764, 3.178338978589365, 5.220106222190492, 4.68381661881293, 6.4933640757203985, 4.39828628871325, 4.21016423505906, 6.006003343642173, 3.7861766153084004, 5.04196878331191, 6.029592819339718, 6.447462202583771, 5.194400724917502, 4.938388711079094, 5.333153445960159, 6.013585291449488, 6.25083153012201, 5.506210578799851, 5.123238945683366, 5.233959511313875, 5.188645867474613, 4.599076841235233, 4.753047268891739, 3.2668934059812558, 7.419574453902745, 6.003718203455669, 4.622945444386177, 5.197624648330743, 5.321598446618486, 6.247730499896567, 5.4655905808497804, 3.5806936355275245, 4.509213114986299, 4.670754365397438, 5.4503039111802565, 3.7489606133168345, 4.946615323661609, 3.953721076237072, 6.471603447647581, 5.151394524401118, 3.094383033833427, 5.42795546724073, 6.351573758550151, 4.3470648205868025, 4.403223383325739, 5.7340549665814295, 6.113167112855745, 4.8589074038153175, 5.4617919929339624, 7.179909794362477, 5.3373694994337075, 4.734874894609524, 4.692903784896739, 3.736603126728275, 6.630061281848648, 5.808534384806025, 5.721829412707804, 4.404066951844861, 5.244490844085984, 5.002918527311009, 4.450585687671854, 6.82761891114238, 6.6871855227239525, 5.0002147010086215, 6.2019252600974974, 4.1304076634663796, 4.716908346931455, 6.349792602133754, 6.191723201474966, 5.162846636458869, 5.602541683537984, 5.677168255085978, 6.37469851742308, 3.5047445905543126, 5.475500764033918, 3.280483177391562, 3.9881289743983537, 3.94709156894294, 2.993564926363172, 5.713683059535223, 6.441081902609404, 5.06493956557433, 5.348733548761557, 5.329016305586674, 7.359315527529149, 5.291270377976451, 4.75056016027407, 3.912305651986968, 4.652076192477964, 5.458683063260757, 6.284719622689429, 2.568370455397019, 4.88031270952816, 3.822626950139934, 4.086224897372392, 5.0259791147615065, 6.160360878331058, 4.531559555129132, 6.520212083961808, 5.1643849088285005, 5.030496799601422, 4.636266207589298, 5.488556620568304, 4.850079610641103, 5.700508237432333, 5.993265327501362, 5.068697523783213, 5.001086272873839, 4.879404363385205, 5.473844088585802, 5.615807021617604, 4.358111109916723, 5.8034242588187075, 4.765341336617399, 4.408724718927348, 5.526941880340413, 6.618789419021743, 5.598476214580674, 5.400248915533575, 4.623699792591299, 4.08753257337432, 4.521677738182754, 6.25654567217764, 6.455416712756836, 6.404236036541849, 3.473539917018407, 3.9986783840516713, 4.289657184311555, 4.488209060437271, 5.118041246653154, 4.175799296344497, 5.402274786830667, 4.5365608187111865, 5.660006359268667, 5.792441235858916, 4.170167490880203, 4.117335203045571, 5.3611003517059554, 4.886882280512638, 4.527238591329277, 5.752802221204449, 4.818347791548248, 5.7830991323871395, 5.148628447198713, 5.346486703615111, 3.31686409817914, 6.707454781877117, 4.018619189842549, 5.013468469826582, 5.596499740404186, 5.854048154110186, 4.892623296956499, 5.984553295918194, 5.68324474562505, 5.31184469134301, 5.942992956628917, 4.384296078267645, 5.0069835749024305, 5.4378096181052555, 3.9478604316501142, 5.534006243387487, 6.186747693851749, 6.280686685633288, 5.162548499354572, 5.3529073630831165, 5.212718026012116, 4.766620534979891, 5.677223392846591, 6.762463323331578, 4.21266472459489, 5.50551284508972, 5.831714536806496, 5.092241730347649, 5.229495417953454, 5.140947344498025, 5.4477899213658105, 4.203022499802645, 4.106479277730971, 5.0852948618807625, 4.004381959850602, 4.627178804348573, 4.301130524969195, 4.529557031484509, 6.057950607407279, 6.465839032553544, 5.638650936706819, 4.2341594333803885, 6.425294193660756, 5.37981426518856, 4.35649767007461, 4.005608216006155, 4.561685140597053, 6.0054100878349805, 4.441661953683263, 4.7147762470090955, 4.624304665082726, 5.559367426124027, 3.9549964228576555, 3.9766894498911034, 5.276126236160097, 5.600665803053102, 7.139772183954881, 5.4189903524538945, 5.341174560288266, 3.987109903381666, 5.546836497399638, 5.2474334870725645, 5.383159876165887, 6.67992626029425, 6.415240685209189, 4.458918699941811, 4.8903325592490265, 4.098611693350446, 4.5804843268719955, 3.215182408746725, 6.081347526038332, 5.942898709571372, 4.547388083994764, 5.313634285275245, 3.6952995056278057, 6.155513533162929, 4.700344100494152, 5.417373682275772, 5.253683487888834, 4.372026447850805, 3.1323925616990453, 5.439613349382669, 5.361952384984476, 4.427602586491956, 5.338771676268255, 5.498539805989414, 5.251668924367313, 4.550502294347198, 5.368905575803122, 6.349949785356218, 2.655685876334847, 5.559781198234466, 6.089679967754371, 5.467701109319354, 4.000856358463849, 5.694100770439801, 5.270886521509751, 6.567017664519408, 4.331487921456281, 3.2566050881687696, 3.999035312700106, 5.719221426304523, 5.291946081840716, 4.906538074218057, 5.127142048521152, 5.026308114551279, 4.676748312626479, 4.686417519870014, 4.493910725637266, 4.369454779539452, 5.324825629511109, 4.37447637806716, 5.348700705782182, 4.532178681764685, 4.575994198303899, 6.255041312711398, 5.730053893663229, 5.640430077423015, 4.133950063840396, 5.801311877201001, 3.292324956596424, 4.096201360738143, 5.49648375998225, 4.668270342892965, 5.41454797409735, 6.571984877889251, 4.159245818840289, 4.805740620677395, 4.6622497962803156, 5.663889191612584, 5.284741335717816, 5.455415614140546, 3.1972071164229714, 5.257643334221295, 6.7376503948399415, 4.7167205232325085, 5.5087742477174855, 4.369230605680347, 5.279581939509647, 5.67813760925621, 5.936705942284281, 6.748502320899499, 3.851660943471591, 3.9781063612354477, 4.086233897379177, 4.409746683631044, 4.438163063519343, 4.684915648405287, 5.869158131059629, 5.950265838791173, 4.8261444169117516, 5.229789732257212, 2.49267367238389, 5.2595508140984215, 5.135017454159776, 4.376619005816564, 4.701311112650521, 5.830168658572052, 2.9416579117026975, 6.256798337277775, 5.469229679426799, 4.673879827273575, 5.693888365991601, 3.2688572423796654, 5.837729023802759, 6.571746260902078, 1.3333132222019077, 4.375494262708558, 5.256292968577299, 5.639166425107547, 5.09684887566826, 4.985479667856032, 3.7874973100608047, 5.316477554855172, 5.182302910957265, 4.267395987048677, 6.1402600456388665, 4.2867093975576545, 5.834628726160632, 4.336675754897178, 2.7240941411303115, 4.764526728742769, 4.818996622222219, 5.536335885482347, 5.3482816744115835, 4.872723657480988, 4.493178971547482, 6.6674052586514625, 3.275365241384062, 6.668070874061068, 2.314243094601577, 5.655863629430067, 5.59654302219537, 3.937353380994406, 6.563894779906245, 5.225789440386992, 2.590797475935505, 4.009528631789021, 6.471879846266331, 6.8478902461450755, 5.054010693017719, 5.965579597057303, 3.5706906099409914, 5.965077503783912, 6.008443280928307, 2.7068789472704697, 4.023920130219107, 6.210986605519358, 3.8920899323303426, 5.074311144632604, 5.379843353234118, 5.674967920641618, 4.7281802917410385, 4.470162250985136, 4.825147768962656, 4.620990160076387, 4.105661441494449, 3.0064876723525646, 5.087751544653446, 6.623191596698924, 4.445583036041847, 3.5050029501380164, 6.7073216523287265, 4.2102159775373105, 5.959345151758053, 5.053377989150199, 4.7883848416429, 5.655503067881433, 5.493756812871286, 3.6946973912038064, 5.183656352756389, 4.830184789503146, 4.572761669158455, 4.608806413464056, 6.386441713770333, 3.093116993078171, 3.5765857924993614, 4.932940679684439, 6.3269860629845, 5.94425815480839, 6.062707085052321, 4.233542366661202, 4.680451175180369, 3.874129227058206, 3.781348384217722, 6.200769945435217, 3.696628445590303, 5.237514243848003, 4.98582664005507, 5.529773961779093, 5.90558429677398, 2.658990167105342, 3.9933611239483406, 4.979376339681217, 5.859794939353348, 5.587894222072076, 4.914002854188922, 7.002993602065502, 4.542255486946038, 4.48706244864086, 3.985697298478447, 4.814494854689769, 4.7886583860487635, 4.000419023915461, 5.553126357142923, 5.302777543678368, 4.8594588994096695, 5.018158035019807, 4.052531038391542, 6.097377226341522, 5.377605975639743, 6.016644857457, 6.380368174813183, 4.008572911888616, 5.668938602235695, 4.246610283633679, 4.4627585227624955, 4.447620464920277, 5.601862044850695, 3.4528548565640267, 6.427940779193832, 5.225470178913018, 5.907883223784398, 5.152177175936106, 4.087840775085313, 4.015940597779346, 5.235454736220161, 4.3041800481536105, 6.260338245928221, 4.149807934904956, 5.332826186388074, 4.620373335736485, 4.488038709557738, 3.366565801627048, 5.337196298694287, 4.490416274328351, 3.5564524988044712, 4.091629946863637, 4.879829892905114, 4.1926418390225075, 4.210216335775425, 4.187504980422656, 4.530710047380715, 4.009372181787566, 3.9432288559544406, 4.411784013963423, 5.289816410463862, 3.4893621991347263, 4.70812136860741, 4.748987698855443, 3.7989359743023168, 4.515802235408018, 6.849494774956813, 3.8533509627750253, 5.262683037408525, 5.174250961590373, 4.811616579786701, 5.4144120224117165, 5.534109611204314, 4.757576389142094, 5.526458646858388, 4.68648626135325, 6.510397251598002, 4.053569161498328, 4.923389693719144, 5.171489048917507, 4.38273385252844, 3.5798745643728505, 5.776623075584453, 5.2610244417351675, 4.290574225520197, 4.856593546566716, 4.745192874359674, 3.6986305219136373, 4.379386401302874, 5.416012422669541, 4.659283968868628, 5.063523399259809, 5.7467416662796, 4.506312504147869, 4.476540161369129, 4.887133675349967, 7.101314492424933, 4.975301110000693, 4.862850942851848, 7.180855356630156, 2.9847168625895826, 6.5661988659086585, 4.130266496729279, 4.635203270500523, 5.2680387343232065, 5.599672587970324, 4.400983390586819, 2.518320319314273, 4.116474903067948, 4.804012214397902, 6.84000545726421, 5.297420062264238, 5.198714774949839, 6.736278646645381, 2.7653837751939716, 3.7074394406887117, 4.321519160329914, 4.295293892672745, 6.226190166839307, 4.668913001705613, 6.094720511993962, 5.253733943180695, 5.065868524410168, 5.120953283929446, 4.379225050802219, 5.923121760179944, 4.267138358789313, 4.759398905325478, 5.266646686893115, 6.473242173540947, 5.087655203318958, 3.965053905052038, 4.353966609391555, 5.343060515208491, 5.783994897105108, 5.01033882071759, 7.4186928993875325, 4.410049001959045, 7.374056817127453, 4.509450562163232, 5.746852509456756, 3.887434446403901, 4.02234765799981, 4.249898068401506, 4.9494169910636625, 4.938595751187503, 3.9514719173927, 4.478787355638231, 6.566513640431479, 4.525491827767309, 4.960166360955473, 6.62730134189487, 5.43112841504713, 4.9471173548619625, 5.43463957307096, 5.245356487899543, 6.597668615285253, 4.871918938790327, 3.6960369330631475, 3.951051628276191, 5.436916857828227, 4.683112858710698, 3.389905575467676, 6.773437205377399, 4.9731129801028136, 5.713770100318756, 5.78071325942541, 3.6774005993445718, 4.707968235738519, 5.894022120518272, 6.0115826438236235, 4.855906884524213, 5.036320434323684, 4.621962251875588, 3.8085309752160392, 6.632916483405368, 6.551570420264441, 5.328097831866994, 4.723646058902793, 4.684965447556247, 5.114917614624182, 6.0797356432271865, 6.334257343429244, 5.846049293573029, 5.846581982171187, 3.1574490721836947, 4.459369571447204, 4.773824744492791, 4.186583225587993, 5.495703629611403, 5.8122130284460605, 3.5691417657236753, 4.26265547659675, 4.523434629227358, 3.405932251233569, 5.626621309337312, 6.824971295892024, 5.9038830058265, 6.122729257147154, 5.230008982894638, 4.196378447740866, 6.177331674922204, 5.517840725857978, 5.210828974954717, 4.789102216128037, 4.781349903460918, 5.339287534498736, 4.9245071078156295, 6.701207863258989, 4.439469034941763, 3.9240069642493474, 3.571461598287253, 3.099771624686494, 5.999122977423209, 4.478828210834569, 4.961603054597413, 5.73423339159917, 4.750660014139897, 6.938201923584461, 5.502791389523651, 4.181805872180821, 6.488535024376849, 5.4999230250685995, 4.9746832551558136, 3.679784846830486, 6.046822572598391, 5.708897408940855, 7.061608116868901, 3.961717066019777, 5.603384170844793, 7.380843307721037, 6.781190249117925, 4.717267839259414, 3.9193847974155704, 4.790649456158062, 6.371011706777922, 4.689633998710858, 4.804061300502477, 4.894834442420745, 4.263084366604324, 5.144180615113666, 4.6066336477791845, 5.381709193101656, 3.5755635239163714, 4.492620532918825, 5.577956818303537, 6.8208208608907395, 5.82923381759672, 4.620459934894899, 6.9904670323299865, 6.164042199831909, 6.630221735394573, 3.8222843450965946, 6.125515905657428, 5.107895262736628, 6.803408080581033, 5.773340456981987, 6.008548120608629, 5.862181357017108, 7.043883173632293, 5.543745390710634, 3.869386773297068, 5.196806275202913, 4.413733794800476, 3.2790332151915615, 5.08138520994593, 4.336847519534556, 5.33269272348815, 5.188281310309852, 3.626726904604922, 6.085726993257404, 5.551210898534239, 4.614813888109768, 4.840177833878102, 5.5778413375269755, 6.115050488749432, 6.353907630856576, 7.664892913421985, 5.905035360960175, 4.321066304382384, 5.454549164953521, 7.487136297287175, 5.214240530104311, 4.200050491923914, 4.800876295931128, 3.8660799414320355, 5.261182328504775, 3.7654295503122177, 5.441861123290136, 5.651112180674322, 6.274835394016853, 5.28945498068728, 5.356461099182567, 5.1832124360021945, 6.665867026962602, 4.839347695362241, 4.919572087574838, 4.6206366192879305, 4.604594413140994, 4.984678484651757, 5.107017553153507, 5.686489608032262, 5.186002447477617, 4.7029937764974115, 6.303807780759701, 3.9430285221898838, 5.58503002718914, 6.18498041549213, 5.762318895721471, 5.686051595888033, 5.164742310256726, 5.479490034757343, 4.60408251234389, 5.379064196745426, 3.955130636735653, 6.114620893503636, 3.958982574562085, 3.838940291278325, 4.865896401991737, 7.062290581115448, 3.8008119492608645, 5.786056747809032, 3.74746781054968, 4.947768377377417, 5.67667560463442, 4.666177588305013, 5.334813027008264, 5.793369424289828, 4.4223341037659605, 5.330722476384998, 6.046541903653283, 4.197513571891748, 6.5968003456033655, 5.936296382786533, 5.170981254862737, 5.880836879058082, 4.096208966409434, 4.696039761279678, 4.652970837455283, 4.341410437282872, 5.258579423394452, 6.543783961505602, 5.099884384474556, 5.867561782235126, 4.424638904638933, 3.288712428222356, 5.059452664266722, 4.743613133176822, 4.434570194608235, 7.187580171723515, 3.471231530182341, 2.3686724515993025, 4.451990509861954, 5.240620487169285, 3.5222411354344105, 4.262005646710565, 5.764483440284738, 4.366434388777134, 4.544654910204675, 6.50567498172823, 4.557750874682837, 4.409615615112985, 5.2441318488689666, 4.550096218292411, 2.801886920218563, 5.00974937216955, 4.33700861055893, 4.488644138023066, 5.279866203663008, 4.525009197400127, 4.33089532384023, 5.995821397467381, 5.324686370964817, 5.857152175945536, 6.967777459049267, 6.706329641211171, 6.2579808766866964, 3.367196124043231, 4.752186044392098, 5.905041372943237, 4.623757033113152, 6.962174237591437, 5.391922240508943, 5.363421243592593, 4.63532822652848, 4.732178835003656, 6.166618800239452, 5.554262945916173, 5.787732891753274, 4.237139419347655, 4.305337043410611, 5.189681390640036, 4.555349944623174, 4.8727662881095295, 5.810353711986783, 4.688964563194054, 4.375800494352086, 4.584832850079451, 5.056132493256727, 3.938853459154085, 4.274378702255976, 5.921418762273382, 4.881950411456388, 4.746217319898652, 4.404092978125731, 3.224623480090594, 6.142760791698945, 6.10674645073701, 5.904647704527291, 6.556375537556776, 6.439555229509324, 5.067414047378016, 4.283845851825088, 6.703802567329461, 4.303873543090519, 6.402834934538062, 5.474713186940502, 5.248566051124064, 4.0775674282289405, 4.939483048591898, 4.638291829692265, 6.804293191051115, 4.918860393464179, 4.404598814093878, 4.691344343334003, 5.101307511848818, 5.66061948488715, 6.349660721726601, 4.742700283719975, 4.087363140683709, 2.70317368568934, 5.570654830830955, 4.9211169911141734, 5.835060040858386, 5.559141551232454, 4.9491463545467544, 5.005910660608396, 5.518166257261246, 4.391522418913977, 5.1673192431057045, 6.109181199181052, 4.08530560033833, 5.599002355304142, 4.745915489373403, 5.580638883138358, 5.080767424792324, 3.3889055446620335, 5.192864981501029, 5.871047988936063, 4.420798334956712, 5.704526635653845, 4.40551057329567, 5.68868091239222, 6.214698275642181, 5.491639083642935, 5.662377060672991, 5.076303314832115, 4.6879231927353855, 5.865231353401842, 5.635352716963373, 5.6603819948737595, 4.645907622701246, 5.703068184959901, 5.606409469121143, 5.35006571491482, 5.070573094465618, 5.022775912555119, 4.812150328463806, 6.0653513278246844, 5.100669618303849, 5.966054107360217, 5.640553456126811, 4.882034350080855, 6.0906605618087655, 5.635658258677284, 5.763931175758994, 5.1295760834957065, 5.643866865635124, 4.463595356358185, 4.762146724205129, 5.689140381483792, 5.643689876618276, 5.657135899915088, 5.09689029921085, 5.630351334023029, 4.483832227800276, 5.506559651096729, 5.052386323956622, 5.592271176795211, 4.6473303023765435, 5.631373347049518, 7.298080000200956, 3.494586881189134, 6.155857006230028, 5.248663624105395, 4.927322423036279, 6.031884461282644, 4.072045706267471, 5.432854015596811, 5.336305912813326, 5.691035743087922, 4.729040479752855, 5.144003774847542, 4.893172725037447, 4.850837419197311, 4.067924956739809, 5.357436860141771, 5.473700241978306, 4.885379869716033, 6.273080296810322, 5.617639568881801, 5.780279338259978, 4.516177687884646, 4.8392404912159295, 5.545366023647382, 6.025021874563144, 4.948578250656107, 5.156982120647383, 5.62497318974314, 6.277570128675382, 4.645792963953393, 4.703531983900468, 4.641046980791542, 5.190800369539718, 5.282296816584589, 4.507150043207611, 5.886459540660359, 5.332673701929155, 6.139120253116274, 5.564337393879438, 5.509122032768994, 6.111724592603875, 4.4387163156444895, 5.753968787060908, 4.806699736324585, 7.08799867042233, 5.571188252755882, 5.265302228773493, 6.08823374732821, 5.688739418706673, 5.039624719542464, 5.150241447087976, 5.234994764757508, 5.805524483325444, 5.5466495856619344, 5.571596375487135, 5.6457686536791885, 4.6846548904766365, 5.391762187040244, 4.988814880700932, 5.801538893565885, 5.0597762553799965, 5.8867796140020845, 4.726414172879279, 5.671094389102945, 5.617999727632223, 5.475483110051719, 5.069282489857713, 5.87111306059723, 4.781123508560691, 5.05147682900074, 4.641678506660446, 4.945440662445091, 5.7083534611631555, 5.9863294310925825, 4.577400973729236, 6.604951691116456, 6.102918448259324, 6.27169659505453, 4.097987659844832, 4.791932910706026, 4.113250666031916, 4.091794682238024, 4.8639562821092355, 4.678339762528495, 4.381259810280782, 5.955747655926387, 5.6580770222674435, 4.674522965625474, 5.220705733663694, 5.335702517976408, 6.238195034736511, 5.609715893938521, 4.72323294262587, 5.7530320718381756, 4.753568669768855, 5.1449872939276915, 5.354122570909834, 5.32365074599894, 5.291778393055308, 5.882198641563992, 5.8221979802375445, 4.109545177934299, 4.924330645176363, 6.065834648556958, 4.908852418420793, 6.613117909707456, 5.805451294765698, 4.0944033025623, 4.050453480680038, 5.560211666220153, 5.042606603739396, 6.2005351155033415, 4.783126259591142, 4.732021265334232, 5.205760223640613, 4.345497142361654, 4.610532546046786, 5.141688029888546, 5.468103808401066, 5.278902749374555, 6.09363873620213, 4.332090151997182, 6.170183436232143, 6.321093116510872, 5.191947004919269, 4.475181838919639, 5.053410647023033, 5.2400997742242845, 5.253770164162756, 4.403502601430028, 5.698483215352255, 5.518211262276038, 5.408284495113768, 5.048531015805539, 6.586748892876148, 5.1960968575696285, 5.800175413825111, 6.133649800420464, 5.108531838544709, 4.996604554593744, 4.317001794504348, 4.2528434145294325, 7.0056981009043, 4.902698544344471, 5.018486950309088, 5.358317497733772, 5.028496845859937, 6.3507532218812335, 5.073849456823711, 5.8868780263200895, 5.174874241347814, 5.573813331874844, 5.828890508183712, 5.280872574143104, 4.999548890432387, 5.484573149864807, 5.331465739012567, 5.259520448780412, 5.622707225175159, 5.230204153613175, 5.520044282574538, 5.5073021683645775, 5.685969880213872, 5.419103249279589, 5.288782754769209, 5.4729576951251175, 5.494282533763233, 5.252713960121123, 5.32230464425795, 5.3147284450839445, 5.663022606386477, 5.3848320329443595, 5.681943564133652, 5.619566146840507, 5.405804497788205, 5.552482123662689, 5.5074122755521, 4.987828570333703, 5.562652404988264, 6.011270185696632, 5.546915822105463, 5.799058163815428, 5.31356373746576, 5.742277484358023, 5.457315789658875, 5.884075540796308, 5.371364534326759, 5.062847190671592, 5.529822186605968, 5.567466038593696, 5.804237514806786, 5.5558325969988935, 5.645629605335047, 5.306084814824554, 5.738220793428211, 5.64802187626193, 5.659061521724451, 5.2989672613391186, 5.744947969980105, 5.674752920454454, 5.439345205171108, 5.524339998089824, 5.449595955464017, 5.79410854500433, 5.694128966756079, 5.583111848495794, 5.641071376328007, 5.319324318610144, 5.317822295752544, 5.65181492323524, 5.420627924316974, 5.7971862208685145, 5.381085423482853, 5.746349908544809, 5.472887281702463, 5.392995567770747, 5.485018482818689, 5.210023928088271, 5.34746461816734, 5.500061474023206, 5.354820067014132, 5.698571717922029, 5.583911581910307, 5.615929904054369, 5.77440589447915, 5.367853779827831, 5.240726222062141, 5.1636840981998935, 5.393225097495435, 5.440611504614147, 5.6250416770320815, 5.436431367426142, 5.453775366772732, 5.493993457781619, 5.389952337595526, 5.148570303990999, 5.942741652237132, 5.641836944990432, 5.750807859857186, 5.54319571885393, 5.922281508973615, 5.249760000334818, 5.718905169377162, 5.514004719718613, 5.433952804217221, 5.3750918748098195, 5.456445297825246, 5.204716579109263, 5.289474940117951, 5.854007074583721, 5.489086241875628, 5.308338838894494, 5.616696342852475, 5.709602707273146, 5.474508190349741, 5.4596484227613695, 5.233758217917958, 5.640717904673551]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

len(X) = 1800

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  

X_sampled_from_uniform = [4.509450562163232, 3.9240069642493474, 5.923121760179944, 5.458683063260757, 4.3041800481536105, 4.237139419347655, 6.2005351155033415, 4.476540161369129, 4.695996151221829, 5.291778393055308, 5.426907341576591, 5.718905169377162, 3.236238053622853, 4.394530396440322, 4.016764105980929, 1.7492531185147764, 3.9478604316501142, 3.748202528473394, 4.9245071078156295, 5.143890922660086, 4.009372181787566, 4.505686099511901, 4.683112858710698, 6.223400843230536, 4.262005646710565, 4.7007530321677224, 5.713770100318756, 5.600665803053102, 3.787735755199361, 3.1694112049284824, 5.095509624227406, 5.674967920641618, 4.478787355638231, 5.801311877201001, 5.9863294310925825]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
calcul de L_list_divided_by_ğ›¾

[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.509450562163232, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.509450562163232, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.509450562163232, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.509450562163232, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.509450562163232, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-5.34049441 11.63455278]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -5.3404944067025895  |
| 11.634552783412744  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5317559322414884e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.06050546072639541

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0002531599485157316

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.0013519992890493801  |
| 0.002945402783652332  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.9240069642493474, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9240069642493474, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9240069642493474, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9240069642493474, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9240069642493474, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-8.41510273 32.78107751]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -8.415102730197077  |
| 32.781077505816825  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0001808479633914019

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0010791467105468976

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.16758422337195514

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                  â€”
| -1.4102384556352965  |
| 5.493591415108181  |
 â€”                  â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.923121760179944, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.923121760179944, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.923121760179944, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.923121760179944, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.923121760179944, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 2.08376516 -0.45484047]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.0837651637517496  |
| -0.45484047084265455  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.608437423817615e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.604683636981169

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.5890023867334844e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 3.31110781859362e-08  |
| -7.227425937519599e-09  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.458683063260757, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.458683063260757, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.458683063260757, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.458683063260757, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.458683063260757, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.35535466 -2.56274016]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.3553546566725796  |
| -2.5627401584182863  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.3505970111661872e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.903319119358374

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.4951493688360254e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.313082906369499e-08  |
| -3.8316793303498365e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.3041800481536105, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3041800481536105, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3041800481536105, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.3041800481536105, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.3041800481536105, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-6.41852555 17.972844  ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -6.418525551055154  |
| 17.97284400417709  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.7848414171754775e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.018098763896082535

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0020912154216204256

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.013422519616431278  |
| 0.03758508855191334  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.237139419347655, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.237139419347655, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.237139419347655, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.237139419347655, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.237139419347655, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-6.77060673 20.29466533]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -6.770606733041973  |
| 20.294665334930073  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 5.039524348190742e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.011631765967634383

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.004332553081117105

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.029334013062273213  |
| 0.08792771482709179  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.2005351155033415, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.2005351155033415, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.2005351155033415, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.2005351155033415, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.2005351155033415, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [3.54067308 3.64230204]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 3.540673081392498  |
| 3.642302037754064  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7878422475989015e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.2771498109596717

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.450815324059708e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.284022817093243e-08   |
| 2.3495817799997815e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.476540161369129, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.476540161369129, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.476540161369129, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.476540161369129, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.476540161369129, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-5.51333188 12.5725262 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -5.513331879924976  |
| 12.572526197907052  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.7758532712014046e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.050609117279072934

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00035089591889320046

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.0019346056561694509  |
| 0.004411648133023431  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.695996151221829, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.695996151221829, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.695996151221829, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.695996151221829, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.695996151221829, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-4.36080204  6.88241255]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -4.360802043912315  |
| 6.882412546538319  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.490776042055121e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.14954548586751168

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.3403356539999873e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.0001892734459122864  |
| 0.0002987198056127711  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.291778393055308, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.291778393055308, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.291778393055308, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.291778393055308, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.291778393055308, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-1.23189766 -1.8670933 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -1.2318976599923204  |
| -1.8670933027431502  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.312482264385184e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.7912516210080976

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.186382910868355e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -5.157195311730565e-07  |
| -7.81636749560068e-07   |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.426907341576591, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.426907341576591, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.426907341576591, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.426907341576591, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.426907341576591, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-0.52223307 -2.48951502]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -0.5222330695664468  |
| -2.4895150169124136  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.6055967120953538e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.8908115549750968

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.8023977160245067e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -9.41271691819031e-08  |
| -4.4870961804916454e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.718905169377162, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.718905169377162, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.718905169377162, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.718905169377162, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.718905169377162, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 1.01126897 -2.11454604]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.0112689705077393  |
| -2.1145460371574387  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.154418103159851e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.8294261862155364

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.80313300397793e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.845990397636767e-08  |
| -8.041899822344198e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (3.236238053622853, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.236238053622853, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.236238053622853, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.236238053622853, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.236238053622853, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-12.02709889  69.69963364]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -12.027098890143861  |
| 69.69963363800957  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.002121659097799586

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 9.552050589400824e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2221.1556334865186

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -26714.058454342492  |
| 154813.73390701142  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.394530396440322, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.394530396440322, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.394530396440322, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.394530396440322, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.394530396440322, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-5.94402735 15.03984114]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -5.944027350324177  |
| 15.039841136932353  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.554964244279498e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.03163689800281307

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0008075899995164876

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.004800337044974292  |
| 0.01214602529650325  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.016764105980929, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.016764105980929, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.016764105980929, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.016764105980929, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.016764105980929, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-7.92796463 28.80041436]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -7.927964631448958  |
| 28.80041435560088  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00012513273866697442

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0023028298300681632

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.05433868236076763

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.43079515187570516  |
| 1.5649765675274883  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (1.7492531185147764, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (1.7492531185147764, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [1.7492531185147764, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (1.7492531185147764, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [1.7492531185147764, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-19.83638441 194.1150866 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -19.836384410609753  |
| 194.1150866002772  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.08639013659045894

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 4.915273377202928e-17

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1757585590073931.2

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -3.486414339925487e+16  |
| 3.411738790246005e+17  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.9478604316501142, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9478604316501142, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9478604316501142, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.9478604316501142, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.9478604316501142, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-8.28983008 31.7347425 ]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -8.28983008460682   |
| 31.734742504951896  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00016464160669330457

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.001317065195252368

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.12500642131216363

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -1.0362819921626092  |
| 3.9670465918070437  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (3.748202528473394, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.748202528473394, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.748202528473394, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.748202528473394, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.748202528473394, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-9.33838516 40.97681453]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -9.338385158486062  |
| 40.97681452819302  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0003549798145841364

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.00022663822310898082

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.566283964437197

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                 â€”
| -14.626562927475032  |
| 64.1813275092259  |
 â€”                 â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.9245071078156295, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9245071078156295, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9245071078156295, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.9245071078156295, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.9245071078156295, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-3.16071771  2.36918625]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -3.1607177120029917  |
| 2.3691862471864056  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 2.162331196372717e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.3531791285719837

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.122477296763584e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -1.9351422433216857e-05  |
| 1.4505289010203286e-05  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.143890922660086, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143890922660086, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143890922660086, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.143890922660086, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.143890922660086, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.00856692 -0.60870956]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                    â€”
| -2.0085669222291713  |
| -0.6087095555384892  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 7.166018536735166e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.6226621066920982

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.1508679361917086e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -2.3115952684888183e-06  |
| -7.005443099227533e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (4.009372181787566, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.009372181787566, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.009372181787566, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.009372181787566, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.009372181787566, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-7.96678523 29.10893605]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -7.9667852315168375  |
| 29.108936052679724  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.00012890031822464804

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.002171443897027847

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.05936156969152172

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                   â€”
| -0.4729208767380727  |
| 1.7279521361371966  |
 â€”                   â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.505686099511901, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.505686099511901, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.505686099511901, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.505686099511901, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.505686099511901, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-5.36026446 11.74032998]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -5.3602644589645365  |
| 11.740329979303965  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5579694177722562e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.059298992644457336

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0002627311777644302

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.001408308594432569  |
| 0.0030845507228055794  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.683112858710698, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.683112858710698, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.683112858710698, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.683112858710698, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.683112858710698, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-4.42846198  7.17975291]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -4.4284619793444335  |
| 7.179752907759962  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.895017167217166e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.1413138458157153

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.879222646172143e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| -0.00021607451977329671  |
| 0.00035031612981462697  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (6.223400843230536, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.223400843230536, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.223400843230536, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (6.223400843230536, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [6.223400843230536, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [3.66075836 4.07469479]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| 3.660758360890526  |
| 4.074694788247513  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.5511060792876206e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.25524534063298476

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.076922209200848e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.224614378581333e-08   |
| 2.4761603254416255e-08  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.262005646710565, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.262005646710565, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.262005646710565, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.262005646710565, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.262005646710565, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-6.64001531 19.41900976]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -6.640015310921399  |
| 19.419009760213157  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.5341646478718016e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.013742257606845557

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.00329943214396822

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                    â€”
| -0.021908279953295196  |
| 0.06407170500687988  |
 â€”                    â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (4.7007530321677224, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.7007530321677224, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.7007530321677224, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.7007530321677224, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.7007530321677224, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-4.33582005  6.77378315]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -4.335820054102157  |
| 6.773783149949253  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.347318816028682e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.15267095682426293

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.1575155799507946e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| -0.00018026239426792816  |
| 0.0002816210898112219  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.713770100318756, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.713770100318756, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.713770100318756, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.713770100318756, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.713770100318756, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.98430083 -2.14145443]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.9843008291765543  |
| -2.1414544343345554  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 3.24838461455268e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.8336868193747918

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 3.896408746138923e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| 3.83523835963532e-08  |
| -8.343981787399142e-08  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (5.600665803053102, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.600665803053102, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.600665803053102, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.600665803053102, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.600665803053102, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.39030439 -2.54970968]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.3903043878372703  |
| -2.5497096768634098  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.159610173396866e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.901080612394671

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6.835803688004523e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                       â€”
| 2.6680441738223597e-08  |
| -1.7429314812443718e-07  |
 â€”                       â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.787735755199361, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.787735755199361, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.787735755199361, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.787735755199361, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.787735755199361, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-9.13076621 39.05954264]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                â€”
| -9.130766205345253  |
| 39.059542640274  |
 â€”                â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0003058518851010263

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.0003264992258931582

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.9367614402893305

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                  â€”
| -8.553349701464365  |
| 36.58947342074559  |
 â€”                  â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (3.1694112049284824, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.1694112049284824, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.1694112049284824, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (3.1694112049284824, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [3.1694112049284824, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-12.37805733  73.98222934]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                  â€”
| -12.378057334672121  |
| 73.98222933829857  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 0.0026280261544907044

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 4.226169519130278e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 6218.458920293234

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                  â€”
| -76972.44104869294  |
| 460055.4539719225  |
 â€”                  â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.095509624227406, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.095509624227406, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.095509624227406, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.095509624227406, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.095509624227406, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-2.26265381 -0.06607931]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                     â€”
| -2.262653810980453    |
| -0.06607931313240556  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 9.180190467001952e-07

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5615385444410564

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.6348281979716494e-06

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                        â€”
| -3.6990502524388586e-06  |
| -1.0802832441145493e-07  |
 â€”                        â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.674967920641618, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.674967920641618, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.674967920641618, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.674967920641618, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.674967920641618, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 0.78052116 -2.32127182]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 0.7805211565958814  |
| -2.3212718161524926  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 4.051599577667177e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.862726131071575

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 4.696275482736063e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.665542371478033e-08  |
| -1.0901331918963165e-07  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61870>

args = (4.478787355638231, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.478787355638231, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.478787355638231, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (4.478787355638231, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [4.478787355638231, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [-5.50153016 12.50752907]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| -5.501530160323398  |
| 12.507529065786116  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.758073914888457e-05

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.05123936120580734

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 0.0003431100375797037

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                     â€”
| -0.0018876302200544345  |
| 0.004291458767791111  |
 â€”                     â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61990>

args = (5.801311877201001, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.801311877201001, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.801311877201001, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.801311877201001, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.801311877201001, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [ 1.44404909 -1.58323978]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 1.4440490880840429  |
| -1.583239784830326  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 1.9623298425278926e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.7496202607995373

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 2.6177652141297407e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.780181470282181e-08  |
| -4.1445500343550825e-08  |
 â€”                      â€”


[INFO] â€”â€” gradient.py â€”â€” gradient_selon  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” DEBUT GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
Params :

arg_num = 2
f = <function SGD_L.<locals>.h.<locals>.<lambda> at 0x12cb61480>

args = (5.9863294310925825, array([5.52634695, 0.19041242])) âˆˆ [<class 'numpy.float64'>, <class 'numpy.ndarray'>]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
index = 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
argument_differencie = [5.52634695 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  p = 2
[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
--- dÃ©but de calcul de gradient composante par composante ---

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 0

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634705 0.19041242]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9863294310925825, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9863294310925825, array([5.52634705, 0.19041242])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
pour la composante : 1

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
theta_plus_h = [5.52634695 0.19041252]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
args = (5.9863294310925825, array([5.52634695, 0.19041242]))

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  
new_args = [5.9863294310925825, array([5.52634695, 0.19041252])]

[DEBUG] â€”â€” gradient.py â€”â€” gradient_selon  

[â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” RÃ‰SULTAT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”]

gradient = [2.41571658 0.29196389]

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” FIN GRADIENT DESCENT â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” h  
h(x,Î¸) = 
 â€”                   â€”
| 2.4157165789961965  |
| 0.29196388862118283  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
f(x) = 6.594640508367233e-09

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
q(x, theta) = 0.5245309896421683

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” Ï‰  
Ï‰(x,Î¸) = 1.2572451654126395e-08

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” L  
L_i(Î¸) = 

 â€”                      â€”
| 3.037147989950128e-08  |
| 3.6707018744405644e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_list_divided_by_ğ›¾ = 


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_1(Î¸) = 
 â€”                     â€”
| -0.0013519992890493801  |
| 0.002945402783652332  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_2(Î¸) = 
 â€”                  â€”
| -1.4102384556352965  |
| 5.493591415108181  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_3(Î¸) = 
 â€”                     â€”
| 3.31110781859362e-08  |
| -7.227425937519599e-09  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_4(Î¸) = 
 â€”                       â€”
| -5.313082906369499e-08  |
| -3.8316793303498365e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_5(Î¸) = 
 â€”                    â€”
| -0.013422519616431278  |
| 0.03758508855191334  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_6(Î¸) = 
 â€”                    â€”
| -0.029334013062273213  |
| 0.08792771482709179  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_7(Î¸) = 
 â€”                       â€”
| 2.284022817093243e-08   |
| 2.3495817799997815e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_8(Î¸) = 
 â€”                     â€”
| -0.0019346056561694509  |
| 0.004411648133023431  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_9(Î¸) = 
 â€”                      â€”
| -0.0001892734459122864  |
| 0.0002987198056127711  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_10(Î¸) = 
 â€”                       â€”
| -5.157195311730565e-07  |
| -7.81636749560068e-07   |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_11(Î¸) = 
 â€”                      â€”
| -9.41271691819031e-08  |
| -4.4870961804916454e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_12(Î¸) = 
 â€”                      â€”
| 3.845990397636767e-08  |
| -8.041899822344198e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_13(Î¸) = 
 â€”                   â€”
| -26714.058454342492  |
| 154813.73390701142  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_14(Î¸) = 
 â€”                    â€”
| -0.004800337044974292  |
| 0.01214602529650325  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_15(Î¸) = 
 â€”                   â€”
| -0.43079515187570516  |
| 1.5649765675274883  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_16(Î¸) = 
 â€”                      â€”
| -3.486414339925487e+16  |
| 3.411738790246005e+17  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_17(Î¸) = 
 â€”                   â€”
| -1.0362819921626092  |
| 3.9670465918070437  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_18(Î¸) = 
 â€”                 â€”
| -14.626562927475032  |
| 64.1813275092259  |
 â€”                 â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_19(Î¸) = 
 â€”                       â€”
| -1.9351422433216857e-05  |
| 1.4505289010203286e-05  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_20(Î¸) = 
 â€”                       â€”
| -2.3115952684888183e-06  |
| -7.005443099227533e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_21(Î¸) = 
 â€”                   â€”
| -0.4729208767380727  |
| 1.7279521361371966  |
 â€”                   â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_22(Î¸) = 
 â€”                      â€”
| -0.001408308594432569  |
| 0.0030845507228055794  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_23(Î¸) = 
 â€”                       â€”
| -0.00021607451977329671  |
| 0.00035031612981462697  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_24(Î¸) = 
 â€”                       â€”
| 2.224614378581333e-08   |
| 2.4761603254416255e-08  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_25(Î¸) = 
 â€”                    â€”
| -0.021908279953295196  |
| 0.06407170500687988  |
 â€”                    â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_26(Î¸) = 
 â€”                      â€”
| -0.00018026239426792816  |
| 0.0002816210898112219  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_27(Î¸) = 
 â€”                     â€”
| 3.83523835963532e-08  |
| -8.343981787399142e-08  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_28(Î¸) = 
 â€”                       â€”
| 2.6680441738223597e-08  |
| -1.7429314812443718e-07  |
 â€”                       â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_29(Î¸) = 
 â€”                  â€”
| -8.553349701464365  |
| 36.58947342074559  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_30(Î¸) = 
 â€”                  â€”
| -76972.44104869294  |
| 460055.4539719225  |
 â€”                  â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_31(Î¸) = 
 â€”                        â€”
| -3.6990502524388586e-06  |
| -1.0802832441145493e-07  |
 â€”                        â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_32(Î¸) = 
 â€”                      â€”
| 3.665542371478033e-08  |
| -1.0901331918963165e-07  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_33(Î¸) = 
 â€”                     â€”
| -0.0018876302200544345  |
| 0.004291458767791111  |
 â€”                     â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_34(Î¸) = 
 â€”                      â€”
| 3.780181470282181e-08  |
| -4.1445500343550825e-08  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
L_35(Î¸) = 
 â€”                      â€”
| 3.037147989950128e-08  |
| 3.6707018744405644e-09  |
 â€”                      â€”


[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
un_sur_ğ›¾_Î£_gradL_i_Î¸t = [-3.48641434e+16  3.41173879e+17]

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î¸_t+1 = [ 3.48641434e+14 -3.41173879e+15]

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
old params : [5.52634695 0.19041242]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” distribution_family.py â€”â€” update_parameters  
new params : [ 3.48641434e+14 -3.41173879e+15]
type : <class 'numpy.ndarray'>

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
Î·_t+1 = 0.01

[DEBUG] â€”â€” stochastic_gradient_descent.py â€”â€” SGD_L  
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
             ITERATION NÂ° 19              
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

