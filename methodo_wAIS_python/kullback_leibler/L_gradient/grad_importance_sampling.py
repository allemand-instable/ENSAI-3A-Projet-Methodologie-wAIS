import numpy as np
# typing
from typing import Callable, Any, Dict, List, Optional, Tuple, Literal
from numpy.typing import ArrayLike, NDArray
from custom_typing.custom_types import ImportanceSamplingGradientEstimation
# My Modules
from gradient.gradient import gradient_selon
from distribution_family.distribution_family import DistributionFamily
# Debug
from utils.log import logstr
from logging import info, debug, warn, error


def compute_grad_L_estimator_importance_sampling(
                                f_target : DistributionFamily, 
                                q_t : DistributionFamily, 
                                q_importance_sampling : DistributionFamily,
                                Î¸_t : NDArray, 
                                # nb_stochastic_choice : int,
                                max_L_gradient_norm : int | float, 
                                X_sampled_from_uniform : List[float],
                                param_composante : Optional[int] = None,
                             ) -> NDArray:
    """calcul de l'estimateur de ğ›L(Î¸) obtenu par la loi des grands nombres et la mÃ©thode d'Importance Sampling
    
    Ï‰ = f / q_importance_sampling
    
    on a donc ğ›Ì‚L = 1/nâ‹…âˆ‘ [ğ›_Î¸]( Ï‰ Ã— log(q_Î¸) )[X_i]
                 = 1/nâ‹…âˆ‘  Ï‰[X_i] Ã— [ğ›_Î¸]log(q_Î¸)[X_i]"""
    nb_stochastic_choice = len(X_sampled_from_uniform)             
    
    def Ï‰(x,Î¸) -> float:
        f_val = f_target.density(x)
        q_val = q_importance_sampling.density_fcn(x, Î¸)
        res = f_val/q_val
        # debug(logstr(f"Ï‰(x,Î¸) = {res}"))
        return res
    # âŸ¶ scalaire

    def h(x,Î¸) -> NDArray:
        # x âŸ¼ log qâ‚œ(x)
        def log_q(u, theta) -> float :
            return np.log(q_t.density_fcn(u, theta)) 
        # [ğ›_Î¸]log qâ‚œ(x)
        if param_composante is None:
            res = gradient_selon(2, log_q, *[x, Î¸] )
        else :
            res = gradient_selon(2, log_q, *[x, Î¸], composante=param_composante)
        # debug(logstr(f"h(x,Î¸) = {get_vector_str(res)}"))
        return res
    # âŸ¶ vecteur
    
    def grad_L(x_i, Î¸) -> NDArray:
        res = h(x_i, Î¸) * Ï‰(x_i, Î¸) #@ #res = h(x_i, Î¸) * Ï‰(x_i, Î¸_0 )            
        norm_res = np.linalg.norm(res)
        norm_theta = np.linalg.norm(np.array(Î¸))
        # avec les Ï‰, si on a un Ï‰ ~ 10 000 lorsque q << f 
        # on va avoir la norme de la direction qui explose
        # on essaye d'Ã©viter cela
        if norm_res > max_L_gradient_norm * norm_theta :
            debug(logstr(f"{norm_res} = || res || > {max_L_gradient_norm} x || Î¸ || = {max_L_gradient_norm*norm_theta}\n\nreturning zeros..."))
            # norm_max * ğ›L/â€–ğ›Lâ€–
            return max_L_gradient_norm * (res/norm_res)
        return res
    # âŸ¶ vecteur

    grad_L_list : list[NDArray] = [ grad_L(x_i = X_sampled_from_uniform[i], Î¸ = Î¸_t) for i in range(nb_stochastic_choice) ]
    
    grad_L_estimator : NDArray = np.add.reduce( grad_L_list )/nb_stochastic_choice
    
    return grad_L_estimator
