from math_tools.gradient import gradient_selon
import numpy as np
from typing import Callable, Any, Optional
from random import randint
import numpy.random as nprd
from math_tools.distribution_family import DistributionFamily
from numpy.typing import ArrayLike

from utils.log import logstr
from logging import info, debug, warn, error
from utils.print_array_as_vector import get_vector_str

import plotly.express as plx
from plotly.subplots import make_subplots
import plotly.graph_objects as plgo

from copy import deepcopy

def xor(a : bool,b : bool) -> bool:
    return (a and not b) or (not a and b)


def SGA_L( f : DistributionFamily ,q : DistributionFamily , N : int, ğ›¾ : int, Î·_0 : float, Î¸_0 : Optional[ArrayLike] = None, É› : float = 1e-6, iter_limit = 100, benchmark : bool = False) -> ArrayLike:
    """effectue une stochastic gradient ascent pour le problÃ¨me d'optimisation de Î¸ suivant le critÃ¨re de la vraissemblance de Kullback-Leibler
        
    f           â€” target density
                    â¤ va Ãªtre utilisÃ©e pour la comparaison avec q dans la maximisation de la vraissemblance de Kullback-Leibler
                    
                    L(Î¸) = - KL( f || q )
    
    q           â€” original sampling policy : q(ğ‘¥, Î¸)
    
                                    parametric family of sampling policies / distributions
                                    given as a (lambda) function of ğ‘¥, Î¸ âˆˆ ğ˜Ÿ Ã— Î˜

                                    q = lambda x,Î¸ : np.exp( - (x-Î¸[0])**2 / (2*Î¸[1]) )/(np.sqrt(2*np.pi*Î¸[1]))
                                    gives a normal law density

                â¤ va Ãªtre modifiÃ©e Ã  chaque itÃ©ration
    
    
    N           â€” Nombre de samples tirÃ©s par la distribution q Ã  chaque itÃ©ration
    
    ğ›¾           â€” nombre d'observations Ã  tirer alÃ©atoirement
    
    Î·_0         â€” initialisation du pas
    
    Î¸_0         â€” initialisation des paramÃ¨tres
    
    É›           â€” threshold pour la norme du gradient
    
    iter_limit  - nombre d'itÃ©rations max du gradient descent avant l'arrÃªt
    
    """
    
    
    info(logstr(f"\n=========    BEGIN : SGD_L   ========="))
    
    # initialisation
    Î·_t = Î·_0
    if Î¸_0 is None :
        Î¸_0 = q.parameters_list()
        Î¸_t = q.parameters_list()
    else :
        Î¸_t = Î¸_0
        q.update_parameters(Î¸_0)
    # on s'assure de commencer la premiÃ¨re itÃ©ration
    norm_grad_L = (É› + 1)
    
    X = []
    
    debug(logstr(
        f"\nÎ·_t = {Î·_t}\nÎ¸_t = {Î¸_t}\nğ›¾ = {ğ›¾}\nN = {N}\n"
                ))
    
    #! importance sampling selon q(Î¸_0)
    q_0 = q.copy()
    
    counter = 1
    
    if benchmark is True :
        benchmark_graph : list[list[float]] = [ list([]) for k in range( len(Î¸_t) + 1)]

    
    while norm_grad_L > É›:
        
        if counter > iter_limit :
            break
        
        debug(logstr(f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n             ITERATION NÂ° {counter}              \nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”"))
        
        # on rajoute N observations samplÃ©es depuis la sampling policy q_t
        #? un seul grand Ã©chantillon
        #! importance sampling selon q(Î¸_0)
        #!               |
        #!               v
        new_sample = q.sample(N)
        #?          |
        #? debug(logstr(f"Nouveau Sample selon la distribution q:\n    â€”> params : {q_0.parameters}\n\n{new_sample}"))
        #?          |
        #? un seul grand Ã©chantillon
        X = X +  new_sample
        
        debug(logstr(f"\nX = {X}"))
        debug(logstr(f"\nlen(X) = {len(X)}"))
        
        # on dÃ©termine les observations alÃ©atoires tirÃ©es :
        
        obs_tirÃ©es = nprd.choice(range(len(X)), ğ›¾, replace=False)
        X_sampled_from_uniform = [  X[i] for i in obs_tirÃ©es  ]
        #                                             b inclu
        debug(logstr(f"\nX_sampled_from_uniform = {X_sampled_from_uniform}"))
        
        # on update la valeur de L_i(Î¸)
        
        
        #Ï‰ : Callable[[Any, Any], float]     = lambda x, Î¸ : f(x)/q.density_fcn(x, Î¸)
        def Ï‰(x,Î¸) -> float:
            
            debug(logstr(f"Î¸ = {Î¸}"))
            
            f_val = f.density(x)
            q_val = q.density_fcn(x, Î¸)
            
            debug(logstr(f"f(x) = {f_val}"))
            debug(logstr(f"q(x, theta) = {q_val}"))
            
            res = f_val/q_val
            debug(logstr(f"Ï‰(x,Î¸) = {res}"))
            return res
        # âŸ¶ scalaire
        
        
        #h : Callable[[Any, Any], Any]       = lambda x, Î¸ : gradient_selon(2, lambda u, v : np.log(q.density_fcn(u, v)), *[x, Î¸] )
        def h(x,Î¸):
            res = gradient_selon(2, lambda u, v : np.log(q.density_fcn(u, v)), *[x, Î¸] )
            debug(logstr(f"h(x,Î¸) = {get_vector_str(res)}"))
            return res
        # âŸ¶ vecteur
        
        
        #L : Callable[[Any, Any], float]     = lambda x_i, Î¸ : h(x_i, Î¸) * Ï‰(x_i, Î¸)
        def grad_L(x_i, Î¸):
            debug("calcul de L :")
            #! importance sampling selon q(Î¸_0)
            #!                        |
            #!                        v
            res = h(x_i, Î¸) * Ï‰(x_i, Î¸) #@ #res = h(x_i, Î¸) * Ï‰(x_i, Î¸_0 )
            debug(logstr(f"âˆ‡L_i(Î¸) = \n{get_vector_str(res)}"))
            
            norm_res = np.linalg.norm(res)
            norm_theta = np.linalg.norm(np.array(Î¸))
            alpha = 10
            
            # avec les Ï‰, si on a un Ï‰ ~ 10 000 lorsque q << f 
            # on va avoir la norme de la direction qui explose
            # on essaye d'Ã©viter cela
            
            if norm_res > alpha * norm_theta :
                debug(logstr(f"{norm_res} = || res || > {alpha} x || Î¸ || = {alpha*norm_theta}\n\nreturning zeros..."))
                return np.zeros(Î¸.shape)
            return res
        
        
        # on update la valeur du gradient de L selon la mÃ©thode de la SGD
        debug(logstr("calcul de L_list_divided_by_ğ›¾"))
        grad_L_list = [ grad_L(x_i = X_sampled_from_uniform[i], Î¸ = Î¸_t) for i in range(ğ›¾) ]
        debug(logstr(f"âˆ‡L_list_divided_by_ğ›¾ = \n"))
        
        
        
        #?  â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”  ?#
        #?                                  DEBUG                                       ?#
        #?                   afficher chaque composante âˆ‡L_i/ğ›¾
        #for k in range(len(grad_L_list)):
        #    debug(logstr(f"âˆ‡L_{k+1}(Î¸) = {get_vector_str(grad_L_list[k])}"))
        #?  â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”  ?#
        
        
        
        un_sur_ğ›¾_Î£_gradL_i_Î¸t = np.add.reduce( grad_L_list )/ğ›¾
        debug(logstr(f"un_sur_ğ›¾_Î£_gradL_i_Î¸t = {un_sur_ğ›¾_Î£_gradL_i_Î¸t}"))

        
        norm_grad_L = np.linalg.norm(un_sur_ğ›¾_Î£_gradL_i_Î¸t)
        
        
        # update des (hyper) paramsw
        
        # paramÃ¨tre
        Î¸_t = Î¸_t + Î·_t * un_sur_ğ›¾_Î£_gradL_i_Î¸t #Î¸_t = Î¸_t - Î·_t * un_sur_ğ›¾_Î£_gradL_i_Î¸t #@
        str_theta = f"Î¸_{counter} = {Î¸_t}"
        print(str_theta)
        debug(logstr(str_theta))
        # âŸ¶ vecteur de la dim de Î¸
        
        # sampling policy
        q.update_parameters(Î¸_t)
        
        # pas
        Î·_t = update_Î·(Î·_t)
        debug(logstr(f"Î·_t+1 = {Î·_t}"))
        counter += 1
        
        if benchmark is True :
            target = f.parameters_list()
            benchmark_graph[0].append(counter)
            for k in range(len(Î¸_t)) :
                d_k = np.abs((Î¸_t[k] - target[k])/(target[k] + 1e-4))
                benchmark_graph[1+k].append(d_k)
            
            
            
    
    info(logstr("\n=========     FIN : SGD_L     ========="))
    
    if benchmark is True :
        
        print(f"c/2 = {counter//2}\n c % 2 ={counter%2}")
        print(benchmark_graph)
        
        fig = make_subplots(
                            rows= len(Î¸_t)//2 + len(Î¸_t)%2 , cols=2,
                            subplot_titles= [ r"$\text{erreur relative : }" + "\\left| \\frac{" "Î¸_" + f"{k}" + "- Î¸^*_"f"{k}" +"}" + "{Î¸^*" + f"_{k}" + "}"  +"\\right|" "$" for k in range(len(Î¸_t))]
                    )

        for k in range(len(Î¸_t)):
            print(f"({1 + k//2}, {1 + k%2})")
            fig.add_trace(plgo.Scatter(x=benchmark_graph[0] , y=benchmark_graph[1+k]), row = 1 + k//2 , col = 1 + k%2)
    
        fig.show()
    return Î¸_t
       
        

def update_Î·(Î·_t):
    Î·_t_plus_1 = Î·_t
    return Î·_t_plus_1



