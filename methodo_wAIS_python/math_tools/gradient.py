import numpy as np
from typing import Any, Callable, Optional
from numpy.typing import NDArray

from utils.log import logstr
from logging import info, debug, warn, error, critical



def gradient_selon(arg_num : int ,f : Callable[[], Any], *args ,h = 1e-7, composante : Optional[int] = None) -> NDArray:
    """renvoie le gradient d'une fonction multivariÃ©e f(... ğ‘¥áµ¢ ...)â‚,â‚™ selon  ğ‘¥_{arg_num}  Ã©valuÃ© en les arguments de f (*args)  |   oÃ¹ ğ‘¥áµ¢ âˆˆ â„^p
    
    si composante = ğ‘˜   âŸ¶   renvoie : [ğ›_Î¸â‚–]f(x) = [ 0 , ..., [ğœ•_Î¸â‚–]f(x) , ... , 0 ] âˆˆ â„^p

    Args:
        arg_num (int): starts at 1
        
        f (function): f :   R x R^p âŸ¶   R
                            (x , Î˜)  âŸ¼   f(x , Î˜)
        h (int, optional): finesse de la dÃ©rivÃ©e. Defaults to 1.
                            doit tendre vers 0 (h â‰ˆ 0)
        
        composante (int, optional) : [ğ›_Î¸]f(x) â‡’ [ğ›_Î¸_composante]f(x)
                                     Î¸ = [ Î¸â‚ , Î¸â‚‚ , ... , Î¸â‚š ]
                                     
                                     [ğ›_Î¸]f(x) = [ [ğœ•_Î¸â‚]f(x) , [ğœ•_Î¸â‚‚]f(x) , ... , [ğœ•_Î¸â‚š]f(x) ]
                                     
                                     va donc renvoyer : [ğ›_Î¸_composante]f(x) = [ 0 , ..., [ğœ•_Î¸câ‚’â‚˜â‚šâ‚’â‚›â‚â‚™â‚œâ‚‘]f(x) , ... , 0 ]

    Returns:


        for f(u,v,w) :
        u vector len p
        v vector len q
        w vector len r
        
        gradient_selon(1, f)
        [ [âˆ‚f/âˆ‚u_1](x) ... [âˆ‚f/âˆ‚u_p](x) ]   âˆˆ â„^p
        
        gradient_selon(2, f)
        [ [âˆ‚f/âˆ‚v_1](x) ... [âˆ‚f/âˆ‚v_q](x) ]   âˆˆ â„^q
        
        gradient_selon(3, f)
        [ [âˆ‚f/âˆ‚w_1](x) ... [âˆ‚f/âˆ‚w_r](x) ]   âˆˆ â„^r
        
        gradient_selon(3, f, composante = 2)
        = [ 0  [âˆ‚f/âˆ‚w_2](x)  0  ...  0 ]    âˆˆ â„^r

    """
    debug(logstr(f"Params :\n\narg_num = {arg_num}\nf = {f}\n\nargs = {args} âˆˆ {[type(obj) for obj in args]}"))
    
    # index
    index = arg_num-1
    #debug(logstr(f"index = {index}"))
    
    
    argument_differencie : np.ndarray = np.array(args[index])
    #                                   on s'assure que on a bien un vecteur numpy
    #                                   si il l'est dÃ©jÃ , il le reste
    #                                   sinon il est transformÃ© en ndarray ( notamment si c'est une liste )
    #debug(logstr(f"argument_differencie = {argument_differencie}"))
    
    
    
    p = argument_differencie.size
    #debug(f"p = {p}")
    
    
    
    gradient = np.zeros(p)
    
    
    # calcul du gradient
    
    debug(logstr("--- dÃ©but de calcul de gradient composante par composante ---"))
    # we compute each partial derivative
    
    # faire selon toutes les composantes du vecteur selon lequel on effectue le gradient
    if composante is None :
        for composante_index in range(p):
            #?debug(logstr(f"pour la composante : {composante_index}"))
            # (u,v,w, ...)
            # on dÃ©cide de modifier w, un vecteur de longueur p
            H = np.zeros(shape=p)
            # on ajoute h Ã  une des composantes de w
            # ici : composante_index dans [1,p]
            H[composante_index] = h
            theta_plus_h = argument_differencie + H
            #?debug(logstr(f"theta_plus_h = {theta_plus_h}"))
            # on renvoie (u, v, w', ...)
            # si la composante modifiÃ© Ã©tait w
            #?debug(logstr(f"args = {args}"))
            new_args = get_new_args(args, index, theta_plus_h)
            #?debug(logstr(f"new_args = {new_args}"))
            # calcul approchÃ© du gradient de f(u,v,w,...) selon w
            gradient_composante = (f(*new_args) - f(*args))/h
            #?debug(logstr(f"f(new_args) = {f(*new_args)}"))
            #?debug(logstr(f"f(args) = {f(*args)}"))
            #?debug(logstr(f"âˆ‚{index}_f[{composante_index}] = {gradient_composante}"))
            # grad_w f(u,v,w,...) 
            gradient[composante_index] = gradient_composante
    
    # en sÃ©lectionnant une composante particuliÃ¨re du vecteur selon lequel on effectue le gradient
    else :
        # (u,v,w, ...)
        # on dÃ©cide de modifier w, un vecteur de longueur p
        H = np.zeros(shape=p)
        # on ajoute h Ã  la composante numÃ©ro [composante] de w
        H[composante] = h
        theta_plus_h = argument_differencie + H
        # on renvoie (u, v, w', ...)
        # si la composante modifiÃ© Ã©tait w
        new_args = get_new_args(args, index, theta_plus_h)
        # calcul approchÃ© du gradient de f(u,v,w,...) selon w
        gradient_composante = (f(*new_args) - f(*args))/h
        # grad_w f(u,v,w,...) 
        gradient[composante] = gradient_composante

    
    
    debug(logstr(f"âˆ‡f = {gradient}\n"))
    
    return(gradient)


def get_new_args(args, index, modified_vec):
    #debug(logstr("=== DEBUT DE GET_NEW_ARGS ==="))
    #debug(logstr(f"index = {index}"))
    #debug(logstr(f"modified vector = {modified_vec}"))
    if index == 0 :
        args_copy = list(args)
        args_copy.pop(0)
        res = [modified_vec] +  args_copy
    else :
        before = [args[k] for k in range(index)]
        after = [args[(index+1) + k] for k in range(len(args)-(index+1))]
        res = before + [modified_vec] + after
        #debug(logstr(f"before = {before}"))
        #debug(logstr(f"after = {after}"))
        #debug(logstr(f"res = {res}"))
    #debug(logstr("=== FIN DE GET_NEW_ARGS ==="))
    return res
