from typing import Callable, Dict, List, Literal, Optional
from distribution_family.distribution_family import DistributionFamily
import kullback_leibler.L_gradient.grad_importance_sampling as kl_grad_is

import numpy as np
from numpy.typing import NDArray
from general.importance_sampling import importance_sampling_given_sample

from wAIS.update_sampling_policy import update_sampling_policy
from wAIS.squared_relative_distance import squared_relative_distance

def get_alpha_t(Î± : NDArray[np.float64], t) -> float:
    return Î±[t] / Î±.sum()
    
def weighted_adaptive_importance_sampling(  q_init : DistributionFamily,
                                            Ï† : Callable,
                                            Ï€ : Callable,
                                            nb_iter : int,
                                            nb_samples_per_iteration : int,
                                            nb_gradient_steps : int, 
                                            gradient_descent_frequency : int,
                                            # update sampling policy variables
                                            update_sampling_policy__step : float,
                                            update_sampling_policy__nb_drawn_samples : int,
                                            update_sampling_policy__nb_stochastic_choice : int,
                                            update_sampling_policy__param_composante : int,
                                            #       gradient ascent kullback-leibler
                                            update_sampling_policy__compute_grad_L_importance_sampling = kl_grad_is.compute_grad_L_estimator_importance_sampling,
                                            update_sampling_policy__method: Literal["descent"] | Literal["ascent"] = "ascent",
                                            update_sampling_policy__update_Î· : Callable = lambda x : x,
                                            update_sampling_policy__max_L_gradient_norm : int | float = np.Infinity,
                                            ) -> float:
    """
    returns an estimation of âˆ«f dÎ» = âˆ« Ï†â‹…Ï€ dÎ»   = âˆ« (Ï€/q)â‹…Ï†â‹…q dÎ»
                                                = âˆ«   Ï‰  â‹…Ï†â‹…q dÎ»
                                                = ğ”¼_q[ Ï‰â‹…Ï† ]
    """
    
    q_t = q_init.copy()
    if Ï€ is None :
        # todo : ce ne peut pas Ãªtre q_t puisque sinon alpha = 0 = (q_t/q_t - 1)Â²
        #! mais alors quoi ???
        # Ï€ = q_t
        raise ValueError("Ï€ can't be None")
    
    # X = [Xâ‚€, ..., Xâ‚œ]
    X : List[float]
    X = []
    # inutile : utilisÃ© pour N mais I_f / I_Ï€
    # n[0] = ğ‘›â‚€            N[t] = ğ‘›â‚œ
    # n : NDArray[np.int64] = np.ndarray([], dtype=np.int64)
    # need ndarray for sum method
    Î± : NDArray[np.float64]
    Î± = np.array([], dtype=np.float64)
    # Xâ‚œ : List[float]
    I_f_array = np.array([], dtype=np.float64)
    importance_sampling_Ï€_array = np.array([], dtype=np.float64)
    
    I_f = 0
    I_Ï€ = 0
    
    I_t : Optional[float] = None
        
    for t in range(nb_iter):
        # sampling
        X_t = q_t.sample(n = nb_samples_per_iteration)
        if X_t is None :
            raise ValueError("a problem happened while generating X_t, result is None")
        X = X + X_t

        """dÃ©termination des poids"""
        relative_distance_sum_t : float
        relative_distance_sum_t = np.array([squared_relative_distance(Ï€, q_t, x_i) for x_i in X_t]).sum()
        Î± = np.append(arr= Î± , values=1/relative_distance_sum_t)    
        # poids : 
        normalized_alpha : NDArray[np.float64] 
        normalized_alpha = Î± / Î±.sum()
        # pas utile car I = I_f / I_Ï€
        # n = np.append(n , nb_samples_per_iteration)
        # weighted_N : float
        # weighted_N = ((n * Î±).sum()) / Î±.sum()
    
        """â€”â€”â€”â€”â€” I_f â€”â€”â€”â€”â€”"""
        def Ï‰(x : float) -> float:
            return Ï€(x) / q_t.density(x)
        
        I_t_fcn = lambda X_t : importance_sampling_given_sample(
                                                Ï‰ = Ï‰,
                                                h = Ï†,
                                                X_from_sampler= X_t
                                                )
        I_f_array = np.append(arr = I_f_array, values = I_t_fcn(X_t) )
        I_f += (normalized_alpha[-1] * I_f_array[-1])
        
        """â€”â€”â€”â€”â€”  I_Ï€  â€”â€”â€”â€”â€”"""
        importance_sampling_Ï€ = lambda X_t : np.array([ Ï€.density(x_i)/q_t.density(x_i) for x_i in X_t ], dtype=np.float64).sum()
        importance_sampling_Ï€_array = np.append(importance_sampling_Ï€_array, importance_sampling_Ï€(X_t))
        I_Ï€ += (normalized_alpha[-1] * importance_sampling_Ï€_array[-1])
        
        I_t = I_f / I_Ï€
        if I_t is None :
            raise ValueError("Iâ‚œ should not be None")
        # si Ï€ est une densitÃ© de probabilitÃ©
        """âˆ« (Ï‰Ã—Ï†)â‹…q dÎ» = ğ”¼_q[ (Ï‰Ã—Ï†) ]"""
        update_sampling_policy( q_t                         = q_t,
                                t                           = t,
                                nb_gradient_steps           = nb_gradient_steps,
                                gradient_descent_frequency  = gradient_descent_frequency,
                                I_t                         = I_t,
                                Ï†                           = Ï†,
                                Ï€                           = Ï€,
                                X                           = X,
                                step                        = update_sampling_policy__step,
                                nb_drawn_samples            = update_sampling_policy__nb_drawn_samples, 
                                nb_stochastic_choice        = update_sampling_policy__nb_stochastic_choice, 
                                param_composante            = update_sampling_policy__param_composante, 
                                update_Î·                    = update_sampling_policy__update_Î·, 
                                max_L_gradient_norm         = update_sampling_policy__max_L_gradient_norm, 
                                method                      = update_sampling_policy__method,
                                compute_grad_L_importance_sampling = update_sampling_policy__compute_grad_L_importance_sampling
                              )
        
        
    if isinstance(I_t, float):
        return I_t
    else :
        raise TypeError("Unknown type for Iâ‚œ")