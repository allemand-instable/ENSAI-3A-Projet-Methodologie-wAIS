
\section{Glossary}
\begin{itemize}
    
    \item \textbf{Gradient}: A vector representing the partial derivatives of a function. 
    It represents the direction of steepest slope of a function. 
    
    \item \textbf{Integral}: A function which computes the area under a function curve.
    
    \item \textbf{Kullback-Leibler Divergence}: 
    $$L(\theta) =\displaystyle\int \left(\log \frac{q_\theta}{f}\right) f \ d \lambda$$ \\
    The Kullback-Leibler Divergence measures the proximity of two distribution 
    functions $q$ and $f$, i.e. how similar they are, assuming f is the distribution 
    from which the the sample $x_i$ was generated from. Other divergence measures exist, which 
    measure the similarity differently, i.e. RÃ©nyi's alpha divergence. 
    
    \item \textbf{Likelihood function}: A function which outputs a number representing the 
    probability of jointly observing the data under a certain probability function.
    The higher the likelihood for a given distribution $q$, the more likely the 
    data has been sampled from $q$. The likelihood can also be understood as a 
    measure of how plausible the observed data is under a given distribution. 

    \item \textbf{Loss function}: The loss function penalizes the 
    deviations of an estimate from the true value it is 
    trying to approximate in a prespecified way. 
    
    \item \textbf{Monte Carlo}: Monte Carlo methods arise from the field of probability theory and  
    serve to approximately solve difficult or infeasible analytical problems via 
    simulations of experiments. An Experiment is a random process with known 
    outcomes, e.g. throwing a dice, that can be performed an infinite amount of times.   
    
    \item \textbf{Normalization constant}: A constant by which we 
    want to divide an integral in order for it to be equal 
    to one and thus making it a probability density function.  
    
    \item \textbf{Probability density function}: A function which characterizes the
    uncertainty that underlies a random variable. It dictates the 
    patterns in which data appears, i.e. the data generating process, 
    and hence, provides insights into how spread out the data is 
    and where it is centered. 
            
    \item \textbf{Random Variable}: A random variable is a process, event or size whose 
    outcome underlies uncertainty. It is therefore governed by a rule that 
    determines how likely outcomes are to appear. That rule is also 
    referred to as probability density function.
    
    \item \textbf{Sample}: A collection of values generated from a distribution. 
    
    \item \textbf{Sampling policy}: A probability density function from which samples 
    are generated during the algorithm. 
    
    \item \textbf{Stochastic}: A property of a process or method that 
    it underlies uncertainty and thus relies on 
    probabilities and random events.
    
    \item \textbf{Weight}: A weight may shift a value, i.e. increase it, decrease it or keep it constant. 
    In the former two cases, this may emphasize or diminish its importance, respectively. 
    In the latter case, i.e. a weight of one, the value remains unchanged by the weight.
    
\end{itemize}