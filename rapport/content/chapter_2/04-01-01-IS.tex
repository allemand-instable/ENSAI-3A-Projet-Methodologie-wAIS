Notice the shape of the gradient given in the above box. This corresponds to the gradient of an importance sampling estimate. By replacing $h_{\theta_t}$ with $\nabla_\theta \log q_{\theta}(X_i)$, one receives the gradient of the importance sampling estimate based on the Kullback-Leibler criterion  as previously seen in section 2.2. We recall

$$
\begin{array}{rcl}
\widehat{\nabla L}(\theta) 
&\approx& 
\frac 1 N \displaystyle\sum\limits_{i = 1}^N \left( \nabla_\theta \ \log q_\theta (X_i) \times \frac{f(X_i)}{q_0(X_i)}\right)
\\
&=& \frac 1 N \displaystyle\sum_{i = 1}^N \omega(X_i) \times h_\theta(X_i)
\end{array}
$$

with:

\faAngleRight \ $\omega: x \mapsto \frac{f(x)}{q_0(x)}$

\faAngleRight \ $h_\theta: x \mapsto \nabla_\theta \ \log q_\theta(x)$

By plugging the gradient into the gradient ascent, we receive algorithm 1. To alleviate the notation we henceforth denote $q_{\theta_t} \equiv q_t$. 


\begin{algorithm}[H]
\caption{Gradient Ascent - IS}\label{alg:gaIS}
\begin{algorithmic}
    \Require 
    \\ \ra Initiate $\theta_0 \in \mathbb R^p$ 
    \\ \ra Initiate $\eta_0$ (or $\eta$ for a fixed step size) 
    \\ \ra Choose a sampling distribution $q_0$
    \\ \ra Choose a small value of $\varepsilon$ ( i.e $\varepsilon \rightarrow 0 $)
    \\ \ra Choose a number of maximum iterations: max.iter
    \State Sample $X = \famfinie X 1 N$ from distribution $q_0$ 
    \For{$t \in \llbracket 1, \max.iter \rrbracket$}
        \If {$\Vert \nabla f \Vert < \varepsilon$}
            \State Break the loop
        \EndIf
        \State \ra Compute $\widehat{\nabla_\theta L}(\theta_t) = \displaystyle\sum\limits_i\omega(X_i) h_\theta(X_i)$
        \State \ra $\underbracket[1pt][5pt]{\theta_{[t]} \gets}_{\theta_{t+1} = } \theta_{[t]} + \eta \nabla L(\theta_t)$
        \State \ra Update $\eta$ such that $f(x_{t+1}) > f(x_t)$ 
    \EndFor  
\end{algorithmic}
\end{algorithm}

