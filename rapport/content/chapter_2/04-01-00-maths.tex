In this section, we design our optimization algorithm. The goal is to have a stochastic gradient descent. Hence, we shall start by recalling the classic gradient descent. Here, we are actually interested in a gradient ascent as we are looking for the maximum of the Kullback-Leibler criterion. 

\bigskip

The goal is to find the maximum of $L$.

in other terms: $dL(\theta_{max}) = 0$

The gradient is a vector of the same dimension as $\theta$ which satisfies the following equation: $$d_xL(h) = \left< \ \nabla L(x) \ | \ h \ \right>$$

$$L(x) \underset{h \rightarrow 0}{=} L(x_0) + d_{x_0}L(x - x_0) + o( \Vert h \Vert )$$

The main idea is that:

$$
\begin{array}{rcl}
\argmax\limits_x L(x) &\underset{h \rightarrow 0}{\approx}& \argmax\limits_{x} \left[ \ L(x_0) + d_{x_0}L(h_x) \ \right]
\\
& = &
\argmax\limits_{h_x} \left[ d_{x_0}L(h_x) \ \right]
\\
& = &
\argmax\limits_{h_x} \left< \ \nabla L(x) \ | \ h_x \ \right>
\end{array}
$$


We are interested to know the direction $h_x$ which maximises $L$. It holds that

$$\underset{||h_x|| = 1}{\operatorname{max}} \left< \ \frac{\nabla L(x)}{|| \nabla L(x) ||} \ | \ h_x \ \right> = 1$$

 and thus, by using Cauchy-Schwarz inequality, we see that:

$$\argmax_{||h_x|| = 1} \left< \ {\nabla L(x)} \ | \ h_x \ \right> = {+} \frac{\nabla L(x)}{|| \nabla L(x) ||}$$

Hence, the direction of greatest ascent is given by

$$h = + \frac{\nabla L(x)}{|| \nabla L(x) ||}$$.

\bigskip

Gradient ascent is an iterative optimization procedure.
Therefore, we iterate from an initialization value of our parameter $\theta_0$ and gradually approach $\theta_{max}$, a point where $L$ has a local maximum.

\bigskip

$$\boxed{\theta_{t+1} \leftarrow \theta_t + \gamma \underbracket{ \widehat{\nabla_\theta L}(\theta_t)}_{\frac 1 N \displaystyle\sum_{i = 1}^N \omega(X_i) \times \nabla_\theta \left[ h_{\theta_t}(X_i)\right]}}$$

